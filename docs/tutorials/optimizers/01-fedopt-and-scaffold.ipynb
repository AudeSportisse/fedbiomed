{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b098b814",
   "metadata": {},
   "source": [
    "# Advanced optimizers in Fed-BioMed\n",
    "\n",
    "\n",
    "**Difficulty level**: **advanced**\n",
    "    \n",
    "## Introduction\n",
    "\n",
    "This tutorial presents on how  to deal with heterogeneous dataset by changing its `Optimizer`. \n",
    "In `Fed-BioMed`, one can specify two sort of `Optimizer`s:\n",
    "\n",
    "1. a `Optimizer` on the `Node` side, defined on the `Training Plan`\n",
    "2. a `Optimizer` on the `Researcher` side, configured in the `Experiment`\n",
    "\n",
    "Advanced `Optimizer` are backed by [`declearn` package](), a python package focused on `Optimization` for Federated Learning. Advanced `Optimizer` can be used regardless of the machine learning framework (compatible with both sklearn and PyTorch)\n",
    "\n",
    "\n",
    "In this tutorial you will learn:\n",
    "- how to use and chain one or several `Optimizers` on `Node` and `Researcher` side\n",
    "- how to use fedopt\n",
    "- how to use `Optimizers` that exchange auxiliary variables such as `Scaffold`\n",
    "\n",
    "For further details you can refer to the [`Optimizer` section in the User Guide]()\n",
    "\n",
    "# 1. Configuring `Nodes`\n",
    "\n",
    "Before starting, we need to configure several `Nodes` and add MedNist dataset to it. Node configuration steps require `fedbiomed-node` conda environment. Please make sure that you have the necessary conda environment: this is explained in the [installation tutorial](../../installation/0-basic-software-installation). \n",
    "\n",
    "\n",
    "Please open a terminal, `cd` to the base directory of the cloned fedbiomed project and follow the steps below.    \n",
    "\n",
    "* **Configuration Steps:**\n",
    "    * Run `${FEDBIOMED_DIR}/scripts/fedbiomed_run node add` in the terminal\n",
    "    * It will ask you to select the data type that you want to add. The third option has been configured to add the MedNIST dataset. Please type `3` and continue. \n",
    "    * Please use default tags which are `#MEDNIST` and `#dataset`.\n",
    "    * For the next step, please select the directory that you want to download the MNIST dataset.\n",
    "    * After the download is completed you will see the details of the MNIST dataset on the screen.\n",
    " \n",
    "Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the Node. \n",
    "\n",
    "```\n",
    "$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config conf1.ini add\n",
    "```\n",
    "\n",
    "Before starting the node, please make sure that you have already launched the network using command `scripts/fedbiomed_run network`. Afterward, all you need to do is to start the node.\n",
    "\n",
    "\n",
    "```\n",
    "$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node config conf1.ini start\n",
    "```\n",
    "\n",
    "In another terminal, you may proceed by launching a second `Node`\n",
    "\n",
    "# 2. Defining an `Optimizer` on `Node` side\n",
    "\n",
    "`Optimizers` are defined through the `init_optimizer` method of the `training plan`. They must be set using `Fed-BioMed` `Optimizer` object (ie from `fedbiomed.common.optimizers.optimizer.Optimizer`)\n",
    "\n",
    "## 2.1 With PyTorch framework\n",
    "\n",
    "In [this tutorial]() we have showcased the use of a PyTorch model with [PyTorch native optimizers](), such as `torch.optim.SGD`. In the present tutorial, we will see how to use `declearn` cross frameworks optimizers\n",
    "\n",
    "### PyTorch `Training Plan`\n",
    "Below is a simple implementation of a `declearn` SGD `Optimizer` on a PyTorch model. It is equivalent to the following\n",
    "\n",
    "```python\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    ...\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.SGD(self.model().parameters(), lr = optimizer_args['lr'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\",\n",
    "               \"from fedbiomed.common.optimizers.optimizer import Optimizer\"]\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        model = densenet121(pretrained=True)\n",
    "        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        return model \n",
    "    \n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        # Defines and return a declearn optimizer\n",
    "        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n",
    "        return Optimizer(lr=optimizer_args['lr'])\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb6e0a",
   "metadata": {},
   "source": [
    "### 2.2 Sklearn `Training Plan`\n",
    "\n",
    "For another machine learning framework such as sklearn, syntax is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import FedSGDClassifier\n",
    "from fedbiomed.common.data import DataManager\n",
    "\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "\n",
    "\n",
    "\n",
    "class SGDRegressorTrainingPlan(FedSGDClassifier):\n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\"]\n",
    "        return deps\n",
    "\n",
    "    def training_data(self, batch_size):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        X_train = dataset.data.numpy()\n",
    "        X_train = X_train.reshape(-1, 28*28)\n",
    "        Y_train = dataset.targets.numpy()\n",
    "        return DataManager(dataset=X_train, target=Y_train, batch_size=batch_size)\n",
    "\n",
    "    # Defines and return a declearn optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Optimizer(lr=optimizer_args['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e638d",
   "metadata": {},
   "source": [
    "### 2.3 Using a more advanced `Optimizer` with `Regularizer`\n",
    "\n",
    "`Optimizer` from `fedbiomed.common.optimizers.optimizer` with learning rate equal `.1` can be written as ```Optimizer(lr=.1, decay=0., modules=[], regualrizers=[])```, where:\n",
    "\n",
    "- `decay` is the weight decay ;\n",
    "- `modules` is a python list containing one or several [`declearn` `OptiModules`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/modules/OptiModule/) ;\n",
    "- `regularizers` is a python list containing one or several [`declearn` `Regularizers`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/regularizers/Regularizer/)\n",
    "\n",
    "We will re-use the `Pytorch Training Plan` already defined above and show how to use a `Adam` `Optimizer` with `Ridge` as the `Regularizer`. For that, we need to import the `Adam`  and the `Ridge` version of `declearn` (`AdamModule` and `RidgeRegularizer`).\n",
    "\n",
    "Then the `Training Plan` can be defined as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3023438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "from declearn.optimizer.modules import AdamModule\n",
    "from declearn.optimizer.regularizers import RidgeRegularizer\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n",
    "                \"from declearn.optimizer.modules import AdamModule\",\n",
    "                \"from declearn.optimizer.regularizers import RidgeRegularizer\"]\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        model = densenet121(pretrained=True)\n",
    "        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        return model \n",
    "    \n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        # Defines and return a declearn optimizer\n",
    "        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n",
    "        return Optimizer(lr=optimizer_args['lr'], modules=[AdamModule()], regularizers=[RidgeRegularizer()])\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc6e7e",
   "metadata": {},
   "source": [
    "### 2.4. Create the `Experiment`\n",
    "\n",
    "Once the `Training Plan` has been created with a specific framework model, definition of the `Experiment` is the same as the one in PyTorch or Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 8,\n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'dry_run': False,\n",
    "    'num_updates': 50\n",
    "}\n",
    "\n",
    "tags =  ['#dataset', '#MEDNIST']\n",
    "rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "\n",
    "exp = Experiment()\n",
    "exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp.set_model_args(model_args=model_args)\n",
    "exp.set_training_args(training_args=training_args)\n",
    "exp.set_tags(tags = tags)\n",
    "exp.set_aggregator(aggregator=FedAverage())\n",
    "exp.set_round_limit(rounds)\n",
    "exp.set_training_data(training_data=None, from_tags=True)\n",
    "exp.set_job()\n",
    "exp.set_strategy(node_selection_strategy=DefaultStrategy)\n",
    "\n",
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb5ea3f",
   "metadata": {},
   "source": [
    "# 3. Defining an `Optimizer` on `Researcher` side: `FedOpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05decc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb6b343f",
   "metadata": {},
   "source": [
    "# 4. Defining `Scaffold` through `Optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046225b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b5bacc7",
   "metadata": {},
   "source": [
    "# 5. Explore advanced `Optimizer` feature through `declearn` and the Fed-BioMed user guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83996012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
