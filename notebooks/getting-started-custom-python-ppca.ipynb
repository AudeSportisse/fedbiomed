{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e87007",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher to train a federated PPCA (Probabilistic PCA) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17d2f",
   "metadata": {},
   "source": [
    "## Description of the exercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb782b14",
   "metadata": {},
   "source": [
    "Three datasets `n1.csv` , `n2.csv` and `n3.csv` will be generated randomly using 3-views PPCA from a 4-dimensional latent space, with views dimensions [15,8,10] and 2 groups. Henceforth, we will distribute the 3 dataset to 3 distinct nodes and use Fed-mv-PPCA. In each center we check the evolution of expected LL during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0bc07",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We will generate three datasets using mv-PPCA.\n",
    "Then save them in a path of your choice on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "def sample_x_n(N:int, q:int, random_state:int=None):\n",
    "    \"\"\"samples from a Guassian dsitribution\n",
    "    Args:\n",
    "    \n",
    "    :N first dimension array\n",
    "    :q second dimesnion array\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(random_state).randn(N,q)\n",
    "\n",
    "def generate_data(N_g: List[int],\n",
    "                  W: np.ndarray,\n",
    "                  a_g: np.ndarray,\n",
    "                  mu:float,\n",
    "                  sigma2:float,\n",
    "                  x_n,\n",
    "                  view:int,\n",
    "                  random_state=None):\n",
    "    \n",
    "    \"\"\"generates Gaussian dataset given several groups of data points, using the following\n",
    "    gausian generative proccess (for a given view):\n",
    "    \n",
    "    Y = WX + mu + epsilon with epsilon ~ N(0, sigma2)\n",
    "    \n",
    "    where X is the latent space of size (q, n_features), Y the observation matrix, W the matrix used\n",
    "    for data reconstruction\n",
    "    \n",
    "    Params:\n",
    "    :N_g: List[int] number of data to generate per group (list of size number of group)\n",
    "    :W: reconstruction matrix, of size (n_features, q)\n",
    "    :a_g: (np.ndarray) array of size (nb_group, n_components), introduces shift when creating different group. \n",
    "    :mu: offset of the dataset\n",
    "    :sigma2: variance used for generating\n",
    "    :x_n: random variable \n",
    "    :view: (int) the given view\n",
    "    :random_state: ransom seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    :Y (pd.DataFrame): synthetic dataset generated wrt above expression\n",
    "    of size (n_features, n_samples)\n",
    "    \"\"\"\n",
    "    rnd=np.random.RandomState(random_state)\n",
    "\n",
    "    N=N_g.sum()\n",
    "    d, q = W.shape\n",
    "    sigma=np.sqrt(sigma2)\n",
    "    G=len(N_g)\n",
    "\n",
    "    g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    print(g_ind)\n",
    "\n",
    "    y_n=np.empty((N, d))\n",
    "\n",
    "    for g in range(G):\n",
    "        # computing Y = W.transpose(X + shift()) + mu\n",
    "        y_n[g_ind[g]:g_ind[g+1]]= np.einsum(\"dq,nq->nd\", W, x_n[g_ind[g]:g_ind[g+1]]+a_g[g]) + mu\n",
    "        \n",
    "    y_n = pd.DataFrame(data=y_n,\n",
    "                     columns=[f'var_{view},{i + 1}' for i in range(d)])\n",
    "\n",
    "    return y_n + sigma*rnd.randn(N,d)\n",
    "\n",
    "\n",
    "\n",
    "def generate_ppca_nodes_dataset(n_nodes: int,\n",
    "                           n_features: Union[List[int],int],\n",
    "                           \n",
    "                           n_components: int,\n",
    "                               n_group: int=2,\n",
    "                           absent_view: Dict[str, int]=None,\n",
    "                           W_init: List[np.ndarray]=None,\n",
    "                           mu_init: List[np.ndarray]=None,\n",
    "                           sigma_init : List[np.ndarray]=None,\n",
    "                           is_validation: bool=True,\n",
    "                           n_sample_validation: int=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for each node\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate PPCA parameters if not defined\n",
    "    ## case where W parameter is not defined\n",
    "    if W_init is None:\n",
    "        W_init = []\n",
    "        for i in range(n_nodes):\n",
    "            W_gen = np.random.uniform(-10, 10, (n_features[i], n_components))\n",
    "            W_init.append(W_gen)\n",
    "    ## case where mu not generated\n",
    "    if mu_init is None:\n",
    "        mu_init = []\n",
    "        for i in range(n_nodes):\n",
    "            mu_gen = np.random.uniform(-10, 10, n_features[i])\n",
    "            mu_init.append(mu_gen)\n",
    "            \n",
    "    ## case where sigma is not definied (we will set sigma =1 for each clients)\n",
    "    if sigma_init is None:\n",
    "        sigma_init = []\n",
    "        for i in range(n_nodes):\n",
    "            sigma_init.append(1)\n",
    "            \n",
    "    \n",
    "    shift = np.concatenate((np.zeros((1, n_components)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ab247",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4\n",
    "\n",
    "\n",
    "# initializing PPCA variables\n",
    "sigma2_gen1, sigma2_gen2, sigma2_gen3 = 2, 1, 3\n",
    "W_gen1 = np.random.uniform(-10, 10, (D_i[0], n_components_generated))\n",
    "W_gen2 = np.random.uniform(-5, 5, (D_i[1], n_components_generated))\n",
    "W_gen3 = np.random.uniform(-15, 15, (D_i[2], n_components_generated))\n",
    "mu_gen1 = np.random.uniform(-10, 10, D_i[0])\n",
    "mu_gen2 = np.random.uniform(-5, 5, D_i[1])\n",
    "mu_gen3 = np.random.uniform(-15, 15, D_i[2])\n",
    "\n",
    "a_g_gen = np.concatenate((np.zeros((1, n_components_generated)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components_generated))))\n",
    "\n",
    "W = [W_gen1,W_gen2,W_gen3]\n",
    "mu = [mu_gen1,mu_gen2,mu_gen3]\n",
    "sigma = [sigma2_gen1,sigma2_gen2,sigma2_gen3]\n",
    "\n",
    "# absent_views contains as key the id of a center in which we want to simulate absent views,\n",
    "# and as argumet the id of the missing view. \n",
    "absent_views = {'2': 2}\n",
    "\n",
    "for i in range(n_centers):\n",
    "    N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)]) \n",
    "    # N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)])?\n",
    "    #g_ind = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    N = N_g.sum()\n",
    "    x_n_gen = sample_x_n(N, n_components_generated, random_state=150)  # randomly generate a Gaussian\n",
    "    # dataset\n",
    "    Y = []\n",
    "    for d in range(len(D_i)):\n",
    "        y_t = generate_data(N_g, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "        if ((str(i+1) in absent_views.keys()) \\\n",
    "            and (type(absent_views[str(i+1)])== int) \\\n",
    "            and (absent_views[str(i+1)]==d+1)):\n",
    "            absent_views.update({str(i+1): y_t})\n",
    "            y_abs=pd.DataFrame(np.nan, index = np.arange(N_g.sum()), \\\n",
    "                               columns = [f'var_{d+1},{i + 1}' for i in range(D_i[d])])\n",
    "            Y.append(y_abs)\n",
    "        else:\n",
    "            Y.append(y_t)\n",
    "\n",
    "    gr = []\n",
    "    for g in range(nb_group):\n",
    "       gr += [int(g) for _ in range(N_g[g])]\n",
    "    gr = pd.Series(gr)\n",
    "    Y.append(gr)\n",
    "\n",
    "###\n",
    "# the output will be a list of n_centers dataset containing: dataframe for each centers,of \n",
    "# different dimensions\n",
    "# \n",
    "    t_i = pd.concat(Y, axis=1)\n",
    "    t_i.columns.values[-1] = 'Label'\n",
    "    t_i.to_csv('== Local path to node' + str(i+1) + '.csv',sep=',')\n",
    "    #np.savetxt('== Local path to node' + str(i+1) + '.csv',t_i,delimiter=',')\n",
    "               \n",
    "# building the test dataset\n",
    "N_g_test = np.array([testing_samples//2,testing_samples//2])\n",
    "g_ind_test = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g_test)))\n",
    "N_test = N_g_test.sum()\n",
    "x_n_gen = sample_x_n(N_test, n_components_generated, random_state=150)\n",
    "Y_test = []\n",
    "for d in range(len(D_i)):\n",
    "    y_t = generate_data(N_g_test, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "    Y_test.append(y_t)\n",
    "\n",
    "gr_test = [0 for _ in range(N_g_test[0])]+[1 for _ in range(N_g_test[1])]\n",
    "gr_test = pd.Series(gr_test)\n",
    "Y_test.append(gr_test)\n",
    "\n",
    "t_test = pd.concat(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "g_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc822b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n_gen[g_ind[g]:g_ind[g+1]].shape, W[0].shape, a_g_gen[g].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "\n",
    "print(W[0].shape)\n",
    "resp = np.einsum(\"dq,nq->nd\", W[0], x_n_gen[g_ind[g]:g_ind[g+1]]+a_g_gen[g])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = x_n_gen[g_ind[g]:g_ind[g+1]]+a_g_gen[g]\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"dq,nq->nd\", np.array([[1, 0,0, 0],\n",
    "                                 [0, 0,0, 0]]), np.array([[0, 1, 1, 1],\n",
    "                                                          [0,0,0,0,],\n",
    "                                                          [0,0,0,0,],\n",
    "                                                          [0,0,0,0,]\n",
    "                                                         ]\n",
    "                                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tt = pd.DataFrame({\"www\": [np.nan, np.nan, np.nan], 'qq': [1, 2, 4]})\n",
    "\n",
    "tt.iloc[:, 1].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f160e",
   "metadata": {},
   "source": [
    "## Start the network and setting the client up\n",
    "Before running this notebook:\n",
    "1. You should start the network from fedbiomed-network, as detailed in :\n",
    "https://gitlab.inria.fr/fedbiomed/fedbiomed\n",
    "\n",
    "2. You need to configure at least 2 nodes: <br/>\n",
    "* **Node 1 :** `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[0],y[0].\n",
    "  * Check that your data has been added in node 1 by executing `./scripts/fedbiomed_run node list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node start`. <br/>\n",
    "\n",
    "* **Node 2 :** Open a second terminal and run ./scripts/fedbiomed_run node add config n2.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[1],y[1].\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n2.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n2.ini start`.\n",
    "  \n",
    "\n",
    "\n",
    " Wait until you get `Connected with result code 0`. it means node is online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c80070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fedbiomed.researcher.environ import TMP_DIR\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=TMP_DIR+'/')\n",
    "model_file = tmp_dir_model.name + '/fed_mv_ppca.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9aaa87",
   "metadata": {},
   "source": [
    "Hereafter the template of the class you should provide to Fedbiomed :\n",
    "       \n",
    "**training_data** : you must return here a tuple (X,X_k,ViewsX,y) or (X,X_k,ViewsX). Note that all centers should provide a dataset with the same view-specific columns. If in a specific center a view has not been observed, then the corresponding columns will be filled of nan. The training_data method take care of identifying view-specific sub-datasets and collecting information concerning non-available observations. Data can also been normalized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10cc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ybouilla/fedbiomed/var/tmp/tmp9s6buqcv/fed_mv_ppca.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fed_MV_PPCA(PpcaPlan):\n",
    "    def __init__(self, kwargs):\n",
    "        super(Fed_MV_PPCA, self).__init__(kwargs)\n",
    "        deps = ['import numpy as np']\n",
    "        self.add_dependency(deps)\n",
    "    \n",
    "    def training_data(self):\n",
    "        \"\"\"\n",
    "            Perform in this method all data reading and data transformations you need.\n",
    "            At the end you should provide a tuple (X_obs,Xk,ViewsX,y), where: \n",
    "            X_obs is the training dataset, \n",
    "            Xk is a list containing the k-specific dataframe if it exists or 'NaN' otherwise,\n",
    "            ViewsX is the indicator function for observed views (ViewsX[k]=1 if view k is observed, 0 otherwise)\n",
    "            y the corresponding labels (optional)\n",
    "            The dataset is normalized using min max scaler if model_args['norm'] is true\n",
    "            Note: since labels are not needed for the optimization, \n",
    "            training_data can also simply return (X_obs,Xk,ViewsX)\n",
    "            :raise NotImplementedError if researcher do not implement this method.\n",
    "        \"\"\"\n",
    "        dataset = pd.read_csv(self.dataset_path,delimiter=',', index_col=0)\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(self.K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append(np.nan)\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = self.normalize_data(X.iloc[:, ind:ind + self.dim_views[k]]) if self.norm \\\n",
    "                    else X.iloc[:, ind:ind + self.dim_views[k]]\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += self.dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cda129",
   "metadata": {},
   "source": [
    "**model_args** is a dictionary containing the mv-ppca model arguments: the total number of views across all datasets (tot_views), the dimension of each view (dim_views), the latent space size (n_components), and a boolean (norm) for data preprocessing. Additionaly, the researcher can provide priors for one ore more global parameters.\n",
    "\n",
    "**training_args** contains here the number of local iterations for EM/MAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05aa5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_views = 3\n",
    "dim_views = [15, 8, 10]\n",
    "n_components = 4\n",
    "norm = True\n",
    "\n",
    "model_args = {'tot_views': tot_views, 'dim_views': dim_views, 'n_components': n_components, 'norm': norm}\n",
    "\n",
    "training_args = {'n_iterations': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1a1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 18:37:00,872 fedbiomed INFO - Messaging researcher_703b6301-fd20-473e-a2fc-31aa6fba20da successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7fee387bd7f0>\n",
      "2021-10-27 18:37:00,901 fedbiomed INFO - Searching dataset with data tags: ['ppca_data'] for all nodes\n",
      "2021-10-27 18:37:00,905 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - DEBUG Message received: {'researcher_id': 'researcher_703b6301-fd20-473e-a2fc-31aa6fba20da', 'tags': ['ppca_data'], 'command': 'search'}\n",
      "2021-10-27 18:37:11,133 fedbiomed INFO - Messaging NodeTrainingFeedbackClient successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7fed78f2c9a0>\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.mlaggregator import MLaggregator\n",
    "\n",
    "tags =  ['ppca_data']\n",
    "rounds = 5\n",
    "\n",
    "# select nodes pr into task <Task pending name='Task-27' coro=<HTTP1ServerConnection._server_request_loop() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/http1connection.py:823> wait_for=<Future finished result=b'GET /kernel...6bd7\"\\r\\n\\r\\n'> cb=[IOLoop.add_future.<locals>.<lambda>() at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/ioloop.py:688]> while another task <Task pending name='Task-2' coro=<KernelManager._async_start_kernel() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/jupyter_carticiping to this experiment\n",
    "exp = Experiment(tags=tags,\n",
    "                 #clients=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='Fed_MV_PPCA',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=MLaggregator(),\n",
    "                 client_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff55da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 18:37:11,142 fedbiomed INFO - Sampled clients in round 0 ['client_492822b8-6b10-490f-a467-58ea9a0e679a']\n",
      "2021-10-27 18:37:11,144 fedbiomed INFO - Send message to client client_492822b8-6b10-490f-a467-58ea9a0e679a - {'researcher_id': 'researcher_703b6301-fd20-473e-a2fc-31aa6fba20da', 'job_id': 'bf0d3c17-32fa-4660-bb9d-113fedb17919', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_60d79641-268e-4ef4-9c28-e7e0fe8cd148.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_3054b3d0-60d5-4fff-aa32-937a7289805e.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_492822b8-6b10-490f-a467-58ea9a0e679a': ['dataset_29f082ac-b0d9-47d8-b354-902174971d60']}}\n",
      "2021-10-27 18:37:11,144 fedbiomed DEBUG - researcher_703b6301-fd20-473e-a2fc-31aa6fba20da\n",
      "2021-10-27 18:37:11,147 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - DEBUG Message received: {'researcher_id': 'researcher_703b6301-fd20-473e-a2fc-31aa6fba20da', 'job_id': 'bf0d3c17-32fa-4660-bb9d-113fedb17919', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_60d79641-268e-4ef4-9c28-e7e0fe8cd148.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_3054b3d0-60d5-4fff-aa32-937a7289805e.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_492822b8-6b10-490f-a467-58ea9a0e679a': ['dataset_29f082ac-b0d9-47d8-b354-902174971d60']}}\n",
      "2021-10-27 18:37:11,148 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - DEBUG [TASKS QUEUE] Item:{'researcher_id': 'researcher_703b6301-fd20-473e-a2fc-31aa6fba20da', 'job_id': 'bf0d3c17-32fa-4660-bb9d-113fedb17919', 'params_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_3054b3d0-60d5-4fff-aa32-937a7289805e.pt', 'training_args': {'n_iterations': 15}, 'training_data': {'client_492822b8-6b10-490f-a467-58ea9a0e679a': ['dataset_29f082ac-b0d9-47d8-b354-902174971d60']}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'model_url': 'http://localhost:8844/media/uploads/2021/10/27/my_model_60d79641-268e-4ef4-9c28-e7e0fe8cd148.py', 'model_class': 'Fed_MV_PPCA', 'command': 'train'}\n",
      "2021-10-27 18:37:11,168 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - INFO {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7fdf7a9eeca0>, 'n_iterations': 15}\n",
      "2021-10-27 18:37:11,170 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - DEBUG Dataset_path/home/ybouilla/fedbiomed/notebooks/== Local path to node1.csv\n",
      "2021-10-27 18:37:11,175 fedbiomed INFO - log from: client_492822b8-6b10-490f-a467-58ea9a0e679a - ERROR Cannot train model in round: training_routine() got an unexpected keyword argument 'monitor'\n",
      "2021-10-27 18:37:11,177 fedbiomed CRITICAL - researcher stopped after receiving error/critical log from node: client_492822b8-6b10-490f-a467-58ea9a0e679a\n"
     ]
    }
   ],
   "source": [
    "# start federated training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a782",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918cbe3",
   "metadata": {},
   "source": [
    "Herafter we test the performance of the aggregated parameters on a test dataset. In particular, for each round we use the global parameters to evaluate the mean absolute error and the separation in the latent space using LDA. Note that we have already defined the test dataset at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import solve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    This function normalize the dataset X\n",
    "    :param X: pandas dataframe\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    col_name = [col.strip() for col in list(X.columns)]\n",
    "    x = X.values  # returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    norm_dataset = pd.DataFrame(x_scaled, index=X.index, columns=col_name)\n",
    "\n",
    "    return norm_dataset\n",
    "\n",
    "def eval_MB(Wk, q, D_i, K, Sigma2, ViewsX):\n",
    "    \"\"\"\n",
    "    Computes matrices M:=inv(I_q+sum_k Wk.TWk/sigma2k) and B:= [W1.T/sigma2K,...,W1.T/sigma2K].\n",
    "    :param Wk: list of matrices (d_k x q)\n",
    "    :param Sigma2: list of float > 0\n",
    "    :return np.arrays\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    M1 = Wk[index].reshape(D_i[index], q).T.dot(Wk[index].reshape(D_i[index],q)) / Sigma2[index]\n",
    "    B = Wk[index].reshape(D_i[index], q).T / Sigma2[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            M1 += Wk[k].reshape(D_i[k], q).T.dot(Wk[k].reshape(D_i[k],q)) / Sigma2[k]\n",
    "            B = np.concatenate((B, (Wk[k].reshape(D_i[k], q)).T / Sigma2[k]), axis=1)\n",
    "\n",
    "    M = solve(np.eye(q) + M1,np.eye(q))\n",
    "\n",
    "    return M, B\n",
    "\n",
    "def concat_params(park, K, ViewsX):\n",
    "    \"\"\"\n",
    "    This function concatenates parameters from a list\n",
    "    :param park: list of vectors/matrices to concatenate\n",
    "    :return np.array\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    par = park[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            par = np.concatenate((par, park[k]), axis=0)\n",
    "\n",
    "    return par\n",
    "\n",
    "def simu_latent(q,dataset,ViewsX,global_params):\n",
    "    \"\"\"\n",
    "    This function allows sampling of x_n (latent variables) from the posterior distribution \n",
    "    (with global parameters).\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "\n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    Xn = [(M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(1, q) for n in range(N)]\n",
    "\n",
    "    df = pd.DataFrame(np.vstack(Xn), index=dataset.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def MAE(dataset,ViewsX,q,global_params):\n",
    "    \"\"\"\n",
    "    This function evaluates the MAE using global parameters\n",
    "    :return float\n",
    "    \"\"\"    \n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "    \n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    W = concat_params(global_params['tilde_Wk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    T_true = dataset.values.tolist()\n",
    "\n",
    "    T_pred = []\n",
    "    for n in range(N):\n",
    "        Xng = (M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(q, 1)\n",
    "        T_pred.append((W.dot(Xng) + mu).reshape(d))\n",
    "\n",
    "    MAE = mean_absolute_error(T_true, T_pred)\n",
    "\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def test_data(dataset,norm,K,dim_views):\n",
    "        \"\"\"\n",
    "            Equivalent to training_data, for the test dataset\n",
    "        \"\"\"\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append('NaN')\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = normalize_data(X.iloc[:, ind:ind + dim_views[k]]) if norm \\\n",
    "                    else X.iloc[:, ind:ind + dim_views[k]]\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if type(item) != str]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1,rounds):\n",
    "    global_params = exp.aggregated_params[r]['params']\n",
    "    ######## Train data\n",
    "    MAE_train = []\n",
    "    Latent_Train = pd.DataFrame()\n",
    "    Label_Train = pd.Series(dtype='int64')\n",
    "    for c in range(n_centers):\n",
    "        dataset_c = pd.read_csv('== Local path to node' + str(c+1) + '.csv', delimiter=',', index_col=0)\n",
    "        X_obs_c,Xk_c,ViewsX_c,y_c = test_data(dataset_c,norm,tot_views,dim_views)\n",
    "        # Dataframe of latent space for LDA\n",
    "        Latent_Train = Latent_Train.append(simu_latent(n_components,X_obs_c,ViewsX_c,global_params))\n",
    "        Label_Train = Label_Train.append(y_c)\n",
    "        # MAE Train\n",
    "        MAE_train.append(MAE(X_obs_c,ViewsX_c,n_components,global_params))\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    X_Train_lda = lda.fit_transform(Latent_Train, Label_Train.reindex(Latent_Train.index))\n",
    "\n",
    "    ######## Test data\n",
    "    X_obs_test,Xk_test,ViewsX_test,y_test = test_data(t_test,norm,tot_views,dim_views)\n",
    "\n",
    "    Latent_Test = pd.DataFrame()\n",
    "    Label_Test = pd.Series(dtype='int64')\n",
    "\n",
    "    # Dataframe of latent space for LDA\n",
    "    Latent_Test = Latent_Test.append(simu_latent(n_components,X_obs_test,ViewsX_test,global_params))\n",
    "    Label_Test = Label_Test.append(y_test)\n",
    "    # MAE Test\n",
    "    MAE_test = MAE(X_obs_test,ViewsX_test,n_components,global_params)\n",
    "    # LDA Test\n",
    "    Size_tes = Latent_Test.shape[0]\n",
    "    y_pred_test = lda.predict(Latent_Test)\n",
    "    conf_LDA_Test = confusion_matrix(Label_Test.reindex(Latent_Test.index), y_pred_test)\n",
    "    TP = np.diag(conf_LDA_Test)\n",
    "    num_classes = len(np.unique(Label_Test))\n",
    "    accuracy_LDA_Test = sum(TP) / Size_tes\n",
    "\n",
    "    print('Round {}:'.format(r))\n",
    "    print('MAE train (mean,std) = ({:.4f},{:.4f}) \\\n",
    "    \\t MAE test = {:.4f} \\\n",
    "    \\t Accuracy in latent space = {:.2f}'.format(np.mean(np.array(MAE_train)), \\\n",
    "                                                 np.std(np.array(MAE_train)), MAE_test, accuracy_LDA_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75715b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
