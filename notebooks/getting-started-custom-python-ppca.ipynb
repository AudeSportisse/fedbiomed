{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e87007",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher to train a federated PPCA (Probabilistic PCA) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17d2f",
   "metadata": {},
   "source": [
    "## Description of the exercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb782b14",
   "metadata": {},
   "source": [
    "Three datasets `n1.csv` , `n2.csv` and `n3.csv` will be generated randomly using 3-views PPCA from a 4-dimensional latent space, with views dimensions [15,8,10] and 2 groups. Henceforth, we will distribute the 3 dataset to 3 distinct nodes and use Fed-mv-PPCA. In each center we check the evolution of expected LL during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0bc07",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We will generate three datasets using mv-PPCA.\n",
    "Then save them in a path of your choice on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48f8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "def sample_x_n(N:int, q:int, random_state:int=None):\n",
    "    \"\"\"samples from a Guassian dsitribution\n",
    "    Args:\n",
    "    \n",
    "    :N first dimension array\n",
    "    :q second dimesnion array\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(random_state).randn(N,q)\n",
    "\n",
    "def generate_data(N: int,\n",
    "                  W: np.ndarray,\n",
    "                  #a_g: np.ndarray,\n",
    "                  mu:float,\n",
    "                  sigma2:float,\n",
    "                  x_n,\n",
    "                  view:int,\n",
    "                  random_state=None):\n",
    "    \n",
    "    \"\"\"generates Gaussian dataset given several groups of data points, using the following\n",
    "    gausian generative proccess (for a given view):\n",
    "    \n",
    "    Y = WX + mu + epsilon with epsilon ~ N(0, sigma2)\n",
    "    \n",
    "    where X is the latent space of size (q, n_features), Y the observation matrix, W the matrix used\n",
    "    for data reconstructionY\n",
    "    \n",
    "    Params:\n",
    "    :N_g: List[int] number of data to generate per group (list of size number of group)\n",
    "    :W: reconstruction matrix, of size (n_features, q)\n",
    "    #:a_g: (np.ndarray) array of size (nb_group, n_components), introduces shift when creating different group. \n",
    "    :mu: offset of the dataset\n",
    "    :sigma2: variance used for generating\n",
    "    :x_n: random variable \n",
    "    :view: (int) the given view\n",
    "    :random_state: ransom seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    :Y (pd.DataFrame): synthetic dataset generated wrt above expression\n",
    "    of size (n_features, n_samples)\n",
    "    \"\"\"\n",
    "    rnd=np.random.RandomState(random_state)\n",
    "\n",
    "    #N=N_g.sum()\n",
    "    d, q = W.shape\n",
    "    sigma=np.sqrt(sigma2)\n",
    "    #G=len(N_g)\n",
    "\n",
    "    #g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    \n",
    "\n",
    "    y_n=np.empty((N, d))\n",
    "\n",
    "    #for g in range(G):\n",
    "    # computing Y = W.transpose(X) + mu\n",
    "    y_n[:]= np.einsum(\"dq,nq->nd\", W, x_n[:]) + mu\n",
    "        \n",
    "    y_n = pd.DataFrame(data=y_n,\n",
    "                     columns=[f'var_{view},{i + 1}' for i in range(d)])\n",
    "\n",
    "    return y_n + sigma*rnd.randn(N,d)\n",
    "\n",
    "#start\n",
    "\n",
    "def generate_ppca_nodes_dataset(n_nodes: int,\n",
    "                           n_features: Union[List[int],int],\n",
    "                           \n",
    "                           n_components: int,\n",
    "                               #n_group: int=2,\n",
    "                           absent_view: Dict[str, int]=None,\n",
    "                           W_init: List[np.ndarray]=None,\n",
    "                           mu_init: List[np.ndarray]=None,\n",
    "                           sigma_init : List[np.ndarray]=None,\n",
    "                           is_validation: bool=True,\n",
    "                           n_sample_validation: int=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for each node\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate PPCA parameters if not defined\n",
    "    ## case where W parameter is not defined\n",
    "    if W_init is None:\n",
    "        W_init = []\n",
    "        for i in range(n_nodes):\n",
    "            W_gen = np.random.uniform(-10, 10, (n_features[i], n_components))\n",
    "            W_init.append(W_gen)\n",
    "    ## case where mu not generated\n",
    "    if mu_init is None:\n",
    "        mu_init = []\n",
    "        for i in range(n_nodes):\n",
    "            mu_gen = np.random.uniform(-10, 10, n_features[i])\n",
    "            mu_init.append(mu_gen)\n",
    "            \n",
    "    ## case where sigma is not definied (we will set sigma =1 for each clients)\n",
    "    if sigma_init is None:\n",
    "        sigma_init = []\n",
    "        for i in range(n_nodes):\n",
    "            sigma_init.append(1)\n",
    "            \n",
    "    \n",
    "    #shift = np.concatenate((np.zeros((1, n_components)),\n",
    "    #                      np.random.uniform(-10, 10, (nb_group - 1, n_components))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc74ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "    \n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "    \n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "    \n",
    "    for key in datasets.keys():\n",
    "        #_sub_dataframe_header.append(list(datasets[key].columns.values))\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # other pass\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate([_concatenated_datasets,\n",
    "                                                         datasets[key].to_numpy()],\n",
    "                                                        axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError('Cannot create multi view dataset: different number of samples for each modality have been detected')\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array, _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. create multi index dataframe\n",
    "    \n",
    "    mulit_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                 columns = _header)\n",
    "    return mulit_view_df\n",
    "\n",
    "\n",
    "def save_multi_view_dataframe(dataframe: pd.DataFrame, file_name: str):\n",
    "    dataframe.to_csv(file_name)\n",
    "    \n",
    "def load_multi_view_dataframe(file_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_name, delimiter=',', index_col=0, header=[0,1])\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a9fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "#nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14ab247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initializing PPCA variables\n",
    "sigma2_gen1, sigma2_gen2, sigma2_gen3 = 2, 1, 3\n",
    "W_gen1 = np.random.uniform(-10, 10, (D_i[0], n_components_generated))\n",
    "W_gen2 = np.random.uniform(-5, 5, (D_i[1], n_components_generated))\n",
    "W_gen3 = np.random.uniform(-15, 15, (D_i[2], n_components_generated))\n",
    "mu_gen1 = np.random.uniform(-10, 10, D_i[0])\n",
    "mu_gen2 = np.random.uniform(-5, 5, D_i[1])\n",
    "mu_gen3 = np.random.uniform(-15, 15, D_i[2])\n",
    "\n",
    "#a_g_gen = np.concatenate((np.zeros((1, n_components_generated)),\n",
    "#                          np.random.uniform(-10, 10, (nb_group - 1, n_components_generated))))\n",
    "\n",
    "W = [W_gen1,W_gen2,W_gen3]\n",
    "mu = [mu_gen1,mu_gen2,mu_gen3]\n",
    "sigma = [sigma2_gen1,sigma2_gen2,sigma2_gen3]\n",
    "\n",
    "# absent_views contains as key the id of a center in which we want to simulate absent views,\n",
    "# and as argumet the id of the missing view. \n",
    "absent_views = {'2': 2}\n",
    "\n",
    "for i in range(n_centers):\n",
    "    # N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)]) \n",
    "    # g_ind = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    N = np.random.randint(50,600)\n",
    "    x_n_gen = sample_x_n(N, n_components_generated, random_state=150)  # randomly generate a Gaussian\n",
    "    # dataset\n",
    "    #Y = []\n",
    "    Y = {}\n",
    "    for d in range(len(D_i)):\n",
    "        y_t = generate_data(N, W[d], mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "        if ((str(i+1) in absent_views.keys()) \\\n",
    "            and (type(absent_views[str(i+1)])== int) \\\n",
    "            and (absent_views[str(i+1)]==d+1)):\n",
    "            absent_views.update({str(i+1): y_t})\n",
    "            y_abs=pd.DataFrame(np.nan, index = np.arange(N), \\\n",
    "                               columns = [f'var_{d+1},{i + 1}' for i in range(D_i[d])])\n",
    "            #Y.append(y_abs)\n",
    "            Y['view_' + str(d+1)] = y_abs\n",
    "        else:\n",
    "            # Y.append(y_t)\n",
    "            Y['view_' + str(d+1)] = y_t\n",
    "\n",
    "    #gr = []\n",
    "    #for g in range(nb_group):\n",
    "    #   gr += [int(g) for _ in range(N_g[g])]\n",
    "    #gr = pd.Series(gr)\n",
    "    #Y['Label'] = pd.DataFrame(gr, columns=['Labels'])\n",
    "\n",
    "###\n",
    "# the output will be a list of n_centers dataset containing: dataframe for each centers,of \n",
    "# different dimensions\n",
    "# \n",
    "    #t_i = pd.concat(Y, axis=1)\n",
    "    \n",
    "    #t_i.columns.values[-1] = 'Label'\n",
    "    t_i = create_multi_view_dataframe(Y)\n",
    "    # t_i.to_csv('../../data/PPCA/ppca-' + str(i+1) + '.csv',sep=',')\n",
    "    t_i.to_csv('== Local path to node' + str(i+1) + '.csv',sep=',')\n",
    "    #np.savetxt('== Local path to node' + str(i+1) + '.csv',t_i,delimiter=',')\n",
    "               \n",
    "# building the test dataset\n",
    "# N_g_test = np.array([testing_samples//2,testing_samples//2])\n",
    "# g_ind_test = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g_test)))\n",
    "N_test = testing_samples\n",
    "x_n_gen = sample_x_n(N_test, n_components_generated, random_state=150)\n",
    "Y_test = {}\n",
    "for d in range(len(D_i)):\n",
    "    y_t = generate_data(N_test, W[d], mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "    Y_test['view_' + str(d+1)] = y_t\n",
    "\n",
    "#gr_test = [0 for _ in range(N_g_test[0])]+[1 for _ in range(N_g_test[1])]\n",
    "#gr_test = pd.Series(gr_test)\n",
    "#Y_test['Label'] = pd.DataFrame(gr_test, columns=['Labels'])\n",
    "\n",
    "t_test = pd.concat(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f160e",
   "metadata": {},
   "source": [
    "## Start the network and setting the client up\n",
    "Before running this notebook:\n",
    "1. You should start the network from fedbiomed-network, as detailed in :\n",
    "https://gitlab.inria.fr/fedbiomed/fedbiomed\n",
    "\n",
    "2. You need to configure at least 2 nodes: <br/>\n",
    "* **Node 1 :** `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'ppca_data' always and it will be good)\n",
    "  * Pick the .csv file where you stored t_0.\n",
    "  * Check that your data has been added in node 1 by executing `./scripts/fedbiomed_run node list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node start`. <br/>\n",
    "\n",
    "* **Node 2 :** Open a second terminal and run ./scripts/fedbiomed_run node add config n2.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'ppca_data' always and it will be good)\n",
    "  * Pick the .csv file where you stored t_1.\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n2.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n2.ini start`.\n",
    "  \n",
    "\n",
    "\n",
    "* **Node 3 :** Open a third terminal and run ./scripts/fedbiomed_run node add config n3.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'ppca_data' always and it will be good)\n",
    "  * Pick the .csv file where you stored t_2.\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n3.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n3.ini start `.\n",
    "\n",
    " Wait until you get `Connected with result code 0`. it means node is online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade4cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c80070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fedbiomed.researcher.environ import TMP_DIR\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=TMP_DIR+'/')\n",
    "model_file = tmp_dir_model.name + '/fed_mv_ppca.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9aaa87",
   "metadata": {},
   "source": [
    "Hereafter the template of the class you should provide to Fedbiomed :\n",
    "       \n",
    "**training_data** : you must return here a tuple (X,X_k,ViewsX,y) or (X,X_k,ViewsX). Note that all centers should provide a dataset with the same view-specific columns. If in a specific center a view has not been observed, then the corresponding columns will be filled of nan. The training_data method take care of identifying view-specific sub-datasets and collecting information concerning non-available observations. Data can also been normalized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10cc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/tmp/tmpftbo2e4w/fed_mv_ppca.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Fed_MV_PPCA(PpcaPlan):\n",
    "    def __init__(self, kwargs):\n",
    "        super(Fed_MV_PPCA, self).__init__(kwargs)\n",
    "        deps = ['import numpy as np', \n",
    "               'import pandas as pd']\n",
    "        self.add_dependency(deps)\n",
    "        self.multi_view = True\n",
    "    \n",
    "    def training_data(self):\n",
    "        \"\"\"\n",
    "            Perform in this method all data reading and data transformations you need.\n",
    "            At the end you should provide a tuple (X_obs,Xk,ViewsX,y), where: \n",
    "            X_obs is the training dataset, \n",
    "            Xk is a list containing the k-specific dataframe if it exists or 'NaN' otherwise,\n",
    "            ViewsX is the indicator function for observed views (ViewsX[k]=1 if view k is observed, 0 otherwise)\n",
    "            y the corresponding labels (optional)\n",
    "            The dataset is normalized using min max scaler if model_args['norm'] is true\n",
    "            Note: since labels are not needed for the optimization, \n",
    "            training_data can also simply return (X_obs,Xk,ViewsX)\n",
    "            :raise NotImplementedError if researcher do not implement this method.\n",
    "        \"\"\"\n",
    "        \n",
    "        #dataset = pd.read_csv(self.dataset_path,delimiter=',', index_col=0)\n",
    "        #dataset = self.load_multi_view_dataframe(self.dataset_path)\n",
    "        #X = dataset.iloc[:,:-1]\n",
    "        #y = dataset[dataset.columns[-1]]\n",
    "        X = self.load_multi_view_dataframe(self.dataset_path)\n",
    "        \n",
    "        # Xk is a list contaning the view-specific local datasets\n",
    "        Xk = []\n",
    "        Xk_obs = []\n",
    "        ViewsX = []\n",
    "        for k in range(self.K):\n",
    "            if ((self.views_id[k] not in X) or (X[self.views_id[k]].isnull().values.any())):\n",
    "                Xk.append(np.nan)\n",
    "                #Xk.append('NaN')\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                # X_k = X.iloc[:, ind:ind + self.dim_views[k]]\n",
    "                X_k = X[self.views_id[k]]\n",
    "                if self.is_norm:\n",
    "                    X_k = self.normalize_data(X_k)                \n",
    "                Xk.append(X_k)\n",
    "                Xk_obs.append(X_k)                \n",
    "                ViewsX.append(1)\n",
    "            \n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        #Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        # Xk_obs = [item for item in Xk if type(item) is not str]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX)\n",
    "    \n",
    "    def load_multi_view_dataframe(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(file_name, delimiter=',', index_col=0, header=[0,1])\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cda129",
   "metadata": {},
   "source": [
    "**model_args** is a dictionary containing the mv-ppca model arguments: the total number of views across all datasets (tot_views), the dimension of each view (dim_views), the latent space size (n_components), and a boolean (norm) for data preprocessing. Additionaly, the researcher can provide priors for one ore more global parameters.\n",
    "\n",
    "**training_args** contains here the number of local iterations for EM/MAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05aa5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_id = ['view_1','view_2','view_3']\n",
    "tot_views = 3\n",
    "dim_views = [15, 8, 10]\n",
    "n_components = 4\n",
    "norm = True\n",
    "\n",
    "model_args = {'views_id': views_id,'tot_views': tot_views, 'dim_views': dim_views, 'n_components': n_components, 'is_norm': norm}\n",
    "\n",
    "# better to increase log interval if number of iteration is higher\n",
    "training_args = {'n_iterations': 15, 'log_interval' : 1} #\n",
    "\n",
    "\n",
    "tags =  ['ppca_data']\n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b1a1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 16:57:51,986 fedbiomed INFO - Messaging researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x15740b310>\n",
      "2021-11-05 16:57:52,114 fedbiomed INFO - Searching dataset with data tags: ['ppca_data'] for all nodes\n",
      "2021-11-05 16:57:52,169 fedbiomed INFO - log from: node_5a749bac-ebb6-4acc-ae7b-fdf91cbec87d - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'tags': ['ppca_data'], 'command': 'search'}\n",
      "2021-11-05 16:57:52,193 fedbiomed INFO - log from: node_6909f8bc-5d7c-4b4e-9a67-651de68ae2c9 - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'tags': ['ppca_data'], 'command': 'search'}\n",
      "2021-11-05 16:57:52,508 fedbiomed INFO - log from: node_77fbcf1f-c0fc-4730-9f5a-85b6ffa6bb91 - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'tags': ['ppca_data'], 'command': 'search'}\n",
      "2021-11-05 16:58:02,129 fedbiomed INFO - Checking data quality of federated datasets...\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.ppca_aggregator import MLaggregator\n",
    "\n",
    "\n",
    "\n",
    "# select nodes pr into task <Task pending name='Task-27' coro=<HTTP1ServerConnection._server_request_loop() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/http1connection.py:823> wait_for=<Future finished result=b'GET /kernel...6bd7\"\\r\\n\\r\\n'> cb=[IOLoop.add_future.<locals>.<lambda>() at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/ioloop.py:688]> while another task <Task pending name='Task-2' coro=<KernelManager._async_start_kernel() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/jupyter_carticiping to this experiment\n",
    "exp = Experiment(tags=tags,\n",
    "                 #clients=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='Fed_MV_PPCA',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=MLaggregator(),\n",
    "                 client_selection_strategy=None,\n",
    "                 tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6450b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 16:59:20,522 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2021-11-05 16:59:20,560 fedbiomed INFO - log from: node_5a749bac-ebb6-4acc-ae7b-fdf91cbec87d - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'command': 'list'}\n",
      "2021-11-05 16:59:20,569 fedbiomed INFO - log from: node_6909f8bc-5d7c-4b4e-9a67-651de68ae2c9 - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'command': 'list'}\n",
      "2021-11-05 16:59:20,575 fedbiomed INFO - log from: node_77fbcf1f-c0fc-4730-9f5a-85b6ffa6bb91 - DEBUG Message received: {'researcher_id': 'researcher_1d8e4b13-c8cc-4a47-a8db-d6b3a2e64cce', 'command': 'list'}\n",
      "2021-11-05 16:59:30,543 fedbiomed INFO - \n",
      " Node: node_5a749bac-ebb6-4acc-ae7b-fdf91cbec87d | Number of Datasets: 1 \n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "| name   | data_type   | tags          | description   | shape     | multi_view   |\n",
      "+========+=============+===============+===============+===========+==============+\n",
      "|        | csv         | ['ppca_data'] |               | [173, 34] | multi_view   |\n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "\n",
      "2021-11-05 16:59:30,545 fedbiomed INFO - \n",
      " Node: node_6909f8bc-5d7c-4b4e-9a67-651de68ae2c9 | Number of Datasets: 1 \n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "| name   | data_type   | tags          | description   | shape     | multi_view   |\n",
      "+========+=============+===============+===============+===========+==============+\n",
      "|        | csv         | ['ppca_data'] |               | [256, 34] | multi_view   |\n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "\n",
      "2021-11-05 16:59:30,547 fedbiomed INFO - \n",
      " Node: node_77fbcf1f-c0fc-4730-9f5a-85b6ffa6bb91 | Number of Datasets: 1 \n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "| name   | data_type   | tags          | description   | shape     | multi_view   |\n",
      "+========+=============+===============+===============+===========+==============+\n",
      "|        | csv         | ['ppca_data'] |               | [134, 34] | multi_view   |\n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "\n",
      "2021-11-05 17:00:04,183 fedbiomed INFO - log from: node_5a749bac-ebb6-4acc-ae7b-fdf91cbec87d - CRITICAL Node stopped in signal_handler, probably by user decision (Ctrl C)\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "\n",
    "req = Requests()\n",
    "datasets = req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import TENSORBOARD_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db565a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601818e",
   "metadata": {},
   "source": [
    "Following cell will launch tensorboard on `TENSORBOARD_RESULTS_DIR`. Since the experiment is not started, it won't show any results. After runing experiment, you can click refresh button to see changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"$TENSORBOARD_RESULTS_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start federated training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a782",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918cbe3",
   "metadata": {},
   "source": [
    "Herafter we test the performance of the aggregated parameters on a test dataset. In particular, for each round we use the global parameters to evaluate the mean absolute error and the separation in the latent space using LDA. Note that we have already defined the test dataset at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import solve\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_MB(Wk, q, D_i, K, Sigma2, ViewsX):\n",
    "    \"\"\"\n",
    "    Computes matrices M:=inv(I_q+sum_k Wk.TWk/sigma2k) and B:= [W1.T/sigma2K,...,W1.T/sigma2K].\n",
    "    :param Wk: list of matrices (d_k x q)\n",
    "    :param Sigma2: list of float > 0\n",
    "    :return np.arrays\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    M1 = Wk[index].reshape(D_i[index], q).T.dot(Wk[index].reshape(D_i[index],q)) / Sigma2[index]\n",
    "    B = Wk[index].reshape(D_i[index], q).T / Sigma2[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            M1 += Wk[k].reshape(D_i[k], q).T.dot(Wk[k].reshape(D_i[k],q)) / Sigma2[k]\n",
    "            B = np.concatenate((B, (Wk[k].reshape(D_i[k], q)).T / Sigma2[k]), axis=1)\n",
    "\n",
    "    M = solve(np.eye(q) + M1,np.eye(q))\n",
    "\n",
    "    return M, B\n",
    "\n",
    "def concat_params(park, K, ViewsX):\n",
    "    \"\"\"\n",
    "    This function concatenates parameters from a list\n",
    "    :param park: list of vectors/matrices to concatenate\n",
    "    :return np.array\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    par = park[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            par = np.concatenate((par, park[k]), axis=0)\n",
    "\n",
    "    return par\n",
    "[]\n",
    "def simu_latent(q,dataset,ViewsX,global_params):\n",
    "    \"\"\"\n",
    "    This function allows sampling of x_n (latent variables) from the posterior distribution \n",
    "    (with global parameters).\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "\n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    Xn = [(M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(1, q) for n in range(N)]\n",
    "\n",
    "    df = pd.DataFrame(np.vstack(Xn), index=dataset.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def MAE(dataset,ViewsX,q,global_params):\n",
    "    \"\"\"\n",
    "    This function evaluates the MAE using global parameters\n",
    "    :return float\n",
    "    \"\"\"    \n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "    \n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    W = concat_params(global_params['tilde_Wk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "    \n",
    "    T_true = dataset.values.tolist()\n",
    "\n",
    "    T_pred = []\n",
    "    for n in range(N):\n",
    "        Xng = (M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(q, 1)\n",
    "        T_pred.append((W.dot(Xng) + mu).reshape(d))\n",
    "    \n",
    "    MAE = mean_absolute_error(T_true, T_pred)\n",
    "    return MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be44826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "\n",
    "ppca = PpcaPlan(model_args)\n",
    "ppca.is_multi_view = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test datasetnp.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4\n",
    "\n",
    "\n",
    "def test_data(ppca, dataset,norm,K,dim_views):\n",
    "        \"\"\"\n",
    "            Equivalent to training_data, for the test dataset\n",
    "        \"\"\"\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append(np.nan)\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = X.iloc[:, ind:ind + dim_views[k]]\n",
    "                if norm:\n",
    "                    \n",
    "                    X_k = ppca.normalize_data(X_k)\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "for r in range(1,rounds):\n",
    "    global_params = exp.aggregated_params[r]['params']\n",
    "    ######## Train data\n",
    "    MAE_train = []\n",
    "    Latent_Train = pd.DataFrame()\n",
    "    Label_Train = pd.Series(dtype='int64')\n",
    "    for c in range(n_centers):\n",
    "        dataset_c = load_multi_view_dataframe('== Local path to node' + str(c+1) + '.csv')\n",
    "        \n",
    "        X_obs_c,Xk_c,ViewsX_c,y_c = test_data(ppca, dataset_c,norm,tot_views,dim_views)\n",
    "        # Dataframe of latent space for LDA\n",
    "        Latent_Train = Latent_Train.append(simu_latent(n_components,X_obs_c,ViewsX_c,global_params))\n",
    "        Label_Train = Label_Train.append(y_c)\n",
    "        # MAE Train\n",
    "        MAE_train.append(MAE(X_obs_c,ViewsX_c,n_components,global_params))\n",
    "\n",
    "    \n",
    "\n",
    "    ######## Test data\n",
    "    \n",
    "    X_obs_test,Xk_test,ViewsX_test,y_test = test_data(ppca, t_test,norm,tot_views,dim_views)\n",
    "\n",
    "    Latent_Test = pd.DataFrame()\n",
    "    Label_Test = pd.Series(dtype='int64')\n",
    "\n",
    "    # Dataframe of latent space for LDA\n",
    "    Latent_Test = Latent_Test.append(simu_latent(n_components,X_obs_test,ViewsX_test,global_params))\n",
    "    Label_Test = Label_Test.append(y_test)\n",
    "    # MAE Test\n",
    "    MAE_test = MAE(X_obs_test,ViewsX_test,n_components,global_params)\n",
    "\n",
    "\n",
    "    print('Round {}:'.format(r))\n",
    "    print('MAE train (mean,std) = ({:.4f},{:.4f}) \\\n",
    "    \\t MAE test = {:.4f} \\\n",
    "    '.format(np.mean(np.array(MAE_train)), \\\n",
    "      np.std(np.array(MAE_train)), MAE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36770ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
