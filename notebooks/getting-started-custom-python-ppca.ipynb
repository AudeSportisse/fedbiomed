{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e87007",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher to train a federated PPCA (Probabilistic PCA) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17d2f",
   "metadata": {},
   "source": [
    "## Description of the exercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb782b14",
   "metadata": {},
   "source": [
    "Three datasets `n1.csv` , `n2.csv` and `n3.csv` will be generated randomly using 3-views PPCA from a 4-dimensional latent space, with views dimensions [15,8,10] and 2 groups. Henceforth, we will distribute the 3 dataset to 3 distinct nodes and use Fed-mv-PPCA. In each center we check the evolution of expected LL during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0bc07",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We will generate three datasets using mv-PPCA.\n",
    "Then save them in a path of your choice on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48f8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "def sample_x_n(N:int, q:int, random_state:int=None):\n",
    "    \"\"\"samples from a Guassian dsitribution\n",
    "    Args:\n",
    "    \n",
    "    :N first dimension array\n",
    "    :q second dimesnion array\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(random_state).randn(N,q)\n",
    "\n",
    "def generate_data(N_g: List[int],\n",
    "                  W: np.ndarray,\n",
    "                  a_g: np.ndarray,\n",
    "                  mu:float,\n",
    "                  sigma2:float,\n",
    "                  x_n,\n",
    "                  view:int,\n",
    "                  random_state=None):\n",
    "    \n",
    "    \"\"\"generates Gaussian dataset given several groups of data points, using the following\n",
    "    gausian generative proccess (for a given view):\n",
    "    \n",
    "    Y = WX + mu + epsilon with epsilon ~ N(0, sigma2)\n",
    "    \n",
    "    where X is the latent space of size (q, n_features), Y the observation matrix, W the matrix used\n",
    "    for data reconstructionY\n",
    "    \n",
    "    Params:\n",
    "    :N_g: List[int] number of data to generate per group (list of size number of group)\n",
    "    :W: reconstruction matrix, of size (n_features, q)\n",
    "    :a_g: (np.ndarray) array of size (nb_group, n_components), introduces shift when creating different group. \n",
    "    :mu: offset of the dataset\n",
    "    :sigma2: variance used for generating\n",
    "    :x_n: random variable \n",
    "    :view: (int) the given view\n",
    "    :random_state: ransom seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    :Y (pd.DataFrame): synthetic dataset generated wrt above expression\n",
    "    of size (n_features, n_samples)\n",
    "    \"\"\"\n",
    "    rnd=np.random.RandomState(random_state)\n",
    "\n",
    "    N=N_g.sum()\n",
    "    d, q = W.shape\n",
    "    sigma=np.sqrt(sigma2)\n",
    "    G=len(N_g)\n",
    "\n",
    "    g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    \n",
    "\n",
    "    y_n=np.empty((N, d))\n",
    "\n",
    "    for g in range(G):\n",
    "        # computing Y = W.transpose(X + shift()) + mu\n",
    "        y_n[g_ind[g]:g_ind[g+1]]= np.einsum(\"dq,nq->nd\", W, x_n[g_ind[g]:g_ind[g+1]]+a_g[g]) + mu\n",
    "        \n",
    "    y_n = pd.DataFrame(data=y_n,\n",
    "                     columns=[f'var_{view},{i + 1}' for i in range(d)])\n",
    "\n",
    "    return y_n + sigma*rnd.randn(N,d)\n",
    "\n",
    "\n",
    "\n",
    "def generate_ppca_nodes_dataset(n_nodes: int,\n",
    "                           n_features: Union[List[int],int],\n",
    "                           \n",
    "                           n_components: int,\n",
    "                           n_group: int=2,\n",
    "                           absent_view: Dict[str, int]=None,\n",
    "                           W_init: List[np.ndarray]=None,\n",
    "                           mu_init: List[np.ndarray]=None,\n",
    "                           sigma_init : List[np.ndarray]=None,\n",
    "                           is_validation: bool=True,\n",
    "                           n_sample_validation: int=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for each node\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate PPCA parameters if not defined\n",
    "    ## case where W parameter is not defined\n",
    "    if W_init is None:\n",
    "        W_init = []\n",
    "        for i in range(n_nodes):\n",
    "            W_gen = np.random.uniform(-10, 10, (n_features[i], n_components))\n",
    "            W_init.append(W_gen)\n",
    "    ## case where mu not generated\n",
    "    if mu_init is None:\n",
    "        mu_init = []\n",
    "        for i in range(n_nodes):\n",
    "            mu_gen = np.random.uniform(-10, 10, n_features[i])\n",
    "            mu_init.append(mu_gen)\n",
    "            \n",
    "    ## case where sigma is not definied (we will set sigma =1 for each clients)\n",
    "    if sigma_init is None:\n",
    "        sigma_init = []\n",
    "        for i in range(n_nodes):\n",
    "            sigma_init.append(1)\n",
    "            \n",
    "    \n",
    "    shift = np.concatenate((np.zeros((1, n_components)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc74ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "    \n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "    \n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "    \n",
    "    for key in datasets.keys():\n",
    "        #_sub_dataframe_header.append(list(datasets[key].columns.values))\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # other pass\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate([_concatenated_datasets,\n",
    "                                                         datasets[key].to_numpy()],\n",
    "                                                        axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError('Cannot create multi view dataset: different number of samples for each modality have been detected')\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array, _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. create multi index dataframe\n",
    "    \n",
    "    mulit_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                 columns = _header)\n",
    "    return mulit_view_df\n",
    "\n",
    "\n",
    "def save_multi_view_dataframe(dataframe: pd.DataFrame, file_name: str):\n",
    "    dataframe.to_csv(file_name)\n",
    "    \n",
    "def load_multi_view_dataframe(file_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_name, delimiter=',', index_col=0, header=[0,1])\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a9fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14ab247",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16261/1000944643.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_g_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_n_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         if ((str(i+1) in absent_views.keys()) \\\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsent_views\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initializing PPCA variables\n",
    "sigma2_gen1, sigma2_gen2, sigma2_gen3 = 2, 1, 3\n",
    "W_gen1 = np.random.uniform(-10, 10, (D_i[0], n_components_generated))\n",
    "W_gen2 = np.random.uniform(-5, 5, (D_i[1], n_components_generated))\n",
    "W_gen3 = np.random.uniform(-15, 15, (D_i[2], n_components_generated))\n",
    "mu_gen1 = np.random.uniform(-10, 10, D_i[0])\n",
    "mu_gen2 = np.random.uniform(-5, 5, D_i[1])\n",
    "mu_gen3 = np.random.uniform(-15, 15, D_i[2])\n",
    "\n",
    "a_g_gen = np.concatenate((np.zeros((1, n_components_generated)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components_generated))))\n",
    "\n",
    "#W = [W_gen1,W_gen2,W_gen3]\n",
    "W = {'view_1': W_gen1, \n",
    "    'view_2': W_gen2,\n",
    "    'view_3': W_gen3}\n",
    "mu = [mu_gen1,mu_gen2,mu_gen3]\n",
    "mu = {'view_1': mu_gen1,\n",
    "     'view_2':mu_gen2,\n",
    "     'view_3': mu_gen3}\n",
    "sigma = [sigma2_gen1,sigma2_gen2,sigma2_gen3]\n",
    "\n",
    "sigma = {'view_1': sigma2_gen1,\n",
    "        'view_2':sigma2_gen2,\n",
    "        'view_3':sigma2_gen3}\n",
    "# absent_views contains as key the id of a center in which we want to simulate absent views,\n",
    "# and as argumet the id of the missing view. \n",
    "absent_views = {'2': 2}\n",
    "\n",
    "for i in range(n_centers):\n",
    "    N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)]) \n",
    "    # N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)])?\n",
    "    #g_ind = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    N = N_g.sum()\n",
    "    x_n_gen = sample_x_n(N, n_components_generated, random_state=150)  # randomly generate a Gaussian\n",
    "    # dataset\n",
    "    #Y = []\n",
    "    Y = {}\n",
    "    for d in range(len(D_i)):\n",
    "        y_t = generate_data(N_g, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "        if ((str(i+1) in absent_views.keys()) \\\n",
    "            and (type(absent_views[str(i+1)])== int) \\\n",
    "            and (absent_views[str(i+1)]==d+1)):\n",
    "            absent_views.update({str(i+1): y_t})\n",
    "            y_abs=pd.DataFrame(np.nan, index = np.arange(N_g.sum()), \\\n",
    "                               columns = [f'var_{d+1},{i + 1}' for i in range(D_i[d])])\n",
    "            #Y.append(y_abs)\n",
    "            Y['view_' + str(d+1)] = y_abs\n",
    "        else:\n",
    "            # Y.append(y_t)\n",
    "            Y['view_' + str(d+1)] = y_t\n",
    "\n",
    "    gr = []\n",
    "    for g in range(nb_group):\n",
    "       gr += [int(g) for _ in range(N_g[g])]\n",
    "    gr = pd.Series(gr)\n",
    "    Y['Label'] = pd.DataFrame(gr, columns=['Labels'])\n",
    "\n",
    "###\n",
    "# the output will be a list of n_centers dataset containing: dataframe for each centers,of \n",
    "# different dimensions\n",
    "# \n",
    "    #t_i = pd.concat(Y, axis=1)\n",
    "    \n",
    "    #t_i.columns.values[-1] = 'Label'\n",
    "    t_i = create_multi_view_dataframe(Y)\n",
    "    t_i.to_csv('== Local path to node' + str(i+1) + '.csv',sep=',')\n",
    "    #np.savetxt('== Local path to node' + str(i+1) + '.csv',t_i,delimiter=',')\n",
    "               \n",
    "# building the test dataset\n",
    "N_g_test = np.array([testing_samples//2,testing_samples//2])\n",
    "g_ind_test = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g_test)))\n",
    "N_test = N_g_test.sum()\n",
    "x_n_gen = sample_x_n(N_test, n_components_generated, random_state=150)\n",
    "Y_test = {}\n",
    "for d in range(len(D_i)):\n",
    "    y_t = generate_data(N_g_test, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "    Y_test['view_' + str(d+1)] = y_t\n",
    "\n",
    "gr_test = [0 for _ in range(N_g_test[0])]+[1 for _ in range(N_g_test[1])]\n",
    "gr_test = pd.Series(gr_test)\n",
    "Y_test['Label'] = pd.DataFrame(gr_test, columns=['Labels'])\n",
    "\n",
    "t_test = pd.concat(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9558c9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['view_2', 'view_3', 'Label', 'view_1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = list(set(t_test.columns.get_level_values(0)))\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeba9c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 8)\n",
      "(40, 10)\n",
      "(40, 1)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "for view in views:\n",
    "    print(t_test[view].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test2 = t_test.drop('Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_abs=pd.DataFrame(np.nan, index = np.arange(N_g.sum()), \\\n",
    "                               columns = [f'var_{d+1},{i + 1}' for i in range(D_i[d])])\n",
    "\n",
    "y_abs.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be208d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "views = list(t_test.columns.levels[0])\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test.iloc[:,:-1].columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77337219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "\n",
    "ppca = PpcaPlan(model_args)\n",
    "ppca.is_multi_view = True\n",
    "\n",
    "\n",
    "def parse_input_values(\n",
    "                       dataset: pd.DataFrame,\n",
    "                       target: pd.Series = None) -> Tuple[pd.DataFrame,\n",
    "                                                          List[pd.DataFrame],\n",
    "                                                          List[int]]:\n",
    "    Xk = []\n",
    "    ViewsX = []\n",
    "    ind = 0\n",
    "    K = 3\n",
    "    k = 0\n",
    "    is_multi_view = True\n",
    "    dim_views = [15, 8, 10]\n",
    "    is_norm = True\n",
    "    if is_multi_view:\n",
    "        # get a list of all views\n",
    "        iterator = list(sorted(set(dataset.columns.get_level_values(0))))\n",
    "        pandas_handler = lambda df, x: df[x]\n",
    "    else:\n",
    "        iterator = range(self.K)\n",
    "        pandas_handler = lambda df, x: df.iloc[:, ind:ind + self.dim_views[x]]\n",
    "    # iterate over number of views\n",
    "    for iter_elem in iterator:\n",
    "        if pandas_handler(dataset, iter_elem).isnull().values.any():\n",
    "            Xk.append(np.nan)\n",
    "            #Xk.append('NaN')\n",
    "            ViewsX.append(0)\n",
    "            \n",
    "        else:\n",
    "            # if norm = true, data are normalized with min max scaler\n",
    "            X_k = pandas_handler(dataset, iter_elem)\n",
    "            if is_norm:\n",
    "                X_k = ppca.normalize_data(X_k) \n",
    "\n",
    "\n",
    "            Xk.append(X_k)\n",
    "\n",
    "            ViewsX.append(1)\n",
    "        \n",
    "        ind += dim_views[k]\n",
    "        k += 1\n",
    "     # The entire dataset is re-built without empty columns\n",
    "    Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "    #Xk_obs = [item for item in Xk if type(item) is not str]\n",
    "    X_obs = pd.concat(Xk_obs, axis=1)\n",
    "\n",
    "    return (X_obs,Xk,ViewsX, target)\n",
    "parse_input_values(t_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_eval(dataset):\n",
    "    Xk = []\n",
    "    ViewsX = []\n",
    "    ind = 0\n",
    "    K = 3\n",
    "    dim_views = [15, 8, 10]\n",
    "    \n",
    "    X = dataset.iloc[:,:-1]\n",
    "    for k in range(K):\n",
    "        if X.iloc[:, ind].isnull().values.any():\n",
    "            Xk.append(np.nan)\n",
    "            #Xk.append('NaN')\n",
    "            ViewsX.append(0)\n",
    "        else:\n",
    "            # if norm = true, data are normalized with min max scaler\n",
    "            X_k = X.iloc[:, ind:ind + dim_views[k]]\n",
    "            if ppca.is_norm:\n",
    "                X_k = ppca.normalize_data(X_k) \n",
    "\n",
    "\n",
    "            Xk.append(X_k)\n",
    "\n",
    "            ViewsX.append(1)\n",
    "        ind += dim_views[k]\n",
    "\n",
    "\n",
    "    # The entire dataset is re-built without empty columns\n",
    "    Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "    #Xk_obs = [item for item in Xk if type(item) is not str]\n",
    "    X_obs = pd.concat(Xk_obs, axis=1)\n",
    "\n",
    "    return (X_obs,Xk,ViewsX,None)\n",
    "\n",
    "f_eval(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b75081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in sorted(list(set(t_test2.columns.get_level_values(0)))):\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f160e",
   "metadata": {},
   "source": [
    "## Start the network and setting the client up\n",
    "Before running this notebook:\n",
    "1. You should start the network from fedbiomed-network, as detailed in :\n",
    "https://gitlab.inria.fr/fedbiomed/fedbiomed\n",
    "\n",
    "2. You need to configure at least 2 nodes: <br/>\n",
    "* **Node 1 :** `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[0],y[0].\n",
    "  * Check that your data has been added in node 1 by executing `./scripts/fedbiomed_run node list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node start`. <br/>\n",
    "\n",
    "* **Node 2 :** Open a second terminal and run ./scripts/fedbiomed_run node add config n2.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[1],y[1].\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n2.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n2.ini start`.\n",
    "  \n",
    "\n",
    "\n",
    "* **Node 3 :** Open a third terminal and run ./scripts/fedbiomed_run node add config n3.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[2],y[2].\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n3.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n3.ini start `.\n",
    "\n",
    " Wait until you get `Connected with result code 0`. it means node is online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade4cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c80070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fedbiomed.researcher.environ import TMP_DIR\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=TMP_DIR+'/')\n",
    "model_file = tmp_dir_model.name + '/fed_mv_ppca.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9aaa87",
   "metadata": {},
   "source": [
    "Hereafter the template of the class you should provide to Fedbiomed :\n",
    "       \n",
    "**training_data** : you must return here a tuple (X,X_k,ViewsX,y) or (X,X_k,ViewsX). Note that all centers should provide a dataset with the same view-specific columns. If in a specific center a view has not been observed, then the corresponding columns will be filled of nan. The training_data method take care of identifying view-specific sub-datasets and collecting information concerning non-available observations. Data can also been normalized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10cc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ybouilla/fedbiomed/var/tmp/tmplwgpt5vc/fed_mv_ppca.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Fed_MV_PPCA(PpcaPlan):\n",
    "    def __init__(self, kwargs):\n",
    "        super(Fed_MV_PPCA, self).__init__(kwargs)\n",
    "        deps = ['import numpy as np', \n",
    "               'import pandas as pd']\n",
    "        self.add_dependency(deps)\n",
    "        self.multi_view = True\n",
    "    \n",
    "    def training_data(self):\n",
    "        \"\"\"\n",
    "            Perform in this method all data reading and data transformations you need.\n",
    "            At the end you should provide a tuple (X_obs,Xk,ViewsX,y), where: \n",
    "            X_obs is the training dataset, \n",
    "            Xk is a list containing the k-specific dataframe if it exists or 'NaN' otherwise,\n",
    "            ViewsX is the indicator function for observed views (ViewsX[k]=1 if view k is observed, 0 otherwise)\n",
    "            y the corresponding labels (optional)\n",
    "            The dataset is normalized using min max scaler if model_args['norm'] is true\n",
    "            Note: since labels are not needed for the optimization, \n",
    "            training_data can also simply return (X_obs,Xk,ViewsX)\n",
    "            :raise NotImplementedError if researcher do not implement this method.\n",
    "        \"\"\"\n",
    "        \n",
    "        #dataset = pd.read_csv(self.dataset_path,delimiter=',', index_col=0)\n",
    "        dataset = self.load_multi_view_dataframe(self.dataset_path)\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        return X\n",
    "\n",
    "    \n",
    "    def load_multi_view_dataframe(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(file_name, delimiter=',', index_col=0, header=[0,1])\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8728e1",
   "metadata": {},
   "source": [
    "        # Xk is a list contistripaning the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(self.K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append(np.nan)\n",
    "                #Xk.append('NaN')\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = X.iloc[:, ind:ind + self.dim_views[k]]\n",
    "                if self.is_norm:\n",
    "                    X_k = self.normalize_data(X_k) \n",
    "                    \n",
    "                    \n",
    "                Xk.append(X_k)\n",
    "                \n",
    "                ViewsX.append(1)\n",
    "            ind += self.dim_views[k]\n",
    "            \n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        #Xk_obs = [item for item in Xk if type(item) is not str]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cda129",
   "metadata": {},
   "source": [
    "**model_args** is a dictionary containing the mv-ppca model arguments: the total number of views across all datasets (tot_views), the dimension of each view (dim_views), the latent space size (n_components), and a boolean (norm) for data preprocessing. Additionaly, the researcher can provide priors for one ore more global parameters.\n",
    "\n",
    "**training_args** contains here the number of local iterations for EM/MAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05aa5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_views = 3\n",
    "dim_views = [15, 8, 10]\n",
    "n_components = 4\n",
    "norm = True\n",
    "\n",
    "model_args = {'tot_views': tot_views, 'dim_views': dim_views, 'n_components': n_components, 'is_norm': norm}\n",
    "\n",
    "# better to increase log interval if number of iteration is higher\n",
    "training_args = {'n_iterations': 15, 'log_interval' : 1} #\n",
    "\n",
    "\n",
    "tags =  ['ppca_data']\n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1a1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 16:52:12,857 fedbiomed INFO - Messaging researcher_36aa5f99-3402-42bc-935d-cbdffde57b21 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f3f86563ac0>\n",
      "2021-11-05 16:52:12,868 fedbiomed INFO - Searching dataset with data tags: ['ppca_data'] for all nodes\n",
      "2021-11-05 16:52:12,871 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG Message received: {'researcher_id': 'researcher_36aa5f99-3402-42bc-935d-cbdffde57b21', 'tags': ['ppca_data'], 'command': 'search'}\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.ppca_aggregator import MLaggregator\n",
    "\n",
    "\n",
    "\n",
    "# select nodes pr into task <Task pending name='Task-27' coro=<HTTP1ServerConnection._server_request_loop() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/http1connection.py:823> wait_for=<Future finished result=b'GET /kernel...6bd7\"\\r\\n\\r\\n'> cb=[IOLoop.add_future.<locals>.<lambda>() at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/tornado/ioloop.py:688]> while another task <Task pending name='Task-2' coro=<KernelManager._async_start_kernel() running at /user/ybouilla/home/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/jupyter_carticiping to this experiment\n",
    "exp = Experiment(tags=tags,\n",
    "                 #clients=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='Fed_MV_PPCA',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=MLaggregator(),\n",
    "                 client_selection_strategy=None,\n",
    "                 tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6450b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 16:40:57,430 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2021-11-05 16:40:57,435 fedbiomed INFO - log from: node_599cdb35-0951-48fa-9c8a-90d2250b5ef5 - DEBUG Message received: {'researcher_id': 'researcher_14fb572e-33d6-4ed0-aff5-4f2405c89a1d', 'command': 'list'}\n",
      "2021-11-05 16:41:07,447 fedbiomed INFO - \n",
      " Node: node_599cdb35-0951-48fa-9c8a-90d2250b5ef5 | Number of Datasets: 1 \n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "| name   | data_type   | tags          | description   | shape     | multi_view   |\n",
      "+========+=============+===============+===============+===========+==============+\n",
      "|        | csv         | ['ppca_data'] |               | [215, 35] | multi_view   |\n",
      "+--------+-------------+---------------+---------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "\n",
    "req = Requests()\n",
    "datasets = req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import TENSORBOARD_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db565a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = self.view[x]%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601818e",
   "metadata": {},
   "source": [
    "Following cell will launch tensorboard on `TENSORBOARD_RESULTS_DIR`. Since the experiment is not started, it won't show any results. After runing experiment, you can click refresh button to see changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"$TENSORBOARD_RESULTS_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 16:53:01,877 fedbiomed INFO - Sampled clients in round 0 ['node_575cf611-9f8a-401a-a725-52531fd74171']\n",
      "2021-11-05 16:53:01,881 fedbiomed INFO - Send message to client node_575cf611-9f8a-401a-a725-52531fd74171 - {'researcher_id': 'researcher_36aa5f99-3402-42bc-935d-cbdffde57b21', 'job_id': '8c97aeb5-584d-41d1-b8a9-8702dcc7441d', 'training_args': {'n_iterations': 15, 'log_interval': 1}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'is_norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_969a5b0e-1291-4bac-ad47-1f6784d1cbd1.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_c3ffdce9-e4a1-4687-948e-559eab75daa6.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'node_575cf611-9f8a-401a-a725-52531fd74171': ['dataset_846769bc-2b15-42a7-b77d-5624c16bacdf']}}\n",
      "2021-11-05 16:53:01,883 fedbiomed DEBUG - researcher_36aa5f99-3402-42bc-935d-cbdffde57b21\n",
      "2021-11-05 16:53:01,890 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG Message received: {'researcher_id': 'researcher_36aa5f99-3402-42bc-935d-cbdffde57b21', 'job_id': '8c97aeb5-584d-41d1-b8a9-8702dcc7441d', 'training_args': {'n_iterations': 15, 'log_interval': 1}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'is_norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_969a5b0e-1291-4bac-ad47-1f6784d1cbd1.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_c3ffdce9-e4a1-4687-948e-559eab75daa6.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'node_575cf611-9f8a-401a-a725-52531fd74171': ['dataset_846769bc-2b15-42a7-b77d-5624c16bacdf']}}\n",
      "2021-11-05 16:53:01,891 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG [TASKS QUEUE] Item:{'researcher_id': 'researcher_36aa5f99-3402-42bc-935d-cbdffde57b21', 'job_id': '8c97aeb5-584d-41d1-b8a9-8702dcc7441d', 'params_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_c3ffdce9-e4a1-4687-948e-559eab75daa6.pt', 'training_args': {'n_iterations': 15, 'log_interval': 1}, 'training_data': {'node_575cf611-9f8a-401a-a725-52531fd74171': ['dataset_846769bc-2b15-42a7-b77d-5624c16bacdf']}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'is_norm': True}, 'model_url': 'http://localhost:8844/media/uploads/2021/11/05/my_model_969a5b0e-1291-4bac-ad47-1f6784d1cbd1.py', 'model_class': 'Fed_MV_PPCA', 'command': 'train'}\n",
      "2021-11-05 16:53:02,370 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - INFO {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7f4fa1f76eb0>, 'n_iterations': 15, 'log_interval': 1}\n",
      "2021-11-05 16:53:02,371 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG Dataset_path/home/ybouilla/fedbiomed/notebooks/== Local path to node1.csv\n",
      "2021-11-05 16:53:02,372 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG is Dataset  multi view ? True\n",
      "2021-11-05 16:53:02,389 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - DEBUG Traceback (most recent call last):\n",
      "  File \"/home/ybouilla/fedbiomed/fedbiomed/node/round.py\", line 133, in run_model_training\n",
      "    model.training_routine(**training_kwargs_with_history)\n",
      "  File \"/home/ybouilla/fedbiomed/fedbiomed/common/ppca.py\", line 113, in training_routine\n",
      "    Wk, Sigma2 = self.initial_loc_params(q_i,ViewsX)\n",
      "  File \"/home/ybouilla/fedbiomed/fedbiomed/common/ppca.py\", line 236, in initial_loc_params\n",
      "    if ((self.params_dict['tilde_Wk'][k_name] is None) or (self.params_dict['sigma_til_Wk'][k_name] is None)):\n",
      "TypeError: list indices must be integers or slices, not str\n",
      "\n",
      "2021-11-05 16:53:02,391 fedbiomed INFO - log from: node_575cf611-9f8a-401a-a725-52531fd74171 - ERROR Cannot train model in round: list indices must be integers or slices, not str\n",
      "2021-11-05 16:53:02,391 fedbiomed CRITICAL - researcher stopped after receiving error/critical log from node: node_575cf611-9f8a-401a-a725-52531fd74171\n"
     ]
    }
   ],
   "source": [
    "# start federated training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7acfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88e2a782",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918cbe3",
   "metadata": {},
   "source": [
    "Herafter we test the performance of the aggregated parameters on a test dataset. In particular, for each round we use the global parameters to evaluate the mean absolute error and the separation in the latent space using LDA. Note that we have already defined the test dataset at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import solve\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_MB(Wk, q, D_i, K, Sigma2, ViewsX):\n",
    "    \"\"\"\n",
    "    Computes matrices M:=inv(I_q+sum_k Wk.TWk/sigma2k) and B:= [W1.T/sigma2K,...,W1.T/sigma2K].\n",
    "    :param Wk: list of matrices (d_k x q)\n",
    "    :param Sigma2: list of float > 0\n",
    "    :return np.arrays\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    M1 = Wk[index].reshape(D_i[index], q).T.dot(Wk[index].reshape(D_i[index],q)) / Sigma2[index]\n",
    "    B = Wk[index].reshape(D_i[index], q).T / Sigma2[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            M1 += Wk[k].reshape(D_i[k], q).T.dot(Wk[k].reshape(D_i[k],q)) / Sigma2[k]\n",
    "            B = np.concatenate((B, (Wk[k].reshape(D_i[k], q)).T / Sigma2[k]), axis=1)\n",
    "\n",
    "    M = solve(np.eye(q) + M1,np.eye(q))\n",
    "\n",
    "    return M, B\n",
    "\n",
    "def concat_params(park, K, ViewsX):\n",
    "    \"\"\"\n",
    "    This function concatenates parameters from a list\n",
    "    :param park: list of vectors/matrices to concatenate\n",
    "    :return np.array\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    par = park[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            par = np.concatenate((par, park[k]), axis=0)\n",
    "\n",
    "    return par\n",
    "[]\n",
    "def simu_latent(q,dataset,ViewsX,global_params):\n",
    "    \"\"\"\n",
    "    This function allows sampling of x_n (latent variables) from the posterior distribution \n",
    "    (with global parameters).\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "\n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    Xn = [(M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(1, q) for n in range(N)]\n",
    "\n",
    "    df = pd.DataFrame(np.vstack(Xn), index=dataset.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def MAE(dataset,ViewsX,q,global_params):\n",
    "    \"\"\"\n",
    "    This function evaluates the MAE using global parameters\n",
    "    :return float\n",
    "    \"\"\"    \n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "    \n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    W = concat_params(global_params['tilde_Wk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "    \n",
    "    T_true = dataset.values.tolist()\n",
    "\n",
    "    T_pred = []\n",
    "    for n in range(N):\n",
    "        Xng = (M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(q, 1)\n",
    "        T_pred.append((W.dot(Xng) + mu).reshape(d))\n",
    "    \n",
    "    MAE = mean_absolute_error(T_true, T_pred)\n",
    "    return MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be44826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "\n",
    "ppca = PpcaPlan(model_args)\n",
    "ppca.is_multi_view = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test datasetnp.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4\n",
    "\n",
    "\n",
    "def test_data(ppca, dataset,norm,K,dim_views):\n",
    "        \"\"\"\n",
    "            Equivalent to training_data, for the test dataset\n",
    "        \"\"\"\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append(np.nan)\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = X.iloc[:, ind:ind + dim_views[k]]\n",
    "                if norm:\n",
    "                    \n",
    "                    X_k = ppca.normalize_data(X_k)\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "for r in range(1,rounds):\n",
    "    global_params = exp.aggregated_params[r]['params']\n",
    "    ######## Train data\n",
    "    MAE_train = []\n",
    "    Latent_Train = pd.DataFrame()\n",
    "    Label_Train = pd.Series(dtype='int64')\n",
    "    for c in range(n_centers):\n",
    "        dataset_c = load_multi_view_dataframe('== Local path to node' + str(c+1) + '.csv')\n",
    "        \n",
    "        X_obs_c,Xk_c,ViewsX_c,y_c = test_data(ppca, dataset_c,norm,tot_views,dim_views)\n",
    "        # Dataframe of latent space for LDA\n",
    "        Latent_Train = Latent_Train.append(simu_latent(n_components,X_obs_c,ViewsX_c,global_params))\n",
    "        Label_Train = Label_Train.append(y_c)\n",
    "        # MAE Train\n",
    "        MAE_train.append(MAE(X_obs_c,ViewsX_c,n_components,global_params))\n",
    "\n",
    "    \n",
    "\n",
    "    ######## Test data\n",
    "    \n",
    "    X_obs_test,Xk_test,ViewsX_test,y_test = test_data(ppca, t_test,norm,tot_views,dim_views)\n",
    "\n",
    "    Latent_Test = pd.DataFrame()\n",
    "    Label_Test = pd.Series(dtype='int64')\n",
    "\n",
    "    # Dataframe of latent space for LDA\n",
    "    Latent_Test = Latent_Test.append(simu_latent(n_components,X_obs_test,ViewsX_test,global_params))\n",
    "    Label_Test = Label_Test.append(y_test)\n",
    "    # MAE Test\n",
    "    MAE_test = MAE(X_obs_test,ViewsX_test,n_components,global_params)\n",
    "\n",
    "\n",
    "    pprintrint('Round {}:'.format(r))\n",
    "    print('MAE train (mean,std) = ({:.4f},{:.4f}) \\\n",
    "    \\t MAE test = {:.4f} \\\n",
    "    '.format(np.mean(np.array(MAE_train)), \\\n",
    "      np.std(np.array(MAE_train)), MAE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36770ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
