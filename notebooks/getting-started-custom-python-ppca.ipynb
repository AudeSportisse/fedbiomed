{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e87007",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher to train a federated PPCA (Probabilistic PCA) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17d2f",
   "metadata": {},
   "source": [
    "## Description of the exercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb782b14",
   "metadata": {},
   "source": [
    "Three datasets `n1.csv` , `n2.csv` and `n3.csv` will be generated randomly using 3-views PPCA from a 4-dimensional latent space, with views dimensions [15,8,10] and 2 groups. Henceforth, we will distribute the 3 dataset to 3 distinct nodes and use Fed-mv-PPCA. In each center we check the evolution of expected LL during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0bc07",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We will generate three datasets using mv-PPCA.\n",
    "Then save them in a path of your choice on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48f8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "def sample_x_n(N:int, q:int, random_state:int=None):\n",
    "    \"\"\"samples from a Guassian dsitribution\n",
    "    Args:\n",
    "    \n",
    "    :N first dimension array\n",
    "    :q second dimesnion array\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(random_state).randn(N,q)\n",
    "\n",
    "def generate_data(N_g: List[int],\n",
    "                  W: np.ndarray,\n",
    "                  a_g: np.ndarray,\n",
    "                  mu:float,\n",
    "                  sigma2:float,\n",
    "                  x_n,\n",
    "                  view:int,\n",
    "                  random_state=None):\n",
    "    \n",
    "    \"\"\"generates Gaussian dataset given several groups of data points, using the following\n",
    "    gausian generative proccess (for a given view):\n",
    "    \n",
    "    Y = WX + mu + epsilon with epsilon ~ N(0, sigma2)\n",
    "    \n",
    "    where X is the latent space of size (q, n_features), Y the observation matrix, W the matrix used\n",
    "    for data reconstruction\n",
    "    \n",
    "    Params:\n",
    "    :N_g: List[int] number of data to generate per group (list of size number of group)\n",
    "    :W: reconstruction matrix, of size (n_features, q)\n",
    "    :a_g: (np.ndarray) array of size (nb_group, n_components), introduces shift when creating different group. \n",
    "    :mu: offset of the dataset\n",
    "    :sigma2: variance used for generating\n",
    "    :x_n: random variable \n",
    "    :view: (int) the given view\n",
    "    :random_state: ransom seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    :Y (pd.DataFrame): synthetic dataset generated wrt above expression\n",
    "    of size (n_features, n_samples)\n",
    "    \"\"\"\n",
    "    rnd=np.random.RandomState(random_state)\n",
    "\n",
    "    N=N_g.sum()\n",
    "    d, q = W.shape\n",
    "    sigma=np.sqrt(sigma2)\n",
    "    G=len(N_g)\n",
    "\n",
    "    g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    print(g_ind)\n",
    "\n",
    "    y_n=np.empty((N, d))\n",
    "\n",
    "    for g in range(G):\n",
    "        # computing Y = W.transpose(X + shift()) + mu\n",
    "        y_n[g_ind[g]:g_ind[g+1]]= np.einsum(\"dq,nq->nd\", W, x_n[g_ind[g]:g_ind[g+1]]+a_g[g]) + mu\n",
    "        \n",
    "    y_n = pd.DataFrame(data=y_n,\n",
    "                     columns=[f'var_{view},{i + 1}' for i in range(d)])\n",
    "\n",
    "    return y_n + sigma*rnd.randn(N,d)\n",
    "\n",
    "\n",
    "\n",
    "def generate_ppca_nodes_dataset(n_nodes: int,\n",
    "                           n_features: Union[List[int],int],\n",
    "                           \n",
    "                           n_components: int,\n",
    "                               n_group: int=2,\n",
    "                           absent_view: Dict[str, int]=None,\n",
    "                           W_init: List[np.ndarray]=None,\n",
    "                           mu_init: List[np.ndarray]=None,\n",
    "                           sigma_init : List[np.ndarray]=None,\n",
    "                           is_validation: bool=True,\n",
    "                           n_sample_validation: int=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for each node\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate PPCA parameters if not defined\n",
    "    ## case where W parameter is not defined\n",
    "    if W_init is None:\n",
    "        W_init = []\n",
    "        for i in range(n_nodes):\n",
    "            W_gen = np.random.uniform(-10, 10, (n_features[i], n_components))\n",
    "            W_init.append(W_gen)\n",
    "    ## case where mu not generated\n",
    "    if mu_init is None:\n",
    "        mu_init = []\n",
    "        for i in range(n_nodes):\n",
    "            mu_gen = np.random.uniform(-10, 10, n_features[i])\n",
    "            mu_init.append(mu_gen)\n",
    "            \n",
    "    ## case where sigma is not definied (we will set sigma =1 for each clients)\n",
    "    if sigma_init is None:\n",
    "        sigma_init = []\n",
    "        for i in range(n_nodes):\n",
    "            sigma_init.append(1)\n",
    "            \n",
    "    \n",
    "    shift = np.concatenate((np.zeros((1, n_components)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14ab247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 148 215]\n",
      "[  0 148 215]\n",
      "[  0 148 215]\n",
      "[  0 294 351]\n",
      "[  0 294 351]\n",
      "[  0 294 351]\n",
      "[  0 206 324]\n",
      "[  0 206 324]\n",
      "[  0 206 324]\n",
      "[ 0 20 40]\n",
      "[ 0 20 40]\n",
      "[ 0 20 40]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "D_i = [15, 8, 10]\n",
    "nb_group = 2\n",
    "n_centers = 3\n",
    "testing_samples = 40\n",
    "\n",
    "n_components_generated = 4\n",
    "\n",
    "\n",
    "# initializing PPCA variables\n",
    "sigma2_gen1, sigma2_gen2, sigma2_gen3 = 2, 1, 3\n",
    "W_gen1 = np.random.uniform(-10, 10, (D_i[0], n_components_generated))\n",
    "W_gen2 = np.random.uniform(-5, 5, (D_i[1], n_components_generated))\n",
    "W_gen3 = np.random.uniform(-15, 15, (D_i[2], n_components_generated))\n",
    "mu_gen1 = np.random.uniform(-10, 10, D_i[0])\n",
    "mu_gen2 = np.random.uniform(-5, 5, D_i[1])\n",
    "mu_gen3 = np.random.uniform(-15, 15, D_i[2])\n",
    "\n",
    "a_g_gen = np.concatenate((np.zeros((1, n_components_generated)),\n",
    "                          np.random.uniform(-10, 10, (nb_group - 1, n_components_generated))))\n",
    "\n",
    "W = [W_gen1,W_gen2,W_gen3]\n",
    "mu = [mu_gen1,mu_gen2,mu_gen3]\n",
    "sigma = [sigma2_gen1,sigma2_gen2,sigma2_gen3]\n",
    "\n",
    "# absent_views contains as key the id of a center in which we want to simulate absent views,\n",
    "# and as argumet the id of the missing view. \n",
    "absent_views = {'2': 2}\n",
    "\n",
    "for i in range(n_centers):\n",
    "    N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)]) \n",
    "    # N_g = np.array([np.random.randint(25,300) for _ in range(nb_group)])?\n",
    "    #g_ind = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "    N = N_g.sum()\n",
    "    x_n_gen = sample_x_n(N, n_components_generated, random_state=150)  # randomly generate a Gaussian\n",
    "    # dataset\n",
    "    Y = []\n",
    "    for d in range(len(D_i)):\n",
    "        y_t = generate_data(N_g, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "        if ((str(i+1) in absent_views.keys()) \\\n",
    "            and (type(absent_views[str(i+1)])== int) \\\n",
    "            and (absent_views[str(i+1)]==d+1)):\n",
    "            absent_views.update({str(i+1): y_t})\n",
    "            y_abs=pd.DataFrame(np.nan, index = np.arange(N_g.sum()), \\\n",
    "                               columns = [f'var_{d+1},{i + 1}' for i in range(D_i[d])])\n",
    "            Y.append(y_abs)\n",
    "        else:\n",
    "            Y.append(y_t)\n",
    "\n",
    "    gr = []\n",
    "    for g in range(nb_group):\n",
    "       gr += [int(g) for _ in range(N_g[g])]\n",
    "    gr = pd.Series(gr)\n",
    "    Y.append(gr)\n",
    "\n",
    "###\n",
    "# the output will be a list of n_centers dataset containing: dataframe for each centers,of \n",
    "# different dimensions\n",
    "# \n",
    "    t_i = pd.concat(Y, axis=1)\n",
    "    t_i.columns.values[-1] = 'Label'\n",
    "    t_i.to_csv('== Local path to node' + str(i+1) + '.csv',sep=',')\n",
    "    #np.savetxt('== Local path to node' + str(i+1) + '.csv',t_i,delimiter=',')\n",
    "               \n",
    "# building the test dataset\n",
    "N_g_test = np.array([testing_samples//2,testing_samples//2])\n",
    "g_ind_test = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g_test)))\n",
    "N_test = N_g_test.sum()\n",
    "x_n_gen = sample_x_n(N_test, n_components_generated, random_state=150)\n",
    "Y_test = []\n",
    "for d in range(len(D_i)):\n",
    "    y_t = generate_data(N_g_test, W[d], a_g_gen, mu[d], sigma[d], x_n_gen, view = d+1, random_state=250)\n",
    "    Y_test.append(y_t)\n",
    "\n",
    "gr_test = [0 for _ in range(N_g_test[0])]+[1 for _ in range(N_g_test[1])]\n",
    "gr_test = pd.Series(gr_test)\n",
    "Y_test.append(gr_test)\n",
    "\n",
    "t_test = pd.concat(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "g_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc822b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n_gen[g_ind[g]:g_ind[g+1]].shape, W[0].shape, a_g_gen[g].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "g_ind=np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(N_g)))\n",
    "\n",
    "print(W[0].shape)\n",
    "resp = np.einsum(\"dq,nq->nd\", W[0], x_n_gen[g_ind[g]:g_ind[g+1]]+a_g_gen[g])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = x_n_gen[g_ind[g]:g_ind[g+1]]+a_g_gen[g]\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"dq,nq->nd\", np.array([[1, 0,0, 0],\n",
    "                                 [0, 0,0, 0]]), np.array([[0, 1, 1, 1],\n",
    "                                                          [0,0,0,0,],\n",
    "                                                          [0,0,0,0,],\n",
    "                                                          [0,0,0,0,]\n",
    "                                                         ]\n",
    "                                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca62f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tt = pd.DataFrame({\"www\": [np.nan, np.nan, np.nan], 'qq': [1, 2, 4]})\n",
    "\n",
    "tt.iloc[:, 1].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f160e",
   "metadata": {},
   "source": [
    "## Start the network and setting the client up\n",
    "Before running this notebook:\n",
    "1. You should start the network from fedbiomed-network, as detailed in :\n",
    "https://gitlab.inria.fr/fedbiomed/fedbiomed\n",
    "\n",
    "2. You need to configure at least 2 nodes: <br/>\n",
    "* **Node 1 :** `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[0],y[0].\n",
    "  * Check that your data has been added in node 1 by executing `./scripts/fedbiomed_run node list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node start`. <br/>\n",
    "\n",
    "* **Node 2 :** Open a second terminal and run ./scripts/fedbiomed_run node add config n2.ini\n",
    "  * Select option 1 to add a csv file to the client\n",
    "  * Choose the name, tags and description of the dataset (you can write 'sk' always and it will be good)\n",
    "  * Pick the .csv file you stored the couple X[1],y[1].\n",
    "  * Check that your data has been added in node 2 by executing `./scripts/fedbiomed_run node config n2.ini list`\n",
    "  * Run the node using `./scripts/fedbiomed_run node config n2.ini start`.\n",
    "  \n",
    "\n",
    "\n",
    " Wait until you get `Connected with result code 0`. it means node is online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c80070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fedbiomed.researcher.environ import TMP_DIR\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=TMP_DIR+'/')\n",
    "model_file = tmp_dir_model.name + '/fed_mv_ppca.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9aaa87",
   "metadata": {},
   "source": [
    "Hereafter the template of the class you should provide to Fedbiomed :\n",
    "       \n",
    "**training_data** : you must return here a tuple (X,X_k,ViewsX,y) or (X,X_k,ViewsX). Note that all centers should provide a dataset with the same view-specific columns. If in a specific center a view has not been observed, then the corresponding columns will be filled of nan. The training_data method take care of identifying view-specific sub-datasets and collecting information concerning non-available observations. Data can also been normalized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10cc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ybouilla/fedbiomed/var/tmp/tmp941yk0km/fed_mv_ppca.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "from fedbiomed.common.ppca import PpcaPlan\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fed_MV_PPCA(PpcaPlan):\n",
    "    def __init__(self, kwargs):\n",
    "        super(Fed_MV_PPCA, self).__init__(kwargs)\n",
    "        deps = ['import numpy as np']\n",
    "        self.add_dependency(deps)\n",
    "    \n",
    "    def training_data(self):\n",
    "        \"\"\"\n",
    "            Perform in this method all data reading and data transformations you need.\n",
    "            At the end you should provide a tuple (X_obs,Xk,ViewsX,y), where: \n",
    "            X_obs is the training dataset, \n",
    "            Xk is a list containing the k-specific dataframe if it exists or 'NaN' otherwise,\n",
    "            ViewsX is the indicator function for observed views (ViewsX[k]=1 if view k is observed, 0 otherwise)\n",
    "            y the corresponding labels (optional)\n",
    "            The dataset is normalized using min max scaler if model_args['norm'] is true\n",
    "            Note: since labels are not needed for the optimization, \n",
    "            training_data can also simply return (X_obs,Xk,ViewsX)\n",
    "            :raise NotImplementedError if researcher do not implement this method.\n",
    "        \"\"\"\n",
    "        dataset = pd.read_csv(self.dataset_path,delimiter=',', index_col=0)\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(self.K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append(np.nan)\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = self.normalize_data(X.iloc[:, ind:ind + self.dim_views[k]]) if self.norm \\\n",
    "                    else X.iloc[:, ind:ind + self.dim_views[k]]\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += self.dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if item is not np.nan]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cda129",
   "metadata": {},
   "source": [
    "**model_args** is a dictionary containing the mv-ppca model arguments: the total number of views across all datasets (tot_views), the dimension of each view (dim_views), the latent space size (n_components), and a boolean (norm) for data preprocessing. Additionaly, the researcher can provide priors for one ore more global parameters.\n",
    "\n",
    "**training_args** contains here the number of local iterations for EM/MAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05aa5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_views = 3\n",
    "dim_views = [15, 8, 10]\n",
    "n_components = 4\n",
    "norm = True\n",
    "\n",
    "model_args = {'tot_views': tot_views, 'dim_views': dim_views, 'n_components': n_components, 'norm': norm}\n",
    "\n",
    "training_args = {'n_iterations': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1a1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 18:47:54,174 fedbiomed INFO - Messaging researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f5956cfd1c0>\n",
      "2021-10-26 18:47:54,206 fedbiomed INFO - Searching for clients with data tags: ['ppca_data']\n",
      "2021-10-26 18:47:54,210 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'success': True, 'databases': [{'name': 'd', 'data_type': 'csv', 'tags': ['ppca_data'], 'description': '', 'shape': [215, 34], 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8'}], 'count': 1, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'command': 'search'}\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.mlaggregator import MLaggregator\n",
    "\n",
    "tags =  ['ppca_data']\n",
    "rounds = 5\n",
    "\n",
    "# select nodes participing to this experiment\n",
    "exp = Experiment(tags=tags,\n",
    "                 #clients=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='Fed_MV_PPCA',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=MLaggregator(),\n",
    "                 client_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ff55da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 18:48:07,009 fedbiomed INFO - Sampled clients in round 0 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:07,011 fedbiomed INFO - Send message to client client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - {'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_eda268eb-23e0-4e10-86bc-59a323706767.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_25515521-0f97-43b4-a575-58442f58af23.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0': ['dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8']}}\n",
      "2021-10-26 18:48:07,013 fedbiomed DEBUG - researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc\n",
      "2021-10-26 18:48:09,344 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'success': True, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/node_params_763b3b28-9b30-4d27-8900-bfb6eeece2d2.pt', 'timing': {'rtime_training': 1.8706445949992485, 'ptime_training': 7.769512263}, 'msg': '', 'command': 'train'}\n",
      "2021-10-26 18:48:17,025 fedbiomed INFO - Downloading model params after training on client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - from http://localhost:8844/media/uploads/2021/10/26/node_params_763b3b28-9b30-4d27-8900-bfb6eeece2d2.pt\n",
      "2021-10-26 18:48:17,181 fedbiomed INFO - Clients that successfully reply in round 0 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:17,213 fedbiomed INFO - Sampled clients in round 1 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:17,214 fedbiomed INFO - Send message to client client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - {'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_eda268eb-23e0-4e10-86bc-59a323706767.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/researcher_params_4a25bbf2-02c3-4d3f-9e89-068ddd18c6cb.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0': ['dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8']}}\n",
      "2021-10-26 18:48:17,215 fedbiomed DEBUG - researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc\n",
      "2021-10-26 18:48:18,811 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'success': True, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/node_params_766cd0af-59d6-46cd-825d-fed3ddfbcd6f.pt', 'timing': {'rtime_training': 1.5523939320009958, 'ptime_training': 6.055448404}, 'msg': '', 'command': 'train'}\n",
      "2021-10-26 18:48:27,227 fedbiomed INFO - Downloading model params after training on client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - from http://localhost:8844/media/uploads/2021/10/26/node_params_766cd0af-59d6-46cd-825d-fed3ddfbcd6f.pt\n",
      "2021-10-26 18:48:27,235 fedbiomed INFO - Clients that successfully reply in round 1 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:27,397 fedbiomed INFO - Sampled clients in round 2 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:27,398 fedbiomed INFO - Send message to client client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - {'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_eda268eb-23e0-4e10-86bc-59a323706767.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/researcher_params_2b6d608f-201c-430a-9f6d-08cce31a83b0.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0': ['dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8']}}\n",
      "2021-10-26 18:48:27,398 fedbiomed DEBUG - researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc\n",
      "2021-10-26 18:48:29,222 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'success': True, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/node_params_709d3a9c-6f5f-40d5-bca7-7a94c0a9cb2d.pt', 'timing': {'rtime_training': 1.7856221310012188, 'ptime_training': 6.978420801}, 'msg': '', 'command': 'train'}\n",
      "2021-10-26 18:48:37,409 fedbiomed INFO - Downloading model params after training on client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - from http://localhost:8844/media/uploads/2021/10/26/node_params_709d3a9c-6f5f-40d5-bca7-7a94c0a9cb2d.pt\n",
      "2021-10-26 18:48:37,439 fedbiomed INFO - Clients that successfully reply in round 2 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:37,477 fedbiomed INFO - Sampled clients in round 3 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:37,478 fedbiomed INFO - Send message to client client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - {'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_eda268eb-23e0-4e10-86bc-59a323706767.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/researcher_params_d8e58411-8b92-4140-b8a6-b48b99a45c95.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0': ['dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8']}}\n",
      "2021-10-26 18:48:37,479 fedbiomed DEBUG - researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc\n",
      "2021-10-26 18:48:39,272 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'success': True, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/node_params_b46fe6f7-f032-4980-88f0-ed60694cce66.pt', 'timing': {'rtime_training': 1.7451691809983458, 'ptime_training': 6.817306317}, 'msg': '', 'command': 'train'}\n",
      "2021-10-26 18:48:47,490 fedbiomed INFO - Downloading model params after training on client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - from http://localhost:8844/media/uploads/2021/10/26/node_params_b46fe6f7-f032-4980-88f0-ed60694cce66.pt\n",
      "2021-10-26 18:48:47,503 fedbiomed INFO - Clients that successfully reply in round 3 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:47,535 fedbiomed INFO - Sampled clients in round 4 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n",
      "2021-10-26 18:48:47,535 fedbiomed INFO - Send message to client client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - {'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'training_args': {'n_iterations': 15}, 'model_args': {'tot_views': 3, 'dim_views': [15, 8, 10], 'n_components': 4, 'norm': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/10/26/my_model_eda268eb-23e0-4e10-86bc-59a323706767.py', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/researcher_params_6902a5f5-bbe6-4aa0-a183-b166f9979f59.pt', 'model_class': 'Fed_MV_PPCA', 'training_data': {'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0': ['dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 18:48:47,536 fedbiomed DEBUG - researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc\n",
      "2021-10-26 18:48:49,155 fedbiomed INFO - message received:{'researcher_id': 'researcher_66c62608-8c1c-4db9-b9d4-92e7df65f4cc', 'job_id': '928ecc58-ea4e-4879-a171-cd2b96e0e37d', 'success': True, 'client_id': 'client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0', 'dataset_id': 'dataset_204ec3d3-0806-42e0-b27f-4ffca8529ee8', 'params_url': 'http://localhost:8844/media/uploads/2021/10/26/node_params_c584a839-4e9a-4904-98ec-48a0f7867978.pt', 'timing': {'rtime_training': 1.5721432230002392, 'ptime_training': 6.141439141999999}, 'msg': '', 'command': 'train'}\n",
      "2021-10-26 18:48:57,549 fedbiomed INFO - Downloading model params after training on client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0 - from http://localhost:8844/media/uploads/2021/10/26/node_params_c584a839-4e9a-4904-98ec-48a0f7867978.pt\n",
      "2021-10-26 18:48:57,576 fedbiomed INFO - Clients that successfully reply in round 4 ['client_67ee5ef1-b153-4dd4-bbb6-ba7a311cc0a0']\n"
     ]
    }
   ],
   "source": [
    "# start federated training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eca4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List the training rounds :  dict_keys([0, 1, 2, 3, 4])\n",
      "\n",
      "Access the federated params for the last training round :\n",
      "\t- params_path:  /home/ybouilla/fedbiomed/var/tmp/researcher_params_1591626a-0754-43e9-afc4-89140acc0987.pt\n",
      "\t- parameter data:  dict_keys(['tilde_muk', 'tilde_Wk', 'tilde_Sigma2k', 'Alpha', 'Beta', 'sigma_til_muk', 'sigma_til_Wk', 'sigma_til_sigma2k'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a782",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918cbe3",
   "metadata": {},
   "source": [
    "Herafter we test the performance of the aggregated parameters on a test dataset. In particular, for each round we use the global parameters to evaluate the mean absolute error and the separation in the latent space using LDA. Note that we have already defined the test dataset at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7749459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import solve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    This function normalize the dataset X\n",
    "    :param X: pandas dataframe\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    col_name = [col.strip() for col in list(X.columns)]\n",
    "    x = X.values  # returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    norm_dataset = pd.DataFrame(x_scaled, index=X.index, columns=col_name)\n",
    "\n",
    "    return norm_dataset\n",
    "\n",
    "def eval_MB(Wk, q, D_i, K, Sigma2, ViewsX):\n",
    "    \"\"\"\n",
    "    Computes matrices M:=inv(I_q+sum_k Wk.TWk/sigma2k) and B:= [W1.T/sigma2K,...,W1.T/sigma2K].\n",
    "    :param Wk: list of matrices (d_k x q)\n",
    "    :param Sigma2: list of float > 0\n",
    "    :return np.arrays\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    M1 = Wk[index].reshape(D_i[index], q).T.dot(Wk[index].reshape(D_i[index],q)) / Sigma2[index]\n",
    "    B = Wk[index].reshape(D_i[index], q).T / Sigma2[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            M1 += Wk[k].reshape(D_i[k], q).T.dot(Wk[k].reshape(D_i[k],q)) / Sigma2[k]\n",
    "            B = np.concatenate((B, (Wk[k].reshape(D_i[k], q)).T / Sigma2[k]), axis=1)\n",
    "\n",
    "    M = solve(np.eye(q) + M1,np.eye(q))\n",
    "\n",
    "    return M, B\n",
    "\n",
    "def concat_params(park, K, ViewsX):\n",
    "    \"\"\"\n",
    "    This function concatenates parameters from a list\n",
    "    :param park: list of vectors/matrices to concatenate\n",
    "    :return np.array\n",
    "    \"\"\"\n",
    "    index = ViewsX.index(1)\n",
    "\n",
    "    par = park[index]\n",
    "    for k in range(index + 1, K):\n",
    "        if ViewsX[k] == 1:\n",
    "            par = np.concatenate((par, park[k]), axis=0)\n",
    "\n",
    "    return par\n",
    "\n",
    "def simu_latent(q,dataset,ViewsX,global_params):\n",
    "    \"\"\"\n",
    "    This function allows sampling of x_n (latent variables) from the posterior distribution \n",
    "    (with global parameters).\n",
    "    :return pandas dataframe\n",
    "    \"\"\"\n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "\n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    Xn = [(M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(1, q) for n in range(N)]\n",
    "\n",
    "    df = pd.DataFrame(np.vstack(Xn), index=dataset.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def MAE(dataset,ViewsX,q,global_params):\n",
    "    \"\"\"\n",
    "    This function evaluates the MAE using global parameters\n",
    "    :return float\n",
    "    \"\"\"    \n",
    "    d = dataset.shape[1]\n",
    "    N = dataset.shape[0]\n",
    "    K = len(ViewsX)\n",
    "    \n",
    "    mu = concat_params(global_params['tilde_muk'], K, ViewsX)\n",
    "    W = concat_params(global_params['tilde_Wk'], K, ViewsX)\n",
    "    M, B = eval_MB(global_params['tilde_Wk'], q, D_i, K, global_params['tilde_Sigma2k'],ViewsX)\n",
    "\n",
    "    T_true = dataset.values.tolist()\n",
    "\n",
    "    T_pred = []\n",
    "    for n in range(N):\n",
    "        Xng = (M.dot(B).dot(dataset.iloc[n].values.reshape(d, 1) - mu)).reshape(q, 1)\n",
    "        T_pred.append((W.dot(Xng) + mu).reshape(d))\n",
    "\n",
    "    MAE = mean_absolute_error(T_true, T_pred)\n",
    "\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7fa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def test_data(dataset,norm,K,dim_views):\n",
    "        \"\"\"\n",
    "            Equivalent to training_data, for the test dataset\n",
    "        \"\"\"\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        y = dataset[dataset.columns[-1]]\n",
    "        \n",
    "        # Xk is a list contianing the view-specific local datasets\n",
    "        Xk = []\n",
    "        ViewsX = []\n",
    "        ind = 0\n",
    "        for k in range(K):\n",
    "            if X.iloc[:, ind].isnull().values.any():\n",
    "                Xk.append('NaN')\n",
    "                ViewsX.append(0)\n",
    "            else:\n",
    "                # if norm = true, data are normalized with min max scaler\n",
    "                X_k = normalize_data(X.iloc[:, ind:ind + dim_views[k]]) if norm \\\n",
    "                    else X.iloc[:, ind:ind + dim_views[k]]\n",
    "                Xk.append(X_k)\n",
    "                ViewsX.append(1)\n",
    "            ind += dim_views[k]\n",
    "        \n",
    "        # The entire dataset is re-built without empty columns\n",
    "        Xk_obs = [item for item in Xk if type(item) != str]\n",
    "        X_obs = pd.concat(Xk_obs, axis=1)\n",
    "        \n",
    "        return (X_obs,Xk,ViewsX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344f79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1:\n",
      "MAE train (mean,std) = (0.0381,0.0049)     \t MAE test = 0.0483     \t Accuracy in latent space = 1.00\n",
      "Round 2:\n",
      "MAE train (mean,std) = (0.0373,0.0049)     \t MAE test = 0.0473     \t Accuracy in latent space = 1.00\n",
      "Round 3:\n",
      "MAE train (mean,std) = (0.0367,0.0049)     \t MAE test = 0.0466     \t Accuracy in latent space = 1.00\n",
      "Round 4:\n",
      "MAE train (mean,std) = (0.0363,0.0049)     \t MAE test = 0.0461     \t Accuracy in latent space = 1.00\n"
     ]
    }
   ],
   "source": [
    "for r in range(1,rounds):\n",
    "    global_params = exp.aggregated_params[r]['params']\n",
    "    ######## Train data\n",
    "    MAE_train = []\n",
    "    Latent_Train = pd.DataFrame()\n",
    "    Label_Train = pd.Series(dtype='int64')\n",
    "    for c in range(n_centers):\n",
    "        dataset_c = pd.read_csv('== Local path to node' + str(c+1) + '.csv', delimiter=',', index_col=0)\n",
    "        X_obs_c,Xk_c,ViewsX_c,y_c = test_data(dataset_c,norm,tot_views,dim_views)\n",
    "        # Dataframe of latent space for LDA\n",
    "        Latent_Train = Latent_Train.append(simu_latent(n_components,X_obs_c,ViewsX_c,global_params))\n",
    "        Label_Train = Label_Train.append(y_c)\n",
    "        # MAE Train\n",
    "        MAE_train.append(MAE(X_obs_c,ViewsX_c,n_components,global_params))\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    X_Train_lda = lda.fit_transform(Latent_Train, Label_Train.reindex(Latent_Train.index))\n",
    "\n",
    "    ######## Test data\n",
    "    X_obs_test,Xk_test,ViewsX_test,y_test = test_data(t_test,norm,tot_views,dim_views)\n",
    "\n",
    "    Latent_Test = pd.DataFrame()\n",
    "    Label_Test = pd.Series(dtype='int64')\n",
    "\n",
    "    # Dataframe of latent space for LDA\n",
    "    Latent_Test = Latent_Test.append(simu_latent(n_components,X_obs_test,ViewsX_test,global_params))\n",
    "    Label_Test = Label_Test.append(y_test)\n",
    "    # MAE Test\n",
    "    MAE_test = MAE(X_obs_test,ViewsX_test,n_components,global_params)\n",
    "    # LDA Test\n",
    "    Size_tes = Latent_Test.shape[0]\n",
    "    y_pred_test = lda.predict(Latent_Test)\n",
    "    conf_LDA_Test = confusion_matrix(Label_Test.reindex(Latent_Test.index), y_pred_test)\n",
    "    TP = np.diag(conf_LDA_Test)\n",
    "    num_classes = len(np.unique(Label_Test))\n",
    "    accuracy_LDA_Test = sum(TP) / Size_tes\n",
    "\n",
    "    print('Round {}:'.format(r))\n",
    "    print('MAE train (mean,std) = ({:.4f},{:.4f}) \\\n",
    "    \\t MAE test = {:.4f} \\\n",
    "    \\t Accuracy in latent space = {:.2f}'.format(np.mean(np.array(MAE_train)), \\\n",
    "                                                 np.std(np.array(MAE_train)), MAE_test, accuracy_LDA_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75715b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
