{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial shows how to do 2d image classification example on MedNIST dataset using pretrained PyTorch model Densnet121.\n",
    "\n",
    "\n",
    "## Load MedNIST to the node\n",
    "### About MedNIST\n",
    "\n",
    "MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.\n",
    "\n",
    "MedNIST dataset is downloaded from the resources provided by the project MONAI:\n",
    "https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\n",
    "\n",
    "The dataset MedNIST has 58954 images of size (3, 64, 64) distributed into 6 classes (10000 images per class except for BreastMRI class which has 8954 images). It has the structure:\n",
    "\n",
    "\n",
    "â””â”€â”€ MedNIST/\n",
    "\n",
    "    â”œâ”€â”€ AbdomenCT/\n",
    "    \n",
    "    â””â”€â”€ BreastMRI/\n",
    "    \n",
    "    â””â”€â”€ CXR/\n",
    "    \n",
    "    â””â”€â”€ ChestCT/\n",
    "    \n",
    "    â””â”€â”€ Hand/\n",
    "    \n",
    "    â””â”€â”€ HeadCT/   \n",
    "\n",
    "\n",
    "### Start the network\n",
    "\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`\n",
    "\n",
    "### Setup the node\n",
    "\n",
    "1. Populate the node with MedNIST dataset`./scripts/fedbiomed_run node add`\n",
    "  * Select option 3 (mednist) to add MedNIST to the node\n",
    "  * Confirm mednist tags tags ['#MEDNIST', '#dataset'] by hitting \"y\" and ENTER\n",
    "  * Select the folder where MedNIST is downloaded (It will be downloaded if it is not found in the selected path)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Start the node using `./scripts/fedbiomed_run node run`. Wait until you get `Starting task manager`. it means you are online.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Fed-Biomed Researcher\n",
    "\n",
    "We are now ready to start the researcher environment with the command source ./scripts/fedbiomed_environment researcher, and open the Jupyter notebook.\n",
    "\n",
    "To make sure that MedNIST dataset is loaded in the node we can send a request to the network to list the available dataset in the node. The list command should output an entry for mednist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 13:39:26,035 fedbiomed INFO - Component environment:\n",
      "2022-04-14 13:39:26,036 fedbiomed INFO - type = ComponentType.RESEARCHER\n",
      "2022-04-14 13:39:26,249 fedbiomed INFO - Messaging researcher_52f9c0d9-e21c-4653-9938-7a88e3e83cc0 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f4695883670>\n",
      "2022-04-14 13:39:26,271 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2022-04-14 13:39:36,285 fedbiomed INFO - \n",
      " Node: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b | Number of Datasets: 2 \n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "| name    | data_type   | tags                     | description     | shape              |\n",
      "+=========+=============+==========================+=================+====================+\n",
      "| MNIST   | default     | ['#MNIST', '#dataset']   | MNIST database  | [60000, 1, 28, 28] |\n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "| MEDNIST | mednist     | ['#MEDNIST', '#dataset'] | MEDNIST dataset | [58954, 3, 64, 64] |\n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b': [{'name': 'MNIST',\n",
       "   'data_type': 'default',\n",
       "   'tags': ['#MNIST', '#dataset'],\n",
       "   'description': 'MNIST database',\n",
       "   'shape': [60000, 1, 28, 28]},\n",
       "  {'name': 'MEDNIST',\n",
       "   'data_type': 'mednist',\n",
       "   'tags': ['#MEDNIST', '#dataset'],\n",
       "   'description': 'MEDNIST dataset',\n",
       "   'shape': [58954, 3, 64, 64]}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment to train a model on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MyTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.model =  densenet121(pretrained=True)\n",
    "        # We re-define the final fully-connected the layer\n",
    "        self.model.classifier =nn.Sequential(nn.Linear(1024,512), nn.ReLU())\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MedNIST data\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 13:40:10,220 fedbiomed INFO - Searching dataset with data tags: ['#MEDNIST', '#dataset'] for all nodes\n",
      "2022-04-14 13:40:20,233 fedbiomed INFO - Node selected for training -> node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b\n",
      "2022-04-14 13:40:20,402 fedbiomed DEBUG - Model file has been saved: /home/gentoo/Projects/Fedbiomed/fedbiomed/var/experiments/Experiment_0004/my_model_5ad29141-25f4-41d5-ad1f-e8641331b31e.py\n",
      "2022-04-14 13:40:20,502 fedbiomed DEBUG - upload (HTTP POST request) of file /home/gentoo/Projects/Fedbiomed/fedbiomed/var/experiments/Experiment_0004/my_model_5ad29141-25f4-41d5-ad1f-e8641331b31e.py successful, with status code 201\n",
      "2022-04-14 13:40:21,211 fedbiomed DEBUG - upload (HTTP POST request) of file /home/gentoo/Projects/Fedbiomed/fedbiomed/var/experiments/Experiment_0004/aggregated_params_init_e9b1f19b-8c53-4f8b-bff2-b53e39c5ea49.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MEDNIST', '#dataset']\n",
    "rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 13:40:31,346 fedbiomed INFO - Sampled nodes in round 0 ['node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b']\n",
      "2022-04-14 13:40:31,347 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: Perform training with the arguments: {'researcher_id': 'researcher_52f9c0d9-e21c-4653-9938-7a88e3e83cc0', 'job_id': '9c086d24-c5e5-4bdd-9c79-18b53dfdea75', 'training_args': {'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'training': True, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2022/04/14/my_model_5ad29141-25f4-41d5-ad1f-e8641331b31e.py', 'params_url': 'http://localhost:8844/media/uploads/2022/04/14/aggregated_params_init_e9b1f19b-8c53-4f8b-bff2-b53e39c5ea49.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b': ['dataset_a5cb044e-0f7d-4d8b-9e63-77a2bb7dbbc2']}} \n",
      " -----------------------------------------------------------------\n",
      "2022-04-14 13:40:31,348 fedbiomed DEBUG - researcher_52f9c0d9-e21c-4653-9938-7a88e3e83cc0\n",
      "2022-04-14 13:40:31,817 fedbiomed INFO - \u001b[1mWARNING\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m There is no test activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for testing will be ignored\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-04-14 13:40:32,176 fedbiomed INFO - \u001b[1mINFO\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m training with arguments {'history_monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7f2decfbeb50>, 'node_args': {'gpu': False, 'gpu_num': None, 'gpu_only': False}, 'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-04-14 13:40:37,907 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 480/111908 (0%) \n",
      " \t\t\t\t\t Loss: \u001b[1m2.201191\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:40:43,413 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 960/111908 (1%) \n",
      " \t\t\t\t\t Loss: \u001b[1m1.098292\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:40:49,205 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 1440/111908 (1%) \n",
      " \t\t\t\t\t Loss: \u001b[1m1.016704\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:40:55,203 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 1920/111908 (2%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.815673\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:41:01,113 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 2400/111908 (2%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.712398\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:41:07,029 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 2880/111908 (3%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.742691\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:41:12,941 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 3360/111908 (3%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.734571\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-04-14 13:41:18,861 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_f6b39db8-f72c-4a0e-86bc-1cb1feedd85b \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 3840/111908 (3%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.705829\u001b[0m \n",
      "\t\t\t\t\t ---------\n"
     ]
    }
   ],
   "source": [
    "exp.run(rounds=rounds, increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
