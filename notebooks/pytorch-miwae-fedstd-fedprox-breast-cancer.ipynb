{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data imputation with Fedbiomed using MIWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show:\n",
    "* how to obtain mean and std in a federated manner, to perform afterwards local dataset standardization with respect to the global dataset\n",
    "* how to impute missing not at random (MAR) data in a federated setting using MIWAE (https://arxiv.org/abs/2006.12871). \n",
    "\n",
    "We will compare results of federated training using FedAvg, FedProx (with both local standardization and federated standardization), with local results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we will use the breast cancer data from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train:  404\n",
      "mean_train:  [3.60912463e+00 1.15693069e+01 1.09850495e+01 7.17821782e-02\n",
      " 5.56484158e-01 6.31589109e+00 6.85564356e+01 3.80819505e+00\n",
      " 9.35643564e+00 4.04032178e+02 1.83183168e+01 3.56278342e+02\n",
      " 1.24573515e+01]\n",
      "std_train:  [8.86406744e+00 2.31238090e+01 6.88607935e+00 2.58126901e-01\n",
      " 1.17558710e-01 7.08573178e-01 2.79602535e+01 2.12858714e+00\n",
      " 8.57908366e+00 1.65966869e+02 2.22594093e+00 9.14531376e+01\n",
      " 7.10157559e+00]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, target, test_size=0.20, random_state=42)\n",
    "df_data_train = pd.DataFrame(data_train)\n",
    "N_train = len(df_data_train)\n",
    "mean_train = np.mean(data_train,0)\n",
    "std_train = np.std(data_train,0)\n",
    "\n",
    "print(\"N_train: \",N_train)\n",
    "print(\"mean_train: \",mean_train)\n",
    "print(\"std_train: \",std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_post 404\n",
      "mean_train_post [3.60912463e+00 1.15693069e+01 1.09850495e+01 7.17821782e-02\n",
      " 5.56484158e-01 6.31589109e+00 6.85564356e+01 3.80819505e+00\n",
      " 9.35643564e+00 4.04032178e+02 1.83183168e+01 3.56278342e+02\n",
      " 1.24573515e+01]\n",
      "std_train_post [8.86436413e+00 2.31246317e+01 6.88614271e+00 2.58134386e-01\n",
      " 1.17559296e-01 7.08579110e-01 2.79608726e+01 2.12859150e+00\n",
      " 8.57923305e+00 1.65969122e+02 2.22596627e+00 9.14558358e+01\n",
      " 7.10158461e+00]\n"
     ]
    }
   ],
   "source": [
    "# split train across datasets\n",
    "client_1, client_2, client_3 = np.split(df_data_train.sample(frac=1,random_state=42), \\\n",
    "                                        [int(.33*N_train), int(.66*len(df_data_train))])\n",
    "\n",
    "Clients_data=[client_1, client_2, client_3]\n",
    "\n",
    "N_cl = [len(i) for i in Clients_data]\n",
    "mean_cl = [np.nanmean(i,0) for i in Clients_data]\n",
    "std_cl = [np.nanstd(i,0) for i in Clients_data]\n",
    "cl = len(Clients_data)\n",
    "\n",
    "N_train_post = sum(N_cl)\n",
    "mean_train_post = sum([N_cl[i]*np.array(mean_cl[i])/N_train_post for i in range(cl)])\n",
    "std_train_post = np.sqrt(sum([((N_cl[i]-1)*np.array(std_cl[i])**2+N_cl[i]*np.array(mean_cl[i])**2)/(N_train_post-cl) for i in range(cl)])-(N_train_post/(N_train_post-cl))*mean_train_post**2)\n",
    "\n",
    "print(\"N_train_post\", N_train_post)\n",
    "print(\"mean_train_post\", mean_train_post)\n",
    "print(\"std_train_post\",std_train_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each dataset we will remove randomly 50% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "# 50% of missing data for client 1, 30% for client 2, 60% for client 3\n",
    "perc_miss_list = [0.5,0.3,0.6] \n",
    "\n",
    "Clients_missing = []\n",
    "for perc,c in enumerate(Clients_data):\n",
    "    perc_miss=perc_miss_list[perc]\n",
    "    n = c.shape[0] # number of observations\n",
    "    p = c.shape[1] # number of features\n",
    "    xmiss = np.copy(c)\n",
    "    #xmiss = (xmiss - np.mean(xmiss,0))/np.std(xmiss,0)\n",
    "    xmiss_flat = xmiss.flatten()\n",
    "    miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                    replace=False)\n",
    "    xmiss_flat[miss_pattern] = np.nan \n",
    "    xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "    mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "    Clients_missing.append(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([65, 70, 62, 72, 64, 65, 73, 62, 63, 70, 72, 63, 64]), array([ 89,  87,  96,  91,  89,  92,  91,  88,  97, 102,  98,  96,  95]), array([61, 50, 48, 57, 51, 54, 58, 58, 59, 58, 63, 50, 51])]\n",
      "[215 207 206 220 204 211 222 208 219 230 233 209 210]\n",
      "N_train_post after missing [215 207 206 220 204 211 222 208 219 230 233 209 210]\n",
      "mean_train_post after missing [3.07500809e+00 1.11014493e+01 1.12669903e+01 7.72727273e-02\n",
      " 5.58312745e-01 6.34579147e+00 7.05545045e+01 3.82085865e+00\n",
      " 9.24657534e+00 4.15421739e+02 1.83309013e+01 3.51897033e+02\n",
      " 1.23715238e+01]\n",
      "std_train_post after missing [8.04864157e+00 2.30081521e+01 6.92799094e+00 2.67054724e-01\n",
      " 1.10852174e-01 7.53521123e-01 2.74137196e+01 2.09178388e+00\n",
      " 8.56198414e+00 1.67620328e+02 2.22012681e+00 9.87384587e+01\n",
      " 6.58858144e+00]\n"
     ]
    }
   ],
   "source": [
    "p = Clients_missing[0].shape[1]\n",
    "N_cl = [np.array([Clients_missing[c][:,dim].size - np.count_nonzero(np.isnan(Clients_missing[c][:,dim])) for dim in range(p)]) for c in range(cl)]\n",
    "print(N_cl)\n",
    "mean_cl = [np.nanmean(i,0) for i in Clients_missing]\n",
    "std_cl = [np.nanstd(i,0) for i in Clients_missing]\n",
    "cl = len(Clients_missing)\n",
    "\n",
    "N_train_post = np.array(sum([N_cl[c] for c in range(cl)]))\n",
    "print(N_train_post)\n",
    "mean_train_post = np.array(sum([N_cl[i]*mean_cl[i]/N_train_post for i in range(cl)]))\n",
    "std_train_post = np.sqrt(sum([((N_cl[i]-1)*(std_cl[i]**2)+N_cl[i]*(mean_cl[i]**2))/(N_train_post-cl) for i in range(cl)])-(N_train_post/(N_train_post-cl))*mean_train_post**2)\n",
    "\n",
    "print(\"N_train_post after missing\", N_train_post)\n",
    "print(\"mean_train_post after missing\", mean_train_post)\n",
    "print(\"std_train_post after missing\",std_train_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clients_missing_norm = []\n",
    "for data in Clients_missing:\n",
    "    data_norm = np.copy(data)\n",
    "    data_norm = (data_norm - mean_train_post)/std_train_post\n",
    "    Clients_missing_norm.append(data_norm)\n",
    "\n",
    "import os \n",
    "os.makedirs('data/clients_data', exist_ok=True) \n",
    "for i in range(len(Clients_missing)):\n",
    "    pd.DataFrame(Clients_missing[i]).to_csv('data/clients_data/client_'+str(i+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean centralized after missing [3.07500809e+00 1.11014493e+01 1.12669903e+01 7.72727273e-02\n",
      " 5.58312745e-01 6.34579147e+00 7.05545045e+01 3.82085865e+00\n",
      " 9.24657534e+00 4.15421739e+02 1.83309013e+01 3.51897033e+02\n",
      " 1.23715238e+01]\n",
      "std centralized after missing [8.05344570e+00 2.30072840e+01 6.92754048e+00 2.67023694e-01\n",
      " 1.10866213e-01 7.53666761e-01 2.74117840e+01 2.09158817e+00\n",
      " 8.56207122e+00 1.67622662e+02 2.21998097e+00 9.88107497e+01\n",
      " 6.58763897e+00]\n"
     ]
    }
   ],
   "source": [
    "### We centralize all data to evaluate mean and std with missing\n",
    "#print([i.shape for i in Clients_missing])\n",
    "Clients_missing_tot = np.concatenate(Clients_missing,axis=0)\n",
    "#print(Clients_missing_tot.shape)\n",
    "mean_clients_missing = np.nanmean(Clients_missing_tot,0)\n",
    "std_clients_missing = np.nanstd(Clients_missing_tot,0)\n",
    "\n",
    "print(\"mean centralized after missing\", mean_clients_missing)\n",
    "print(\"std centralized after missing\", std_clients_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 (csv) to add client_1 dataset to the first node\n",
    "  * Provide the correct tag by entering:  breast_cancer\n",
    "  * Pick the folder where client_1 dataset has been saved\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online.\n",
    "4. Following the same procedure, you can create additional nodes for clients 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)\n",
    "xx = req.list()\n",
    "dataset_size = [xx[i][0]['shape'][1] for i in xx]\n",
    "assert min(dataset_size)==max(dataset_size)\n",
    "data_size = dataset_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover global mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class FedMeanStdTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(FedMeanStdTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"import pandas as pd\",\n",
    "               \"import numpy as np\",\n",
    "               \"from copy import deepcopy\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.n_features=model_args['n_features']\n",
    "        \n",
    "        self.mean = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "        self.size = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "        self.fake = nn.Parameter(torch.randn(1),requires_grad=True)\n",
    "        \n",
    "    def training_data(self):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        \n",
    "        ### NOTE: batch_size should be == dataset size ###\n",
    "        batch_size = df.shape[0]\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        ### NOTE: we keep nan when data is missing\n",
    "        #xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        \n",
    "        ### Implementing with np.nanmean, np.nanstd\n",
    "        \n",
    "        data_np = data.numpy()\n",
    "        self.size += torch.Tensor([data_np[:,dim].size - np.count_nonzero(np.isnan(data_np[:,dim]))\\\n",
    "                                   for dim in range(self.n_features)])\n",
    "        self.mean += torch.from_numpy(np.nanmean(data_np,0))\n",
    "        self.std += torch.from_numpy(np.nanstd(data_np,0))\n",
    "        \n",
    "        # ### Implementing with torch.mean, torch.std\n",
    "        \n",
    "        # size_loc = torch.zeros(self.n_features)\n",
    "        # mean_loc = torch.zeros(self.n_features)\n",
    "        # std_loc = torch.zeros(self.n_features)\n",
    "        # for dim in range(self.n_features):\n",
    "        #     data_i = deepcopy(data[:,dim][mask[:,dim].bool()])\n",
    "        #     size_loc[dim] = data_i.shape[0]\n",
    "        #     mean_loc[dim] = torch.mean(data_i, dim=0)\n",
    "        #     std_loc[dim] = torch.std(data_i, unbiased=False, dim=0)\n",
    "        # self.size += size_loc\n",
    "        # self.mean += mean_loc\n",
    "        # self.std += std_loc\n",
    "        \n",
    "        return self.fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {'n_features':data_size}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['breast_cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedstandard import FedStandard\n",
    "\n",
    "fed_mean_std = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=FedMeanStdTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=1,\n",
    "                 aggregator=FedStandard(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_mean_std.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_mean = fed_mean_std.aggregated_params()[0]['params']['fed_mean']\n",
    "fed_std = fed_mean_std.aggregated_params()[0]['params']['fed_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fed_mean': tensor([3.0750e+00, 1.1101e+01, 1.1267e+01, 7.7273e-02, 5.5831e-01, 6.3458e+00,\n",
      "        7.0555e+01, 3.8209e+00, 9.2466e+00, 4.1542e+02, 1.8331e+01, 3.5190e+02,\n",
      "        1.2372e+01], dtype=torch.float64), 'fed_std': tensor([8.0486e+00, 2.3008e+01, 6.9280e+00, 2.6705e-01, 1.1085e-01, 7.5352e-01,\n",
      "        2.7414e+01, 2.0918e+00, 8.5620e+00, 1.6762e+02, 2.2201e+00, 9.8738e+01,\n",
      "        6.5886e+00], dtype=torch.float64), 'N_tot': tensor([215., 207., 206., 220., 204., 211., 222., 208., 219., 230., 233., 209.,\n",
      "        210.], dtype=torch.float64)}\n",
      "[3.07500809e+00 1.11014493e+01 1.12669903e+01 7.72727273e-02\n",
      " 5.58312745e-01 6.34579147e+00 7.05545045e+01 3.82085865e+00\n",
      " 9.24657534e+00 4.15421739e+02 1.83309013e+01 3.51897033e+02\n",
      " 1.23715238e+01]\n",
      "[8.05344570e+00 2.30072840e+01 6.92754048e+00 2.67023694e-01\n",
      " 1.10866213e-01 7.53666761e-01 2.74117840e+01 2.09158817e+00\n",
      " 8.56207122e+00 1.67622662e+02 2.21998097e+00 9.88107497e+01\n",
      " 6.58763897e+00]\n",
      "tensor([ 6.0892e-08, -1.9350e-07,  1.6666e-07,  9.1439e-10,  1.8380e-08,\n",
      "         1.5543e-07,  6.7290e-06, -4.4811e-07,  2.5693e-07,  2.6537e-06,\n",
      "        -2.6015e-06,  6.0066e-05,  6.6975e-07], dtype=torch.float64)\n",
      "tensor([-4.8047e-03,  8.7256e-04,  4.5076e-04,  3.1029e-05, -1.4032e-05,\n",
      "        -1.4563e-04,  1.9381e-03,  1.9577e-04, -8.6844e-05, -2.3104e-03,\n",
      "         1.4601e-04, -7.2291e-02,  9.4263e-04], dtype=torch.float64)\n",
      "torch.float64 torch.float64\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(fed_mean_std.aggregated_params()[0]['params'])\n",
    "print(mean_clients_missing)\n",
    "print(std_clients_missing)\n",
    "print(fed_mean-mean_clients_missing)\n",
    "print(fed_std-std_clients_missing)\n",
    "print(fed_mean.dtype, fed_std.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiment model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MIWAETrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we include a function, ``standardize_data``, which allow to standardize data either with respect to a mean and std provided by the user, or locally, considering only local data for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "import pandas as pd\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MIWAETrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MIWAETrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"import torch.distributions as td\",\n",
    "               \"import pandas as pd\",\n",
    "               \"import numpy as np\"]\n",
    "        \n",
    "        self.n_features=model_args['n_features']\n",
    "        self.n_latent=model_args['n_latent']\n",
    "        self.n_hidden=model_args['n_hidden']\n",
    "        self.n_samples=model_args['n_samples']\n",
    "        \n",
    "        if 'standardization' in model_args:\n",
    "            self.standardization = True\n",
    "            if (('fed_mean' in model_args['standardization']) and ('fed_std' in model_args['standardization'])):\n",
    "                self.fed_mean = np.array(model_args['standardization']['fed_mean'])\n",
    "                self.fed_std = np.array(model_args['standardization']['fed_std'])\n",
    "            else:\n",
    "                self.fed_mean = None\n",
    "                self.fed_std = None\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        # the encoder will output both the mean and the diagonal covariance\n",
    "        self.encoder=nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_features, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 2*self.n_latent),  \n",
    "                        )\n",
    "        # the decoder will output both the mean, the scale, \n",
    "        # and the number of degrees of freedoms (hence the 3*p)\n",
    "        self.decoder = nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_latent, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 3*self.n_features),  \n",
    "                        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) \\\n",
    "                                    + list(self.decoder.parameters()),lr=1e-3)\n",
    "              \n",
    "        self.encoder.apply(self.weights_init)\n",
    "        self.decoder.apply(self.weights_init)\n",
    "    \n",
    "    def weights_init(self,layer):\n",
    "        if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "    \n",
    "    def miwae_loss(self,iota_x,mask):\n",
    "        batch_size = iota_x.shape[0]\n",
    "        out_encoder = self.encoder(iota_x)\n",
    "        # prior\n",
    "        p_z = td.Independent(td.Normal(loc=torch.zeros(self.n_latent).to(self.device)\\\n",
    "                                       ,scale=torch.ones(self.n_latent).to(self.device)),1)\n",
    "        \n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :self.n_latent],\\\n",
    "                                                scale=torch.nn.Softplus()\\\n",
    "                                                (out_encoder[..., self.n_latent:\\\n",
    "                                                             (2*self.n_latent)])),1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([self.n_samples])\n",
    "        zgivenx_flat = zgivenx.reshape([self.n_samples*batch_size,self.n_latent])\n",
    "\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        all_means_obs_model = out_decoder[..., :self.n_features]\n",
    "        all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., self.n_features:\\\n",
    "                                                               (2*self.n_features)]) + 0.001\n",
    "        all_degfreedom_obs_model = torch.nn.Softplus()\\\n",
    "        (out_decoder[..., (2*self.n_features):(3*self.n_features)]) + 3\n",
    "\n",
    "        data_flat = torch.Tensor.repeat(iota_x,[self.n_samples,1]).reshape([-1,1])\n",
    "        tiledmask = torch.Tensor.repeat(mask,[self.n_samples,1])\n",
    "\n",
    "        all_log_pxgivenz_flat = torch.distributions.StudentT\\\n",
    "        (loc=all_means_obs_model.reshape([-1,1]),\\\n",
    "         scale=all_scales_obs_model.reshape([-1,1]),\\\n",
    "         df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.n_samples*batch_size,self.n_features])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([self.n_samples,batch_size])\n",
    "        logpz = p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "        return neg_bound\n",
    "\n",
    "    def training_data(self,  batch_size = 48):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        # xhat_0: missing values are replaced by zeros. \n",
    "        #This x_hat0 is what will be fed to our encoder.\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        \n",
    "        # Data standardization\n",
    "        if self.standardization:\n",
    "            xhat_0 = self.standardize_data(xhat_0)\n",
    "            \n",
    "        xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def standardize_data(self,data):\n",
    "        data_norm = np.copy(data)\n",
    "        if ((self.fed_mean is not None) and (self.fed_std is not None)):\n",
    "            print('FEDERATED STANDARDIZATION')\n",
    "            data_norm = (data_norm - self.fed_mean)/self.fed_std\n",
    "        else:\n",
    "            print('LOCAL STANDARDIZATION')\n",
    "            data_norm = (data_norm - np.nanmean(data_norm,0))/np.nanstd(data_norm,0)\n",
    "        return data_norm\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        self.encoder.zero_grad()\n",
    "        self.decoder.zero_grad()\n",
    "        loss = self.miwae_loss(iota_x = data,mask = mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. \n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "* data `tags` to search nodes for training.\n",
    "* total number of `rounds`.\n",
    "If FedProx optimisation is requested, `fedprox_mu` parameter must be defined here. It also must be a float between XX and YY.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "h = 128 # number of hidden units in (same for all MLPs)\n",
    "d = 10 # dimension of the latent space, we choose d=1 for visualisation purposes\n",
    "K = 20 # number of IS during training\n",
    "\n",
    "n_epochs=5\n",
    "\n",
    "print(type(fed_mean.tolist()[0]))\n",
    "\n",
    "model_args = {'n_features':data_size, 'n_latent':d,'n_hidden':h,'n_samples':K,'standardization':{'fed_mean':fed_mean.tolist(),'fed_std':fed_std.tolist()}}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': n_epochs, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['breast_cancer']\n",
    "rounds = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the federated training but using FedProx as aggregation scheme (starting from the second iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the first round we will simply use FedAvg \n",
    "# with standard optimization scheme: the FedProx penalization\n",
    "# term will be introduced exclusively from the second round.\n",
    "# training_args.update(fedprox_mu = 0.)\n",
    "\n",
    "exp_fedprox = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the second round, FedProx is used with mu=0.1\n",
    "# We first update the training args\n",
    "training_args.update(fedprox_mu = 0.1)\n",
    "\n",
    "# Then update training args in the experiment\n",
    "exp_fedprox.set_training_args(training_args)\n",
    "exp_fedprox.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx and performing the standardization locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we propose to use FedCos as well, which introduce an alternative penalization term with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del training_args['fedprox_mu'] \n",
    "\n",
    "model_args.update(standardization = {})\n",
    "\n",
    "exp_fedprox_std_local = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox_std_local.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.update(fedprox_mu = 0.1)\n",
    "\n",
    "exp_fedprox_std_local.set_training_args(training_args)\n",
    "exp_fedprox_std_local.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and comparison to local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing on an external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we are going to test the performance of the final federated model to impute missing data on a test dataset. To this extent we are going to remove randomly 50% of samples from the test dataset, `data_test`, defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the test dataset, we will remove randomly 50% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "perc_miss = 0.5 # 50% of missing data\n",
    "\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "\n",
    "xmiss = np.copy(data_test)\n",
    "xmiss_flat = xmiss.flatten()\n",
    "miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                replace=False)\n",
    "xmiss_flat[miss_pattern] = np.nan \n",
    "xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "\n",
    "mask_test = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "\n",
    "mean_test_missing = np.nanmean(xmiss,0)\n",
    "std_test_missing = np.nanstd(xmiss,0)\n",
    "xmiss = (xmiss - mean_test_missing)/std_test_missing\n",
    "\n",
    "xhat_0_test = np.copy(xmiss)\n",
    "xhat_0_test[np.isnan(xmiss)] = 0\n",
    "xhat_test = np.copy(xhat_0_test) # This will be out imputed data matrix\n",
    "\n",
    "xfull_test = np.copy(data_test)\n",
    "xfull_test = (xfull_test - mean_test_missing)/std_test_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the MIWAE imputation routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_impute(encoder,decoder,iota_x,mask,d,L):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([L])\n",
    "    zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[L,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
    "\n",
    "    imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
    "    xms = xgivenz.mean.reshape([L,batch_size,p])  # that's the only line that changed!\n",
    "    xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
    "\n",
    "    return xm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the MSE function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
    "    xhat = np.array(xhat)\n",
    "    xtrue = np.array(xtrue)\n",
    "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model using last updated federated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model into PyTorch framework\n",
    "model = exp.model_instance()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the models trained with FedProx and FedCos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fedprox = exp_fedprox.model_instance()\n",
    "model_fedprox.load_state_dict(exp_fedprox.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox = model_fedprox.encoder\n",
    "decoder_fedprox = model_fedprox.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally do the imputation and evaluate the corresponding imputation error through MSE for each federated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on testing data 0.772005\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on testing data  0.7994\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "L = 100\n",
    "\n",
    "xhat = np.copy(xhat_test)\n",
    "mask = np.copy(mask_test)\n",
    "xhat_0 = np.copy(xhat_0_test)\n",
    "xfull = np.copy(xfull_test)\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder,decoder = decoder,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model on testing data %g' %err_test_data)\n",
    "print('-----')\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedprox = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedprox) on testing data  %g' %err_test_data_fedprox)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing on a client's dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use the final federated model to impute missing data of client 1, which have been used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on data from client 1  0.555779\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on data from client 1  0.544626\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We first recover data (full and with missing entries) from client 1\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xfull_cl1 = np.copy(Clients_data[0])\n",
    "xfull_cl1 = (xfull_cl1 - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "xmiss_cl1 = (xmiss_cl1 - fed_mean.numpy())/fed_std.numpy()\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "### Now we do the imputation\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model on data from client 1  %g' %err_cl1_data)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 1  %g' %err_cl1_data_fedprox)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing of FedProx model with local standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the federated model with FedProx, where data standardization is performed locally. In order to be as much coherent as possible, each time the standardization will be realized locally as well in the testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model (with fedprox and local standardization) on testing data  0.77014\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox and local standardization) on data from client 1  0.768991\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We recover the model\n",
    "model_fedprox_std_local = exp_fedprox_std_local.model_instance()\n",
    "model_fedprox_std_local.load_state_dict(exp_fedprox_std_local.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox_std_local = model_fedprox_std_local.encoder\n",
    "decoder_fedprox_std_local = model_fedprox_std_local.decoder\n",
    "\n",
    "# We do the imputation on the test data\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xhat = np.copy(xhat_test)\n",
    "mask = np.copy(mask_test)\n",
    "xhat_0 = np.copy(xhat_0_test)\n",
    "xfull = np.copy(xfull_test)\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedprox_std_local = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on testing data  %g' %err_test_data_fedprox_std_local)\n",
    "print('-----')\n",
    "\n",
    "# Same for the dataset from client 1. In this case the dataset standardization is done with respect to his own data\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "mean_cl1_missing = np.nanmean(xmiss_cl1,0)\n",
    "std_cl1_missing = np.nanstd(xmiss_cl1,0)\n",
    "xmiss_cl1 = (xmiss_cl1 - mean_cl1_missing)/std_cl1_missing\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "xfull_cl1 = np.copy(Clients_data[0])\n",
    "xfull_cl1 = (xfull_cl1 - mean_cl1_missing)/std_cl1_missing\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox_std_local = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on data from client 1  %g' %err_cl1_data_fedprox_std_local)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local training and testing on a client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the performance of the same model trained locally and tested on the dataset from client 1. We will use a total of `epochs`x`rounds` local epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_loss(encoder,decoder,iota_x,mask,d):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([K])\n",
    "    zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "    return neg_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the local training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MIWAE likelihood bound  -9.09662\n",
      "Epoch 51\n",
      "MIWAE likelihood bound  -3.76211\n",
      "Epoch 101\n",
      "MIWAE likelihood bound  -3.02126\n",
      "Epoch 151\n",
      "MIWAE likelihood bound  -2.67265\n",
      "Epoch 201\n",
      "MIWAE likelihood bound  -1.76575\n"
     ]
    }
   ],
   "source": [
    "# Recall all hyperparameters\n",
    "\n",
    "n_epochs_local = n_epochs*rounds\n",
    "\n",
    "bs = training_args.get('batch_size')\n",
    "lr = training_args.get('lr')\n",
    "\n",
    "n = xfull_cl1.shape[0] # number of observations\n",
    "p = xfull_cl1.shape[1] # number of features\n",
    "\n",
    "h = model_args.get('n_hidden') \n",
    "d = model_args.get('n_latent') \n",
    "K = model_args.get('n_samples') \n",
    "\n",
    "# Data\n",
    "\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "mean_cl1_missing = np.nanmean(xmiss_cl1,0)\n",
    "std_cl1_missing = np.nanstd(xmiss_cl1,0)\n",
    "xmiss_cl1 = (xmiss_cl1 - mean_cl1_missing)/std_cl1_missing\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "xfull_cl1 = np.copy(Clients_data[0])\n",
    "xfull_cl1 = (xfull_cl1 - mean_cl1_missing)/std_cl1_missing\n",
    "\n",
    "encoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl1 = torch.optim.Adam(list(encoder_cl1.parameters()) + list(decoder_cl1.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl1.apply(weights_init)\n",
    "decoder_cl1.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl1[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl1[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl1.zero_grad()\n",
    "        encoder_cl1.zero_grad()\n",
    "        decoder_cl1.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(encoder = encoder_cl1,decoder = decoder_cl1, iota_x = b_data,mask = b_mask, d = d)\n",
    "        loss.backward()\n",
    "        optimizer_cl1.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(encoder = encoder_cl1,decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(), d = d).cpu().data.numpy())) # Gradient step      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do the imputation on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on data from same client (cl 1)  0.807842\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_cl1, decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on data from same client (cl 1)  %g' %err_local_cl1_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the imputation on the external test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on testing data 0.889466\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xhat = np.copy(xhat_test)\n",
    "mask = np.copy(mask_test)\n",
    "xhat_0 = np.copy(xhat_0_test)\n",
    "xfull = np.copy(xfull_test)\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_cl1,decoder = decoder_cl1,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_local_cl1_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of local model on testing data %g' %err_local_cl1_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Local training on centralized data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We centralized data from all clients, and perform the local training, hence test results on the external testing dataset as well as data from client 1, as we deed for the others models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MIWAE likelihood bound  -9.29921\n",
      "Epoch 51\n",
      "MIWAE likelihood bound  -3.01712\n",
      "Epoch 101\n",
      "MIWAE likelihood bound  -2.26313\n",
      "Epoch 151\n",
      "MIWAE likelihood bound  -1.73027\n",
      "Epoch 201\n",
      "MIWAE likelihood bound  -1.21782\n",
      "Epoch 251\n",
      "MIWAE likelihood bound  -0.0541024\n",
      "Epoch 301\n",
      "MIWAE likelihood bound  0.435457\n",
      "Epoch 351\n",
      "MIWAE likelihood bound  1.02151\n",
      "Epoch 401\n",
      "MIWAE likelihood bound  0.92007\n",
      "Epoch 451\n",
      "MIWAE likelihood bound  0.783818\n",
      "Epoch 501\n",
      "MIWAE likelihood bound  1.40394\n",
      "Epoch 551\n",
      "MIWAE likelihood bound  1.28166\n",
      "Epoch 601\n",
      "MIWAE likelihood bound  2.22676\n",
      "Epoch 651\n",
      "MIWAE likelihood bound  2.30973\n",
      "Epoch 701\n",
      "MIWAE likelihood bound  2.1331\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "\n",
    "n_epochs_local = n_epochs*rounds*len(Clients_missing)\n",
    "\n",
    "# Data\n",
    "\n",
    "xmiss_tot = np.concatenate(Clients_missing,axis=0)\n",
    "\n",
    "n = xmiss_tot.shape[0] # number of observations\n",
    "p = xmiss_tot.shape[1] # number of features\n",
    "\n",
    "mean_tot_missing = np.nanmean(xmiss_tot,0)\n",
    "std_tot_missing = np.nanstd(xmiss_tot,0)\n",
    "xmiss_tot = (xmiss_tot - mean_tot_missing)/std_tot_missing\n",
    "mask_tot = np.isfinite(xmiss_tot) # binary mask that indicates which values are missing\n",
    "xhat_0_tot = np.copy(xmiss_tot)\n",
    "xhat_0_tot[np.isnan(xmiss_tot)] = 0\n",
    "xhat_tot = np.copy(xhat_0_tot) # This will be out imputed data matrix\n",
    "\n",
    "xfull_tot = np.concatenate(Clients_data,axis=0)\n",
    "xfull_tot = (xfull_tot - mean_tot_missing)/std_tot_missing\n",
    "\n",
    "# Model\n",
    "\n",
    "encoder_tot = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_tot = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_tot = torch.optim.Adam(list(encoder_tot.parameters()) + list(decoder_tot.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_tot.apply(weights_init)\n",
    "decoder_tot.apply(weights_init)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_tot[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_tot[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_tot.zero_grad()\n",
    "        encoder_tot.zero_grad()\n",
    "        decoder_tot.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(encoder = encoder_tot,decoder = decoder_tot, iota_x = b_data,mask = b_mask, d = d)\n",
    "        loss.backward()\n",
    "        optimizer_tot.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(encoder = encoder_tot,decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_tot).float(),mask = torch.from_numpy(mask_tot).float(), d = d).cpu().data.numpy())) # Gradient step      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on the whole dataset, on data from same client 1  0.797672\n",
      "-----\n",
      "Imputation MSE of local model on the whole dataset, on testing data 0.871439\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_tot, decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_tot_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on the whole dataset, on data from same client 1  %g' %err_local_tot_cl1_data)\n",
    "print('-----')\n",
    "\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xhat = np.copy(xhat_test)\n",
    "mask = np.copy(mask_test)\n",
    "xhat_0 = np.copy(xhat_0_test)\n",
    "xfull = np.copy(xfull_test)\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_tot,decoder = decoder_tot,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_local_tot_cl1_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of local model on the whole dataset, on testing data %g' %err_local_tot_cl1_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE on testing data\n",
      "-----\n",
      "Model          Mean Squared Error (↓)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.772005\n",
      "FedProx                      0.7994\n",
      "FedLocStd                    0.77014\n",
      "Local (cl1)                  0.889466\n",
      "Centralized                  0.871439\n",
      "-----\n",
      "-----\n",
      "Imputation MSE on local data from client 1\n",
      "-----\n",
      "Model          Mean Squared Error (↓)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.555779\n",
      "FedProx                      0.544626\n",
      "FedLocStd                    0.768991\n",
      "Local (cl1)                  0.807842\n",
      "Centralized                  0.871439\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('Imputation MSE on testing data')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_test_data],\n",
    "['FedProx', err_test_data_fedprox],\n",
    "['FedLocStd', err_test_data_fedprox_std_local],\n",
    "['Local (cl1)', err_local_cl1_test_data],\n",
    "['Centralized', err_local_tot_cl1_test_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 1')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_cl1_data],\n",
    "['FedProx', err_cl1_data_fedprox],\n",
    "['FedLocStd', err_cl1_data_fedprox_std_local],\n",
    "['Local (cl1)', err_local_cl1_data],\n",
    "['Centralized', err_local_tot_cl1_test_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
