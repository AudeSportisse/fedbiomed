{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process for Approved Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fed-BioMed offers a feature to run only the pre-approved models on the nodes. The nodes that you will be sending your model might require approved models. Therefore, if the node accepts on the approved model, the model files that are sent by a researcher with the training request should be approved by the node side in adnvance. The approval process is done by a real user/person who will review the model file. The reviewer make sure the model doesn't contain any code that might cause privacy issues. In this tutorial, we will be creating a node with activated model approval option.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Node\n",
    "\n",
    "\n",
    "Enabling model approval can be done both from config file or Fed-BioMed CLI while starting the node. The process of creating and starting a node with model approval option is not so different than setting up a normal node. By default, if any option is not specified in the CLI, the node disables model approval and security section of config file looks like the snippet below. \n",
    "\n",
    "```shell\n",
    "[security]\n",
    "hashing_algorithm = SHA256\n",
    "allow_default_models = True\n",
    "model_approval = False\n",
    "```\n",
    "The Fed-BioMed CLI gets two extra parameters as `--enable-model-approval` and `--allow-default-models` to activate model approval;\n",
    "\n",
    "* `--enable-model-approval` : This parameter enables model approval for the node. If there isn't a config file for the node while running CLI, it creates a new config file with enabled model approval mode `model_approval = True`. \n",
    "* `--allow-default-models`  : This parameter allows default models for train requests. These are the models that comes for Fed-BioMed tutorilas. For example, the model for MNIST dataset that we will be using for this tutorial. If the default models are enabled, node updates/registers model file which is located in `envs/developments/default_models` directory during starting process of the node. \n",
    "\n",
    "\n",
    "### Adding MNIST Dataset to The Node. \n",
    "\n",
    "In this section we will add MNIST dataset to the node. While adding the dataset through CLi we'll also specify `--enable-model-approval` and `--allow-default-models` options. This will create new `config-n1.ini` file with following configuration. \n",
    "\n",
    "```\n",
    "[security]\n",
    "hashing_algorithm = SHA256\n",
    "allow_default_models = True\n",
    "model_approval = True\n",
    "\n",
    "```\n",
    "Now, let's run the following command. \n",
    "\n",
    "```shell\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-model-approval --allow-default-models add \n",
    "```\n",
    "\n",
    "The CLI will ask you to select the dataset type. Since we will be working on MNIST dataset, please select `2` (default) and continue by typing `y` for the next prompt and select folder that you want to store MNIST dataset. Afterward, if you go to `etc` directory of fedbiomed, you can see `config-n1.ini` file. \n",
    "\n",
    "### Starting the Node\n",
    "\n",
    "Now you can start your node by running following command; \n",
    "\n",
    "```\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini start\n",
    "```\n",
    "\n",
    "Since, config file has been configured to enable model approval mode, you do not need to specifiy any extra parameter while starting the node. But it is also possible to start node with `--enable-model-approval`, `--allow-default-models` or `--disable-model-approval`, `--disable-default-models`. If you start your node with `--disable-model-approval` it will disable model approval even it is enabled in the config file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Experiment\n",
    "\n",
    "In this section we will be using default MNIST model which has been already registered by the node.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file = tmp_dir_model.name + '/class_export_mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is the model that will be sent to the node for traning. Since the model files are processed by the Experiment to configure dependencies, import part of the final file might be different than this one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to get/see the final model file we need to initialize the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Final Model File From Experiment\n",
    "\n",
    "`get_model_file` displays the model file that will be send to the nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_model_file(display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exp.get_model_status()` sends request to the nodes to check whether the model is approved or not. The nodes that will receive the reqeusts are the nodes that have been found after searching datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = exp.check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs should indicate that the model is approved. You can also get status object from the result of the `check_model_status()`. it returns list of status objects each for different node. Since we have only launched single node, it will return only one status object. \n",
    "\n",
    "* `approval_obligaiton` : Indicates whether the model approval is enabled in the node.  \n",
    "* `is_approved`         : Indicates whether the models is approved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Model And Testing Model Approval Status\n",
    "\n",
    "Let's change the model codes and test whether it is approved or not. We will be changing the network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file_2 = tmp_dir_model.name + '/class_export_mnist_2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmp/tmppt99dkkn/class_export_mnist_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file_2\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)        \n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 15:52:58,468 fedbiomed INFO - Messaging researcher_dac6b4aa-3359-4918-a20e-36f8564c3910 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7fedf99c0e50>\n",
      "2021-11-23 15:52:58,505 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2021-11-23 15:52:58,510 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n",
      "2021-11-23 15:53:08,525 fedbiomed INFO - Node selected for training -> node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4\n",
      "2021-11-23 15:53:08,645 fedbiomed DEBUG - torchnn saved model filename: /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmpgklynhso/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "exp2 = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file_2,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we changed the codes, the output of  the following method should say that the model is not approved by the node and `is_approved` key of the result object should be equal to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = exp2.check_model_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.get_model_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is not approved, you won't be able to train your model in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 15:53:10,566 fedbiomed INFO - Sampled nodes in round 0 ['node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4']\n",
      "2021-11-23 15:53:10,571 fedbiomed INFO - Send message to node node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_79fa7bab-0e3c-4f5d-b581-ab19e12f9bb3.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}}\n",
      "2021-11-23 15:53:10,573 fedbiomed DEBUG - researcher_dac6b4aa-3359-4918-a20e-36f8564c3910\n",
      "2021-11-23 15:53:10,579 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_79fa7bab-0e3c-4f5d-b581-ab19e12f9bb3.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}}\n",
      "2021-11-23 15:53:10,581 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG [TASKS QUEUE] Item:{'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_79fa7bab-0e3c-4f5d-b581-ab19e12f9bb3.pt', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}, 'model_args': {}, 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'model_class': 'MyTrainingPlan', 'command': 'train'}\n",
      "2021-11-23 15:53:10,609 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7fd61c09ccd0>, 'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}\n",
      "2021-11-23 15:53:10,611 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Dataset_path../data\n",
      "2021-11-23 15:53:10,757 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316353\n",
      "2021-11-23 15:53:10,979 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [480/60000 (1%)]\tLoss: 1.811966\n",
      "2021-11-23 15:53:11,178 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.531069\n",
      "2021-11-23 15:53:11,547 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [1440/60000 (2%)]\tLoss: 0.373971\n",
      "2021-11-23 15:53:11,774 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.214979\n",
      "2021-11-23 15:53:12,083 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.443882\n",
      "2021-11-23 15:53:12,310 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.400795\n",
      "2021-11-23 15:53:12,480 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [3360/60000 (6%)]\tLoss: 0.385383\n",
      "2021-11-23 15:53:12,632 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.233347\n",
      "2021-11-23 15:53:12,780 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [4320/60000 (7%)]\tLoss: 0.345088\n",
      "2021-11-23 15:53:12,962 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Reached 100 batches for this epoch, ignore remaining data\n",
      "2021-11-23 15:53:12,985 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO results uploaded successfully \n",
      "2021-11-23 15:53:20,588 fedbiomed INFO - Downloading model params after training on node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - from http://localhost:8844/media/uploads/2021/11/23/node_params_02eed012-b571-48ce-ae54-b41477356895.pt\n",
      "2021-11-23 15:53:20,648 fedbiomed INFO - Nodes that successfully reply in round 0 ['node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4']\n",
      "2021-11-23 15:53:20,706 fedbiomed INFO - Sampled nodes in round 1 ['node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4']\n",
      "2021-11-23 15:53:20,708 fedbiomed INFO - Send message to node node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/researcher_params_10a831e4-b822-4beb-b01f-c8599849bcd4.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}}\n",
      "2021-11-23 15:53:20,710 fedbiomed DEBUG - researcher_dac6b4aa-3359-4918-a20e-36f8564c3910\n",
      "2021-11-23 15:53:20,724 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/researcher_params_10a831e4-b822-4beb-b01f-c8599849bcd4.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}}\n",
      "2021-11-23 15:53:20,726 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG [TASKS QUEUE] Item:{'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '791e248e-3571-4f19-83ae-d3f0264ca4e3', 'params_url': 'http://localhost:8844/media/uploads/2021/11/23/researcher_params_10a831e4-b822-4beb-b01f-c8599849bcd4.pt', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}, 'training_data': {'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4': ['dataset_9973b4d0-b884-4c05-a552-181ae8562113']}, 'model_args': {}, 'model_url': 'http://localhost:8844/media/uploads/2021/11/23/my_model_57466105-fe0f-4044-aa3a-4d894d11dff5.py', 'model_class': 'MyTrainingPlan', 'command': 'train'}\n",
      "2021-11-23 15:53:20,748 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7fd61c09cf10>, 'batch_size': 48, 'lr': 0.001, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100}\n",
      "2021-11-23 15:53:20,750 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Dataset_path../data\n",
      "2021-11-23 15:53:20,837 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.333690\n",
      "2021-11-23 15:53:21,135 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [480/60000 (1%)]\tLoss: 0.381812\n",
      "2021-11-23 15:53:21,390 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.266138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 15:53:21,715 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [1440/60000 (2%)]\tLoss: 0.192151\n",
      "2021-11-23 15:53:22,138 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.257468\n",
      "2021-11-23 15:53:22,468 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.385568\n",
      "2021-11-23 15:53:22,627 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.214053\n",
      "2021-11-23 15:53:22,913 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [3360/60000 (6%)]\tLoss: 0.124304\n",
      "2021-11-23 15:53:23,280 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.160300\n",
      "2021-11-23 15:53:23,499 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO Train Epoch: 1 [4320/60000 (7%)]\tLoss: 0.157612\n",
      "2021-11-23 15:53:23,708 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Reached 100 batches for this epoch, ignore remaining data\n",
      "2021-11-23 15:53:23,778 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - INFO results uploaded successfully \n",
      "2021-11-23 15:53:30,726 fedbiomed INFO - Downloading model params after training on node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - from http://localhost:8844/media/uploads/2021/11/23/node_params_46c02d15-9e12-4d63-a5a0-4d8684434d8d.pt\n",
      "2021-11-23 15:53:30,752 fedbiomed INFO - Nodes that successfully reply in round 1 ['node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4']\n",
      "2021-11-23 16:49:48,882 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - CRITICAL Node stopped in signal_handler, probably by user decision (Ctrl C)\n"
     ]
    }
   ],
   "source": [
    "exp2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering/Approving the Model \n",
    "\n",
    "To register/approve the model that has been created in the previous section, we can use Fed-BioMed CLI. You do not need to stop your node to register new models you can perfom registeritation process in a different terminal window. However, first we need to get final model from `exp2` object \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "<div class=\"note\">\n",
    "<p>\n",
    "    In the previous notebook cells, we tried to run a model which is not approved by the node. Therefore, your notebook kernel should have been killed. You might need to restart your kernel to be able to run your expirement. After restarting, please do not forget to create second model file and the experiment in the previous section. Otherwise, for the following cells you might get an error that says the <code>exp</code> is not defined or, <code>model_file_2</code> is doesn't exist.  \n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.get_model_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `exp2.get_model_file` is a file a path that show where the final model is saved. It also prints the content of the model file. You can either get the content of model from the output cell or the path where it is save. Anyway, you need to create a new `txt` file and copy the model content in it. You can create new directory in Fedi-BioMed call `models` and inside it you can create new `my-model.txt` file and copy the model content into it. \n",
    "\n",
    "Afterward, please run following command in other terminal to register model file. \n",
    "\n",
    "```shell\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --register-model\n",
    "```\n",
    "\n",
    "You should type a unique name for your model e.g. 'MyTestModel-1' and a description. The CLI will ask you select model file you want to register. Select the file that you saved and continue. \n",
    "\n",
    "Now, you should be able to train your model. \n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.check_model_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
