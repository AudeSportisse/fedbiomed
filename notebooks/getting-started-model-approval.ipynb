{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. By default model approval won't be enabled. Thats't you should start the by indicating that you want enable model approval and default models. Defaults models are the models that will be automaticly regsitered while starting the node. Please run, `./scripts/fedbiomed_run node --enable-model-approval --allow-default-models start`. As output should be presented as follows. \n",
    "\n",
    "```\n",
    "\t- ðŸ†” Your node ID: node_ba338496-736c-471d-8e0c-944493a36e57 \n",
    "\n",
    "2021-11-22 17:36:31,408 fedbiomed INFO - Node started as process with pid = 42355\n",
    "To stop press Ctrl + C.\n",
    "2021-11-22 17:36:31,409 fedbiomed INFO - Launching node...\n",
    "2021-11-22 17:36:31,409 fedbiomed INFO - Checking hashes for registered models...\n",
    "2021-11-22 17:36:31,410 fedbiomed INFO - There is no models registered\n",
    "2021-11-22 17:36:31,410 fedbiomed INFO - Loading default models\n",
    "2021-11-22 17:36:31,563 fedbiomed INFO - Starting communication channel with network\n",
    "2021-11-22 17:36:31,571 fedbiomed INFO - Messaging node_ba338496-736c-471d-8e0c-944493a36e57 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f4b44843280>\n",
    "2021-11-22 17:36:31,577 fedbiomed DEBUG -  adding handler: MQTT\n",
    "2021-11-22 17:36:31,577 fedbiomed INFO - Starting task manager\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file = tmp_dir_model.name + '/class_export_mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : write **only** the code to export in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmp/tmpgw_010lb/class_export_mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `rounds` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 17:43:33,063 fedbiomed INFO - Messaging researcher_dac6b4aa-3359-4918-a20e-36f8564c3910 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7fa4e86bd7f0>\n",
      "2021-11-22 17:43:33,111 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2021-11-22 17:43:33,113 fedbiomed INFO - log from: node_ba338496-736c-471d-8e0c-944493a36e57 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n",
      "2021-11-22 17:43:33,113 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n",
      "2021-11-22 17:43:43,147 fedbiomed INFO - Node selected for training -> node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4\n",
      "2021-11-22 17:43:43,257 fedbiomed DEBUG - torchnn saved model filename: /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmpwujcu76s/my_model_2b5987a1-4d77-4719-8565-5e434e970086.py\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Final Model File From Experiment\n",
    "\n",
    "`get_model_file` displays the model model file that will be send to the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "from torchvision import datasets, transforms\n",
      "from torchvision import datasets, transforms\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "class MyTrainingPlan(TorchTrainingPlan):\n",
      "    def __init__(self):\n",
      "        super(MyTrainingPlan, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
      "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
      "        self.dropout1 = nn.Dropout(0.25)\n",
      "        self.dropout2 = nn.Dropout(0.5)\n",
      "        self.fc1 = nn.Linear(9216, 128)\n",
      "        self.fc2 = nn.Linear(128, 10)\n",
      "        \n",
      "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
      "        # In this case, we need the torch DataLoader classes\n",
      "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
      "        deps = [\"from torchvision import datasets, transforms\",\n",
      "               \"from torch.utils.data import DataLoader\"]\n",
      "        self.add_dependency(deps)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.conv1(x)\n",
      "        x = F.relu(x)\n",
      "        x = self.conv2(x)\n",
      "        x = F.relu(x)\n",
      "        x = F.max_pool2d(x, 2)\n",
      "        x = self.dropout1(x)\n",
      "        x = torch.flatten(x, 1)\n",
      "        x = self.fc1(x)\n",
      "        x = F.relu(x)\n",
      "        x = self.dropout2(x)\n",
      "        x = self.fc2(x)\n",
      "        \n",
      "        \n",
      "        output = F.log_softmax(x, dim=1)\n",
      "        return output\n",
      "\n",
      "    def training_data(self, batch_size = 48):\n",
      "        # Custom torch Dataloader for MNIST data\n",
      "        transform = transforms.Compose([transforms.ToTensor(),\n",
      "        transforms.Normalize((0.1307,), (0.3081,))])\n",
      "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
      "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
      "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
      "        return data_loader\n",
      "    \n",
      "    def training_step(self, data, target):\n",
      "        output = self.forward(data)\n",
      "        loss   = torch.nn.functional.nll_loss(output, target)\n",
      "        return loss\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmpwujcu76s/my_model_2b5987a1-4d77-4719-8565-5e434e970086.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.get_model_file(display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exp.get_model_status()` send request to nodes that have been found after dataset search to check whether the model is approved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 17:43:46,994 fedbiomed INFO - Sending request to node node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 to check model is approved or not\n",
      "2021-11-22 17:43:46,995 fedbiomed DEBUG - researcher_dac6b4aa-3359-4918-a20e-36f8564c3910\n",
      "2021-11-22 17:43:47,022 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': 'b33424ee-0d2f-4afa-860c-8ebdb7bea24a', 'model_url': 'http://localhost:8844/media/uploads/2021/11/22/my_model_2b5987a1-4d77-4719-8565-5e434e970086.py', 'command': 'model-status'}\n",
      "2021-11-22 17:43:57,006 fedbiomed INFO - Model has been approved by the node: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910',\n",
       "  'node_id': 'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4',\n",
       "  'job_id': 'b33424ee-0d2f-4afa-860c-8ebdb7bea24a',\n",
       "  'success': True,\n",
       "  'approval_obligation': True,\n",
       "  'is_approved': True,\n",
       "  'msg': 'Model is approved by the node',\n",
       "  'model_url': 'http://localhost:8844/media/uploads/2021/11/22/my_model_2b5987a1-4d77-4719-8565-5e434e970086.py',\n",
       "  'command': 'model-status'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Model And Testing Model Approval Status\n",
    "\n",
    "Let's change the model codes and test whether it is approved or not. All we'll do is the add `print` function in `traning_data` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file_2 = tmp_dir_model.name + '/class_export_mnist_2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmp/tmp4vk9uq4_/class_export_mnist_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file_2\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        \n",
    "        # New added code\n",
    "        print(dataset1)\n",
    "        \n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 17:44:23,211 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2021-11-22 17:44:23,213 fedbiomed INFO - log from: node_ba338496-736c-471d-8e0c-944493a36e57 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n",
      "2021-11-22 17:44:23,214 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'tags': ['#MNIST', '#dataset'], 'command': 'search'}\n",
      "2021-11-22 17:44:33,222 fedbiomed INFO - Node selected for training -> node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4\n",
      "2021-11-22 17:44:33,265 fedbiomed DEBUG - torchnn saved model filename: /user/scansiz/home/Desktop/Inria/development/fedbiomed/var/tmprgrylmi5/my_model_15521569-303f-42a1-a7dd-ef797b7d8a0d.py\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp2 = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file_2,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we changed the codes follwing method should say that the model is not approved by the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 17:44:33,479 fedbiomed INFO - Sending request to node node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 to check model is approved or not\n",
      "2021-11-22 17:44:33,480 fedbiomed DEBUG - researcher_dac6b4aa-3359-4918-a20e-36f8564c3910\n",
      "2021-11-22 17:44:33,506 fedbiomed INFO - log from: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4 - DEBUG Message received: {'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910', 'job_id': '11f11c99-f065-49de-b6a2-a74da2492d29', 'model_url': 'http://localhost:8844/media/uploads/2021/11/22/my_model_15521569-303f-42a1-a7dd-ef797b7d8a0d.py', 'command': 'model-status'}\n",
      "2021-11-22 17:44:43,520 fedbiomed WARNING - Model has NOT been approved by the node: node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'researcher_id': 'researcher_dac6b4aa-3359-4918-a20e-36f8564c3910',\n",
       "  'node_id': 'node_5f0bce0e-4f54-4ca1-aecf-f0a9c0ba1bd4',\n",
       "  'job_id': '11f11c99-f065-49de-b6a2-a74da2492d29',\n",
       "  'success': True,\n",
       "  'approval_obligation': True,\n",
       "  'is_approved': False,\n",
       "  'msg': 'Model is not approved by the node',\n",
       "  'model_url': 'http://localhost:8844/media/uploads/2021/11/22/my_model_15521569-303f-42a1-a7dd-ef797b7d8a0d.py',\n",
       "  'command': 'model-status'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2.check_model_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
