{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data imputation with Fedbiomed using MIWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to impute missing not at random (MAR) data in a federated setting using MIWAE (https://arxiv.org/abs/2006.12871). We will compare results of federated training using FedAvg and FedProx with local results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we will use the data extracted from ADNI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311, 130)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "cwd = os.getcwd()\n",
    "data_file = os.path.join(cwd, \"data/ADNI/data_irene_dx_2g_corrected.csv\")\n",
    "raw_df = pd.read_csv(data_file, sep=\",\",index_col=\"RID\")\n",
    "target = raw_df[\"DX\"]\n",
    "data = deepcopy(raw_df)\n",
    "del data[\"DX\"]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, target, test_size=0.20, random_state=42)\n",
    "\n",
    "# split train across datasets in a non-iid manner: \n",
    "#client 1 will only contain ADNI subjects, client 2 will only contain controls\n",
    "client_1 = data_train.loc[labels_train == 'AD']\n",
    "client_2 = data_train.loc[labels_train == 'NL']\n",
    "\n",
    "#Clients_data contains original full data of each client \n",
    "Clients_data=[client_1, client_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each dataset we will remove randomly 30% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "# 30% of missing data for client 1, 30% for client 2\n",
    "perc_miss_list = [0.3,0.3] \n",
    "\n",
    "#Clients_data contains data of each client with missing entries wrt to perc_miss_list\n",
    "Clients_missing = []\n",
    "for perc,c in enumerate(Clients_data):\n",
    "    perc_miss=perc_miss_list[perc]\n",
    "    n = c.shape[0] # number of observations\n",
    "    p = c.shape[1] # number of features\n",
    "    xmiss = np.copy(c)\n",
    "    xmiss_flat = xmiss.flatten()\n",
    "    miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                    replace=False)\n",
    "    xmiss_flat[miss_pattern] = np.nan \n",
    "    xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "    mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "    Clients_missing.append(xmiss)\n",
    "    \n",
    "# And we finally save clients data to be provided to the nodes\n",
    "import os \n",
    "os.makedirs('data/clients_data', exist_ok=True) \n",
    "for i in range(len(Clients_missing)):\n",
    "    pd.DataFrame(Clients_missing[i]).to_csv('data/clients_data/client_'+str(i+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 (csv) to add client_1 dataset to the first node\n",
    "  * Provide the correct tag by entering:  `adni`\n",
    "  * Pick the folder where client_1 dataset has been saved\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online.\n",
    "4. Following the same procedure, you can create additional nodes for clients 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)\n",
    "xx = req.list()\n",
    "dataset_size = [xx[i][0]['shape'][1] for i in xx]\n",
    "assert min(dataset_size)==max(dataset_size)\n",
    "data_size = dataset_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover global mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run a model whose objective is to recover the federated mean and std, which we will use afterwards to standardize clients datasets with respect to the global dataset. We will need to run the following model exclusively for 1 round of 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class FedMeanStdTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(FedMeanStdTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"import pandas as pd\",\n",
    "               \"import numpy as np\",\n",
    "               \"from copy import deepcopy\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.n_features=model_args['n_features']\n",
    "        \n",
    "        # Here we define the parameter of our custom model: we want to recover the mean and std of each client,\n",
    "        # a specifically designed aggregatod will allow to evaluate the global mean and std\n",
    "        self.mean = nn.Parameter(torch.zeros(self.n_features),requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.zeros(self.n_features),requires_grad=False)\n",
    "        self.size = nn.Parameter(torch.zeros(self.n_features),requires_grad=False)\n",
    "        # Fake parameter with requires_grad=True to be passed to the backward (included in TorchTrainingPlan)\n",
    "        self.fake = nn.Parameter(torch.randn(1),requires_grad=True)\n",
    "        \n",
    "    def training_data(self):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        \n",
    "        ### NOTE: batch_size should be == dataset size ###\n",
    "        batch_size = df.shape[0]\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        ### NOTE: for standardization purposes, we keep nan when data is missing\n",
    "        #xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        data_np = data.numpy()\n",
    "        self.size += torch.Tensor([data_np[:,dim].size - np.count_nonzero(np.isnan(data_np[:,dim]))\\\n",
    "                                   for dim in range(self.n_features)])\n",
    "        self.mean += torch.from_numpy(np.nanmean(data_np,0))\n",
    "        self.std += torch.from_numpy(np.nanstd(data_np,0))\n",
    "        return self.fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {'n_features':data_size}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': 1, \n",
    "    'dry_run': False\n",
    "}\n",
    "\n",
    "tags =  ['adni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedstandard import FedStandard\n",
    "\n",
    "fed_mean_std = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=FedMeanStdTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=1,\n",
    "                 aggregator=FedStandard(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_mean_std.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the federated mean and std:\n",
    "fed_mean = fed_mean_std.aggregated_params()[0]['params']['fed_mean']\n",
    "fed_std = fed_mean_std.aggregated_params()[0]['params']['fed_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the MIWAE experiment model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MIWAETrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we include a function, ``standardize_data``, which allow to standardize data either with respect to a mean and std provided by the user, or locally, considering only local data for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "import pandas as pd\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MIWAETrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MIWAETrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"import torch.distributions as td\",\n",
    "               \"import pandas as pd\",\n",
    "               \"import numpy as np\"]\n",
    "        \n",
    "        self.n_features=model_args['n_features']\n",
    "        self.n_latent=model_args['n_latent']\n",
    "        self.n_hidden=model_args['n_hidden']\n",
    "        self.n_samples=model_args['n_samples']\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        if 'standardization' in model_args:\n",
    "            self.standardization = True\n",
    "            if (('fed_mean' in model_args['standardization']) and ('fed_std' in model_args['standardization'])):\n",
    "                self.fed_mean = np.array(model_args['standardization']['fed_mean'])\n",
    "                self.fed_std = np.array(model_args['standardization']['fed_std'])\n",
    "            else:\n",
    "                self.fed_mean = None\n",
    "                self.fed_std = None\n",
    "        \n",
    "        # the encoder will output both the mean and the diagonal covariance\n",
    "        self.encoder=nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_features, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 2*self.n_latent),  \n",
    "                        )\n",
    "        # the decoder will output both the mean, the scale, \n",
    "        # and the number of degrees of freedoms (hence the 3*p)\n",
    "        self.decoder = nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_latent, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 3*self.n_features),  \n",
    "                        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) \\\n",
    "                                    + list(self.decoder.parameters()),lr=1e-3)\n",
    "              \n",
    "        self.encoder.apply(self.weights_init)\n",
    "        self.decoder.apply(self.weights_init)\n",
    "        \n",
    "        # prior\n",
    "        self.p_z = td.Independent(td.Normal(loc=torch.zeros(self.n_latent).to(self.device)\\\n",
    "                                       ,scale=torch.ones(self.n_latent).to(self.device)),1)\n",
    "    \n",
    "    def weights_init(self,layer):\n",
    "        if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "    \n",
    "    def miwae_loss(self,iota_x,mask):\n",
    "        batch_size = iota_x.shape[0]\n",
    "        out_encoder = self.encoder(iota_x)\n",
    "        \n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :self.n_latent],\\\n",
    "                                                scale=torch.nn.Softplus()\\\n",
    "                                                (out_encoder[..., self.n_latent:\\\n",
    "                                                             (2*self.n_latent)])),1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([self.n_samples])\n",
    "        zgivenx_flat = zgivenx.reshape([self.n_samples*batch_size,self.n_latent])\n",
    "\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        all_means_obs_model = out_decoder[..., :self.n_features]\n",
    "        all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., self.n_features:\\\n",
    "                                                               (2*self.n_features)]) + 0.001\n",
    "        all_degfreedom_obs_model = torch.nn.Softplus()\\\n",
    "        (out_decoder[..., (2*self.n_features):(3*self.n_features)]) + 3\n",
    "\n",
    "        data_flat = torch.Tensor.repeat(iota_x,[self.n_samples,1]).reshape([-1,1])\n",
    "        tiledmask = torch.Tensor.repeat(mask,[self.n_samples,1])\n",
    "\n",
    "        all_log_pxgivenz_flat = torch.distributions.StudentT\\\n",
    "        (loc=all_means_obs_model.reshape([-1,1]),\\\n",
    "         scale=all_scales_obs_model.reshape([-1,1]),\\\n",
    "         df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.n_samples*batch_size,self.n_features])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([self.n_samples,batch_size])\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "        return neg_bound\n",
    "\n",
    "    def training_data(self,  batch_size = 48):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        # xhat_0: missing values are replaced by zeros. \n",
    "        #This x_hat0 is what will be fed to our encoder.\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        \n",
    "        # Data standardization\n",
    "        if self.standardization:\n",
    "            xhat_0 = self.standardize_data(xhat_0)\n",
    "            \n",
    "        xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def standardize_data(self,data):\n",
    "        data_norm = np.copy(data)\n",
    "        if ((self.fed_mean is not None) and (self.fed_std is not None)):\n",
    "            print('FEDERATED STANDARDIZATION')\n",
    "            data_norm = (data_norm - self.fed_mean)/self.fed_std\n",
    "        else:\n",
    "            print('LOCAL STANDARDIZATION')\n",
    "            data_norm = (data_norm - np.nanmean(data_norm,0))/np.nanstd(data_norm,0)\n",
    "        return data_norm\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        self.encoder.zero_grad()\n",
    "        self.decoder.zero_grad()\n",
    "        loss = self.miwae_loss(iota_x = data,mask = mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. \n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "* data `tags` to search nodes for training.\n",
    "* total number of `rounds`.\n",
    "If FedProx optimisation is requested, `fedprox_mu` parameter must be defined here. It also must be a float between XX and YY.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 128 # number of hidden units in (same for all MLPs)\n",
    "d = 5 # dimension of the latent space, we choose d=1 for visualisation purposes\n",
    "K = 100 # number of IS during training\n",
    "\n",
    "n_epochs=10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model_args = {'n_features':data_size,\n",
    "              'n_latent':d,\n",
    "              'n_hidden':h,\n",
    "              'n_samples':K,\n",
    "              'standardization':{'fed_mean':fed_mean.tolist(),'fed_std':fed_std.tolist()}}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': batch_size, \n",
    "    'lr': 1e-3, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': n_epochs, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['adni']\n",
    "rounds = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the federated training but using FedProx as aggregation scheme (starting from the second iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: during the first round\n",
    "# we will simply use FedAvg with standard optimization scheme: the FedProx penalization\n",
    "# term will be introduced exclusively from the second round.\n",
    "# training_args.update(fedprox_mu = 0.)\n",
    "\n",
    "exp_fedprox = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the second round, FedProx is used with mu=0.1\n",
    "# We first update the training args\n",
    "training_args.update(fedprox_mu = 0.1)\n",
    "# Then update training args in the experiment\n",
    "exp_fedprox.set_training_args(training_args)\n",
    "exp_fedprox.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx and performing the standardization locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we propose to use FedCos as well, which introduce an alternative penalization term with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del training_args['fedprox_mu'] \n",
    "\n",
    "model_args.update(standardization = {})\n",
    "\n",
    "exp_fedprox_std_local = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_fedprox_std_local.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.update(fedprox_mu = 0.1)\n",
    "# Then update training args in the experiment\n",
    "exp_fedprox_std_local.set_training_args(training_args)\n",
    "exp_fedprox_std_local.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and comparison to local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing on an external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we are going to test the performance of the final federated model to impute missing data on a test dataset. To this extent we are going to remove randomly 50% of samples from the test dataset, `data_test`, defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the test dataset, we will remove randomly 50% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "perc_miss = 0.5 # 50% of missing data\n",
    "\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "\n",
    "xmiss = np.copy(data_test)\n",
    "xmiss_flat = xmiss.flatten()\n",
    "miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                replace=False)\n",
    "xmiss_flat[miss_pattern] = np.nan \n",
    "xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "\n",
    "# We save the training dataset with missing values before normalization: we will need it later\n",
    "data_test_missing = np.copy(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfull = np.copy(data_test)\n",
    "xfull = (xfull - fed_mean.numpy())/fed_std.numpy()\n",
    "xmiss = np.copy(data_test_missing)\n",
    "xmiss = (xmiss - fed_mean.numpy())/fed_std.numpy() #xmiss is normalized wrt global mean and std\n",
    "mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "xhat_0 = np.copy(xmiss)\n",
    "xhat_0[np.isnan(xmiss)] = 0\n",
    "xhat = np.copy(xhat_0) # This will be out imputed data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_impute_single(encoder,decoder,iota_x,mask,d,L):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([L])\n",
    "    zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[L,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
    "\n",
    "    imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
    "    xms = xgivenz.mean.reshape([L,batch_size,p])  # that's the only line that changed!\n",
    "    xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
    "\n",
    "    return xm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the MSE function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
    "    xhat = np.array(xhat)\n",
    "    xtrue = np.array(xtrue)\n",
    "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model using last updated federated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model into PyTorch framework\n",
    "model = exp.model_instance()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for model obtained with FedProx (and federated standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fedprox = exp_fedprox.model_instance()\n",
    "model_fedprox.load_state_dict(exp_fedprox.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox = model_fedprox.encoder\n",
    "decoder_fedprox = model_fedprox.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fedprox_std_local = exp_fedprox_std_local.model_instance()\n",
    "model_fedprox_std_local.load_state_dict(exp_fedprox_std_local.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox_std_local = model_fedprox_std_local.encoder\n",
    "decoder_fedprox_std_local = model_fedprox_std_local.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally do the imputation and evaluate the corresponding imputation error through MSE for each federated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on testing data 0.62712\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on testing data  0.604235\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "L = 500\n",
    "\n",
    "xhat[~mask] = miwae_impute_single(encoder = encoder,decoder = decoder,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model on testing data %g' %err_test_data)\n",
    "print('-----')\n",
    "\n",
    "xhat[~mask] = miwae_impute_single(encoder = encoder_fedprox,decoder = decoder_fedprox,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedprox = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedprox) on testing data  %g' %err_test_data_fedprox)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing on the client's datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use the final federated model to impute missing data of client 1, which have been used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on data from client 1  0.603596\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on data from client 1  0.630319\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We first recover data (full and with missing entries) from client 1\n",
    "data_client_1 = np.copy(Clients_data[0])\n",
    "n = data_client_1.shape[0] # number of observations\n",
    "p = data_client_1.shape[1] # number of features\n",
    "\n",
    "xfull_cl1 = np.copy(data_client_1)\n",
    "xfull_cl1 = (xfull_cl1 - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "xmiss_cl1 = (xmiss_cl1 - fed_mean.numpy())/fed_std.numpy()\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "### Now we do the imputation\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute_single(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model on data from client 1  %g' %err_cl1_data)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute_single(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 1  %g' %err_cl1_data_fedprox)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on data from client 2  0.532805\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on data from client 2  0.53263\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We first recover data (full and with missing entries) from client 2\n",
    "data_client_2 = np.copy(Clients_data[1])\n",
    "n = data_client_2.shape[0] # number of observations\n",
    "p = data_client_2.shape[1] # number of features\n",
    "\n",
    "xfull_cl2 = np.copy(data_client_2)\n",
    "xfull_cl2 = (xfull_cl2 - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "xmiss_cl2 = np.copy(Clients_missing[1])\n",
    "xmiss_cl2 = (xmiss_cl2 - fed_mean.numpy())/fed_std.numpy()\n",
    "mask_cl2 = np.isfinite(xmiss_cl2) # binary mask that indicates which values are missing\n",
    "xhat_0_cl2 = np.copy(xmiss_cl2)\n",
    "xhat_0_cl2[np.isnan(xmiss_cl2)] = 0\n",
    "xhat_cl2 = np.copy(xhat_0_cl2) # This will be out imputed data matrix\n",
    "\n",
    "### Now we do the imputation\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute_single(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model on data from client 2  %g' %err_cl2_data)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute_single(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_fedprox = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 2  %g' %err_cl2_data_fedprox)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing of FedProx model with local standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the federated model with FedProx, where data standardization is performed locally. In order to be as much coherent as possible, each time the standardization will be realized locally as well in the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model (with fedprox and local standardization) on testing data  0.851073\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox and local standardization) on data from client 1  0.615122\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox and local standardization) on data from client 2  0.641046\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We recover the model\n",
    "model_fedprox_std_local = exp_fedprox_std_local.model_instance()\n",
    "model_fedprox_std_local.load_state_dict(exp_fedprox_std_local.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox_std_local = model_fedprox_std_local.encoder\n",
    "decoder_fedprox_std_local = model_fedprox_std_local.decoder\n",
    "\n",
    "# We re-create the testing dataset, and standardize with respect to his own data\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xmiss = np.copy(data_test_missing)\n",
    "mean_miss_test = np.nanmean(xmiss,0)\n",
    "std_miss_test = np.nanstd(xmiss,0)\n",
    "xmiss = (xmiss - mean_miss_test)/std_miss_test\n",
    "mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "xhat_0 = np.copy(xmiss)\n",
    "xhat_0[np.isnan(xmiss)] = 0\n",
    "xhat = np.copy(xhat_0) # This will be out imputed data matrix\n",
    "\n",
    "xfull = np.copy(data_test)\n",
    "xfull = (xfull - mean_miss_test)/std_miss_test\n",
    "\n",
    "# We do the imputation\n",
    "xhat[~mask] = miwae_impute_single(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedprox_std_local = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on testing data  %g' %err_test_data_fedprox_std_local)\n",
    "print('-----')\n",
    "\n",
    "# Same for the dataset from client 1\n",
    "data_client_1 = np.copy(Clients_data[0])\n",
    "n = data_client_1.shape[0] # number of observations\n",
    "p = data_client_1.shape[1] # number of features\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "mean_miss_cl1 = np.nanmean(xmiss_cl1,0)\n",
    "std_miss_cl1 = np.nanstd(xmiss_cl1,0)\n",
    "xmiss_cl1 = (xmiss_cl1 - mean_miss_cl1)/std_miss_cl1\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "xfull_cl1 = np.copy(data_client_1)\n",
    "xfull_cl1 = (xfull_cl1 - mean_miss_cl1)/std_miss_cl1\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute_single(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox_std_local = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on data from client 1  %g' %err_cl1_data_fedprox_std_local)\n",
    "print('-----')\n",
    "\n",
    "# And for the dataset from client 2\n",
    "data_client_2 = np.copy(Clients_data[1])\n",
    "n = data_client_2.shape[0] # number of observations\n",
    "p = data_client_2.shape[1] # number of features\n",
    "\n",
    "xmiss_cl2 = np.copy(Clients_missing[1])\n",
    "mean_miss_cl2 = np.nanmean(xmiss_cl2,0)\n",
    "std_miss_cl2 = np.nanstd(xmiss_cl2,0)\n",
    "xmiss_cl2 = (xmiss_cl2 - mean_miss_cl2)/std_miss_cl2\n",
    "mask_cl2 = np.isfinite(xmiss_cl2) # binary mask that indicates which values are missing\n",
    "xhat_0_cl2 = np.copy(xmiss_cl2)\n",
    "xhat_0_cl2[np.isnan(xmiss_cl2)] = 0\n",
    "xhat_cl2 = np.copy(xhat_0_cl2) # This will be out imputed data matrix\n",
    "xfull_cl2 = np.copy(data_client_2)\n",
    "xfull_cl2 = (xfull_cl2 - mean_miss_cl2)/std_miss_cl2\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute_single(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_fedprox_std_local = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on data from client 2  %g' %err_cl2_data_fedprox_std_local)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local training and testing on client 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the performance of the same model trained locally and tested on the dataset from client 1. We will use a total of `epochs`x`rounds` local epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "def miwae_loss(iota_x,mask, encoder, decoder, d, p, K, batch_size):\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([K])\n",
    "    zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "    return neg_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the local training on data from client 1, standardized with respect to his own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MIWAE likelihood bound  -132.373\n",
      "Loss: 122.189293\n",
      "Epoch 101\n",
      "MIWAE likelihood bound  -69.277\n",
      "Loss: 63.758804\n",
      "Epoch 201\n",
      "MIWAE likelihood bound  -44.2144\n",
      "Loss: 42.097584\n",
      "Epoch 301\n",
      "MIWAE likelihood bound  -29.7305\n",
      "Loss: 27.548759\n",
      "Epoch 401\n",
      "MIWAE likelihood bound  -14.2438\n",
      "Loss: 12.528351\n",
      "Epoch 501\n",
      "MIWAE likelihood bound  -5.80943\n",
      "Loss: -4.812748\n",
      "Epoch 601\n",
      "MIWAE likelihood bound  0.699142\n",
      "Loss: -6.148830\n",
      "Epoch 701\n",
      "MIWAE likelihood bound  12.8062\n",
      "Loss: -34.333340\n",
      "Epoch 801\n",
      "MIWAE likelihood bound  18.5732\n",
      "Loss: -20.412441\n",
      "Epoch 901\n",
      "MIWAE likelihood bound  25.4719\n",
      "Loss: -33.905273\n",
      "Epoch 1001\n",
      "MIWAE likelihood bound  30.8882\n",
      "Loss: -38.381958\n",
      "Epoch 1101\n",
      "MIWAE likelihood bound  31.5629\n",
      "Loss: -32.376366\n",
      "Epoch 1201\n",
      "MIWAE likelihood bound  40.3618\n",
      "Loss: -23.410786\n",
      "Epoch 1301\n",
      "MIWAE likelihood bound  42.746\n",
      "Loss: -37.975700\n",
      "Epoch 1401\n",
      "MIWAE likelihood bound  46.3974\n",
      "Loss: -47.984089\n"
     ]
    }
   ],
   "source": [
    "n_epochs_local = n_epochs*rounds\n",
    "\n",
    "bs = training_args.get('batch_size')\n",
    "lr = training_args.get('lr')\n",
    "\n",
    "n = xfull_cl1.shape[0] # number of observations\n",
    "p = xfull_cl1.shape[1] # number of features\n",
    "\n",
    "h = model_args.get('n_hidden') \n",
    "d = model_args.get('n_latent') \n",
    "K = model_args.get('n_samples') \n",
    "\n",
    "encoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl1 = torch.optim.Adam(list(encoder_cl1.parameters()) + list(decoder_cl1.parameters()),lr=lr)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl1.apply(weights_init)\n",
    "decoder_cl1.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl1[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl1[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl1.zero_grad()\n",
    "        encoder_cl1.zero_grad()\n",
    "        decoder_cl1.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(iota_x = b_data,mask = b_mask, encoder = encoder_cl1, decoder = decoder_cl1, d = d, p = p, K = K, batch_size = bs)\n",
    "        loss.backward()\n",
    "        optimizer_cl1.step()\n",
    "    if ep % 100 == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(), encoder = encoder_cl1, decoder = decoder_cl1, d = d, p = p, K = K, batch_size = bs).cpu().data.numpy())) # Gradient step      \n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do the imputation on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on data from same client (cl 1)  0.700608\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_cl1_loc = np.copy(xhat_cl1)\n",
    "xhat_cl1_loc[~mask_cl1] = miwae_impute_single(encoder = encoder_cl1, decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_cl1_data = np.array([mse(xhat_cl1_loc,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on data from same client (cl 1)  %g' %err_local_cl1_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the imputation on the external test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local (cl1) model on testing data 0.879791\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_loc1 = np.copy(xhat)\n",
    "xhat_loc1[~mask] = miwae_impute_single(encoder = encoder_cl1,decoder = decoder_cl1,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_local_cl1_test_data = np.array([mse(xhat_loc1,xfull,mask)])\n",
    "print('Imputation MSE of local (cl1) model on testing data %g' %err_local_cl1_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local training and testing on client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MIWAE likelihood bound  -132.631\n",
      "Loss: 122.724594\n",
      "Epoch 501\n",
      "MIWAE likelihood bound  18.5143\n",
      "Loss: -14.616351\n",
      "Epoch 1001\n",
      "MIWAE likelihood bound  70.4203\n",
      "Loss: -69.737221\n"
     ]
    }
   ],
   "source": [
    "n_epochs_local = n_epochs*rounds\n",
    "\n",
    "bs = training_args.get('batch_size')\n",
    "lr = training_args.get('lr')\n",
    "\n",
    "n = xfull_cl2.shape[0] # number of observations\n",
    "p = xfull_cl2.shape[1] # number of features\n",
    "\n",
    "h = model_args.get('n_hidden') \n",
    "d = model_args.get('n_latent') \n",
    "K = model_args.get('n_samples') \n",
    "\n",
    "encoder_cl2 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl2 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl2 = torch.optim.Adam(list(encoder_cl2.parameters()) + list(decoder_cl2.parameters()),lr=lr)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl2.apply(weights_init)\n",
    "decoder_cl2.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl2[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl2[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl2.zero_grad()\n",
    "        encoder_cl2.zero_grad()\n",
    "        decoder_cl2.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(iota_x = b_data,mask = b_mask, encoder = encoder_cl2, decoder = decoder_cl2, d = d, p = p, K = K, batch_size = bs)\n",
    "        loss.backward()\n",
    "        optimizer_cl2.step()\n",
    "    if ep % 500 == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(), encoder = encoder_cl2, decoder = decoder_cl2, d = d, p = p, K = K, batch_size = bs).cpu().data.numpy())) # Gradient step      \n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on data from same client (cl 2)  0.75784\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_cl2_loc = np.copy(xhat_cl2)\n",
    "xhat_cl2_loc[~mask_cl2] = miwae_impute_single(encoder = encoder_cl2, decoder = decoder_cl2, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_local_cl2_data = np.array([mse(xhat_cl2_loc,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of local model on data from same client (cl 2)  %g' %err_local_cl2_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local (cl2) model on testing data 0.940715\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_loc2 = np.copy(xhat)\n",
    "xhat_loc2[~mask] = miwae_impute_single(encoder = encoder_cl2,decoder = decoder_cl2,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_local_cl2_test_data = np.array([mse(xhat_loc2,xfull,mask)])\n",
    "print('Imputation MSE of local (cl2) model on testing data %g' %err_local_cl2_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE on testing data\n",
      "-----\n",
      "Model          Mean Squared Error (↓)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.62712\n",
      "FedProx                      0.604235\n",
      "FedLocStd                    0.851073\n",
      "Local (cl1)                  0.879791\n",
      "Local (cl2)                  0.940715\n",
      "-----\n",
      "-----\n",
      "Imputation MSE on local data from client 1\n",
      "-----\n",
      "Model          Mean Squared Error (↓)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.603596\n",
      "FedProx                      0.630319\n",
      "FedLocStd                    0.615122\n",
      "Local (cl1)                  0.700608\n",
      "-----\n",
      "-----\n",
      "Imputation MSE on local data from client 2\n",
      "-----\n",
      "Model          Mean Squared Error (↓)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.532805\n",
      "FedProx                      0.53263\n",
      "FedLocStd                    0.641046\n",
      "Local (cl2)                  0.75784\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('Imputation MSE on testing data')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_test_data],\n",
    "['FedProx', err_test_data_fedprox],\n",
    "['FedLocStd', err_test_data_fedprox_std_local],\n",
    "['Local (cl1)', err_local_cl1_test_data],\n",
    "['Local (cl2)', err_local_cl2_test_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 1')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_cl1_data],\n",
    "['FedProx', err_cl1_data_fedprox],\n",
    "['FedLocStd', err_cl1_data_fedprox_std_local],\n",
    "['Local (cl1)', err_local_cl1_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 2')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_cl2_data],\n",
    "['FedProx', err_cl2_data_fedprox],\n",
    "['FedLocStd', err_cl2_data_fedprox_std_local],\n",
    "['Local (cl2)', err_local_cl2_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
