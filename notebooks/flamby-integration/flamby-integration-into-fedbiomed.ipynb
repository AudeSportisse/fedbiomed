{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d89cd",
   "metadata": {},
   "source": [
    "### Fed-IXI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from unet import UNet\n",
    "\n",
    "class UNetTrainingPlan(TorchTrainingPlan):\n",
    "    # Init of UNetTrainingPlan\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(UNetTrainingPlan, self).__init__(model_args)\n",
    "        self.CHANNELS_DIMENSION = 1\n",
    "        \n",
    "        self.unet = UNet(\n",
    "            in_channels = model_args.get('in_channels',1),\n",
    "            out_classes = model_args.get('out_classes',2),\n",
    "            dimensions = model_args.get('dimensions',2),\n",
    "            num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n",
    "            out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n",
    "            normalization = model_args.get('normalization', None),\n",
    "            pooling_type = model_args.get('pooling_type', 'max'),\n",
    "            upsampling_type = model_args.get('upsampling_type','conv'),\n",
    "            preactivation = model_args.get('preactivation',False),\n",
    "            residual = model_args.get('residual',False),\n",
    "            padding = model_args.get('padding',0),\n",
    "            padding_mode = model_args.get('padding_mode','zeros'),\n",
    "            activation = model_args.get('activation','ReLU'),\n",
    "            initial_dilation = model_args.get('initial_dilation',None),\n",
    "            dropout = model_args.get('dropout',0),\n",
    "            monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"import torch.nn as nn\",\n",
    "               'import torch.nn.functional as F',\n",
    "               'from torch.optim import AdamW',\n",
    "               'from unet import UNet']\n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.optimizer = AdamW(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unet.forward(x)\n",
    "        x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dice_loss(output, target, epsilon=1e-9):\n",
    "        CHANNELS_DIMENSION = 1\n",
    "        SPATIAL_DIMENSIONS = 2, 3, 4\n",
    "        p0 = output\n",
    "        g0 = target\n",
    "        p1 = 1 - p0\n",
    "        g1 = 1 - g0\n",
    "        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        num = 2 * tp\n",
    "        denom = 2 * tp + fp + fn + epsilon\n",
    "        dice_score = num / denom\n",
    "        return 1. - dice_score\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        #this function must return the loss to backward it \n",
    "        img = data\n",
    "        output = self.forward(img)\n",
    "        loss = UNetTrainingPlan.get_dice_loss(output, target)\n",
    "        avg_loss = loss.mean()\n",
    "        return avg_loss\n",
    "    \n",
    "    def testing_step(self, data, target):\n",
    "        img = data\n",
    "        prediction = self.forward(img)\n",
    "        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n",
    "        avg_loss = loss.mean()  # average per batch\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dac8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'in_channels': 1,\n",
    "    'out_classes': 2,\n",
    "    'dimensions': 3,\n",
    "    'num_encoding_blocks': 3,\n",
    "    'out_channels_first_layer': 8,\n",
    "    'normalization': 'batch',\n",
    "    'upsampling_type': 'linear',\n",
    "    'padding': True,\n",
    "    'activation': 'PReLU',\n",
    "}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 16,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,\n",
    "    'log_interval': 2,\n",
    "    'test_ratio' : 0.0,\n",
    "    'test_on_global_updates': False,\n",
    "    'test_on_local_updates': False,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e2dc5",
   "metadata": {},
   "source": [
    "*train_transform_flamby* key in **training_args** can optionally be used to perform extra transformations on a flamby dataset.\n",
    "\n",
    "As a reminder, flamby datasets are already internally handling a transformation through their dataloader (this internal transform\n",
    "is the one officially used for the flamby benchmark). Thus, one should check what is already performed on the flamby side before\n",
    "adding transforms through the researcher.\n",
    "\n",
    "*train_transform_flamby* has to be defined as a list containing two elements:\n",
    "- the first is the imports needed to perform the transformation\n",
    "- the second is the Compose object that will be used to input the transform parameter of the flamby dataset federated class\n",
    "\n",
    "Example:\n",
    "```python\n",
    "training_args = {\n",
    "    ...,\n",
    "    'train_transform_flamby':[\"from monai.transforms import (Compose, NormalizeIntensity, Resize,)\",\n",
    "                         \"Compose([Resize((48,60,48)), NormalizeIntensity()])\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['ixi']\n",
    "num_rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=UNetTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=num_rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b05f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6e5a3",
   "metadata": {},
   "source": [
    "### Fed-Heart-Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd453726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from torch.optim import Adam\n",
    "\n",
    "class FedHeartTrainingPlan(TorchTrainingPlan):\n",
    "    # Init of FedHeartTrainingPlan\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(FedHeartTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"import torch.nn as nn\",\n",
    "               'from torch.optim import Adam',]\n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.model = nn.Linear(13, 1)\n",
    "        self.optimizer = Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.model(x))\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        #this function must return the loss to backward it \n",
    "        output = self.forward(data)\n",
    "        bce = nn.BCELoss()\n",
    "        loss = bce(output, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e81615",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'batch_size': 16,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 5,\n",
    "    'dry_run': False,\n",
    "    'log_interval': 2,\n",
    "    'test_ratio' : 0.0,\n",
    "    'test_on_global_updates': False,\n",
    "    'test_on_local_updates': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['dd']\n",
    "num_rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_class=FedHeartTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=num_rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd857d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733357d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
