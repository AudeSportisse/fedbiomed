{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f118f0ad",
   "metadata": {},
   "source": [
    "# Simulation of a Federated KMeans using scikit Learn and Federated Averaging\n",
    "\n",
    "The purpose of this notebook is to test a KMeans algorithm in a federated setting.\n",
    "\n",
    "Clustering data will be generated iusing `make_blob` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91316b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin, pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9d3ba",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    " 1. Problem 1\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd68e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "## parameters for data generation\n",
    "cluster_centers = [[1, 1], [-1, -1], [1, -1], [0, 1]]\n",
    "n_samples = 200\n",
    "cluster_std = .9\n",
    "space_dim = 2\n",
    "\n",
    "## algorithm parameters\n",
    "batch_size = 20\n",
    "n_init = 10  # nb of random initialization\n",
    "n_clusters = 4\n",
    "n_epochs = 10\n",
    "## nodes parameters (for federated learning only)\n",
    "\n",
    "nodes_id = [\"alice\", \"bob\", \"charlie\", \"danny\"]\n",
    "n_rounds = 200\n",
    "distance_metric = \"euclidean\"\n",
    "nb_samples_per_nodes = [1, 1, 1, 1] # indicates how to sample data per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "\n",
    "\n",
    "X, labels_true = make_blobs(n_samples=n_samples, centers=cluster_centers, cluster_std=cluster_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca32f15",
   "metadata": {},
   "source": [
    "## training on local dataset\n",
    "\n",
    "Th goal of this sub section is to compare performance of K means on local dataset and on the same dataset but split among clients. \n",
    "\n",
    "Here we will train a k mean on local dataset, using `partial_fit` (performing one update using one batch size).\n",
    "\n",
    "We iterate over partial fit to perfrom a complete epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0aac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train kmeans on local dataset\n",
    "mbk_local = MiniBatchKMeans(init='k-means++', n_clusters=n_clusters, batch_size=batch_size,\n",
    "                              n_init=10, max_no_improvement=10, verbose=0, random_state=SEED)\n",
    "\n",
    "\n",
    "history_local = {\"euclidian\": [], \"obj\": [], \"db\": []}\n",
    "for r in range(n_rounds):\n",
    "    for epoch in range(n_epochs):\n",
    "        mbk_local.partial_fit(X) \n",
    "\n",
    "        \n",
    "        history_local[\"obj\"].append(mbk_local.score(X))\n",
    "        order = pairwise_distances_argmin(cluster_centers,\n",
    "                                  mbk_local.cluster_centers_,\n",
    "                                     metric=distance_metric)\n",
    "        euclidian_dist = np.sum(np.square(mbk_local.cluster_centers_[order] - cluster_centers), axis=1)\n",
    "\n",
    "    history_local[\"euclidian\"].append(np.sqrt(euclidian_dist).tolist())\n",
    "    predicted = mbk_local.predict(X)\n",
    "    db_score = davies_bouldin_score(X, predicted)\n",
    "    history_local[\"db\"].append(db_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff084d6c",
   "metadata": {},
   "source": [
    "### displaying local mini batch results & losses plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d860cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,3, 1)\n",
    "plt.title(\"local mini batch k means:\\n obj \")\n",
    "plt.plot(history_local[\"obj\"])\n",
    "plt.xlabel(\"rounds\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"local mini batch k means:\\n mean of euclidian distance\\n between cluster center \")\n",
    "plt.plot(np.mean(history_local[\"euclidian\"], axis=1))\n",
    "plt.xlabel(\"rounds\")\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "\n",
    "plt.title(\"local mini batch k means:\\n davies bouldin score\")\n",
    "plt.plot(history_local[\"db\"])\n",
    "plt.xlabel(\" rounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d59ef3",
   "metadata": {},
   "source": [
    "### Displaying clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "mbk_local_cluster_centers = mbk_local.cluster_centers_\n",
    "mbk_local_labels = mbk_local.predict(X)\n",
    "colors = ['r', 'b', 'g', 'm']\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = mbk_local_labels == k\n",
    "    cluster_center = mbk_local_cluster_centers[k]\n",
    "    plt.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "            markerfacecolor=col, marker='.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=6)\n",
    "plt.title('Local MiniBatchKMeans')\n",
    "print(\"Mini batch k mean inertia: \", mbk_local.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca15c52",
   "metadata": {},
   "source": [
    "## KMeans in a federated environment\n",
    "\n",
    "### 1. split data into different nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_federated_clustering_dataset(dataset: np.ndarray,\n",
    "                                          labels: np.ndarray,\n",
    "                                          nodes_id: List[str],\n",
    "                                          nb_samples_per_nodes: list,\n",
    "                                         random_seed:int=None) -> Tuple[Dict[str, np.ndarray],\n",
    "                                                                        Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Split classification / clustering dataset into several nodes\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "    federated_dataset (Dict[str, np.ndarray]): a dictionary containing node id defined in argument\n",
    "    `nodes_id` mapping their respective training input datasets\n",
    "    federated_labels (Dict[str, np.ndarray]]): a dictionary containing node id defined in argument\n",
    "    `nodes_id` mapping their respective training label datasets (from which cluster point in \n",
    "    `federated_dataset` returned argument belongs to)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(nodes_id) == len(nb_samples_per_nodes), \"len(nodes_id) must equal len(nb_samples_per_nodes)\"\n",
    "    nb_samples_per_nodes = np.array(nb_samples_per_nodes)\n",
    "    w_sum_nodes = nb_samples_per_nodes / nb_samples_per_nodes.sum()\n",
    "    \n",
    "    nb_samples = labels.shape[0]\n",
    "    w_sum_nodes *= nb_samples\n",
    "    \n",
    "    print(w_sum_nodes)\n",
    "    \n",
    "        \n",
    "    all_X, all_labels = {}, {}\n",
    "    labels_tags = np.unique(labels)\n",
    "    n_labels = len(labels_tags)\n",
    "        \n",
    "    idx_pointer = np.zeros((n_labels))\n",
    "    \n",
    "    # this part is for splitting equally cluster for nodes\n",
    "    # making sure every clients have the same number of labels\n",
    "    for i,idx in enumerate(w_sum_nodes):\n",
    "        # iterates over node\"\n",
    "        for i_lab, lab in enumerate(labels_tags):\n",
    "            label_idx = lab == labels\n",
    "\n",
    "            idx1 = int(idx_pointer[i_lab])\n",
    "            idx2 = idx1 + int(np.floor(idx / n_labels))\n",
    "\n",
    "            #idx1_sample, idx2_sample = int(np.floor(idx / n_labels)), int(np.floor(idx / n_labels))\n",
    "            sampled_X, sampled_labels = (dataset[label_idx][idx1: idx2],\n",
    "                                            labels[label_idx][idx1: idx2])\n",
    "            if nodes_id[i] not in all_X.keys():\n",
    "                all_X[nodes_id[i]] = sampled_X\n",
    "                all_labels[nodes_id[i]] = sampled_labels\n",
    "            else:\n",
    "                all_X[nodes_id[i]] = np.concatenate([all_X[nodes_id[i]], sampled_X], axis=0)\n",
    "                all_labels[nodes_id[i]] = np.concatenate([all_labels[nodes_id[i]], sampled_labels], axis=0)\n",
    "            idx1 = idx2\n",
    "            idx_pointer[i_lab] = idx1\n",
    "        \n",
    "    \n",
    "    \n",
    "    # this part is for splitting remaining points into \n",
    "    # nodes datasets: as a consequence, datasets among clients will be uneven\n",
    "    remaining_data_pts = dataset.shape[0] - idx_pointer.sum()\n",
    "    print(\"remaining : \", remaining_data_pts, \" points\")\n",
    "    for i,idx in enumerate(w_sum_nodes):\n",
    "\n",
    "        if all_labels[nodes_id[i]].shape[0] < idx:\n",
    "            diff = np.abs(all_labels[nodes_id[i]].shape[0] - idx)\n",
    "            diff = np.ceil(diff)\n",
    "            #diff = nb_samples - idx_pointer.sum()\n",
    "            \n",
    "            while diff> 0 and remaining_data_pts >0:\n",
    "                for i_lab, lab in enumerate(labels_tags):\n",
    "\n",
    "                    label_idx = lab == labels\n",
    "                    idx1 = int(idx_pointer[i_lab])\n",
    "                    #maximum = int(np.ceil(labels[label_idx].shape[0] / len(nodes_id)))\n",
    "                    maximum = int(labels[label_idx].shape[0])\n",
    "\n",
    "                    if idx1  < maximum :\n",
    "\n",
    "\n",
    "                        all_X[nodes_id[i]] = np.concatenate([all_X[nodes_id[i]],\n",
    "                                                             dataset[label_idx][idx1].reshape(1,-1)],\n",
    "                                                            axis=0)\n",
    "\n",
    "                        all_labels[nodes_id[i]] = np.concatenate([all_labels[nodes_id[i]],\n",
    "                                                                  np.array([labels[label_idx][idx1]])], axis=0)\n",
    "                        diff -= 1\n",
    "                        idx1 += 1\n",
    "                        remaining_data_pts -= 1\n",
    "                        idx_pointer[i_lab] = idx1\n",
    "                        if diff == 0:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                print(\"added remaining point(s) to node \", nodes_id[i])\n",
    "                \n",
    "        print(idx_pointer)\n",
    "        \n",
    "        # randomly permute index of dataset of each node\n",
    "        if random_seed is not None:\n",
    "            rng = np.random.RandomState(random_seed)\n",
    "            permuted_idx = rng.permutation(all_labels[nodes_id[i]].shape[0])\n",
    "            all_X[nodes_id[i]]  = all_X[nodes_id[i]] [permuted_idx]\n",
    "            all_labels[nodes_id[i]] = all_labels[nodes_id[i]][permuted_idx]\n",
    "    return all_X, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3973ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset, federated_labels = generate_federated_clustering_dataset(X,\n",
    "                                                                            labels_true,\n",
    "                                                                            nodes_id,\n",
    "                                                                            nb_samples_per_nodes,\n",
    "                                                                            random_seed=SEED\n",
    "                                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc09df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bb9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0b966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603df21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "n_clients = len(nodes_id)\n",
    "for n_cli, cli in enumerate(nodes_id):\n",
    "    plt.subplot(1, n_clients, n_cli+1)\n",
    "    for k, col in zip(range(n_clusters), colors):\n",
    "        my_members = federated_labels[cli] == k\n",
    "        \n",
    "        plt.plot(federated_dataset[cli][my_members, 0], federated_dataset[cli][my_members, 1], 'w',\n",
    "                markerfacecolor=col, marker='.')\n",
    "        plt.title(f\"{cli} dataset\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f21260",
   "metadata": {},
   "source": [
    "### 2. Create different nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create several nodes (as many as in nodes_id)\n",
    "\n",
    "nodes = {}\n",
    "\n",
    "for n in nodes_id:\n",
    "    mbk = MiniBatchKMeans(init='k-means++', n_clusters=n_clusters, batch_size=batch_size,\n",
    "                              n_init=10, max_no_improvement=10, verbose=0, random_state=SEED)\n",
    "    nodes[n] = {'model' : mbk}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94c79a",
   "metadata": {},
   "source": [
    "training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"obj\": [], 'db':[], 'euclidian': []}\n",
    "\n",
    "# initialisation: compute dataset weigths\n",
    "w_sum = 0\n",
    "for n in nodes.keys():\n",
    "    nodes[n]['weigths'] = federated_dataset[n].shape[0]\n",
    "w_sum +=  nodes[n]['weigths']\n",
    "for n in nodes.keys():\n",
    "    nodes[n]['weigths'] /= w_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8eed37",
   "metadata": {},
   "source": [
    "there is still an issue in the code : in this piece of code cluster distance are assumed to be unique\n",
    "If not, training is unstable; and cluster centers will not be averaged correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# federated training\n",
    "for r in range(n_rounds):\n",
    "    for n in nodes_id:\n",
    "        for epoch in range(n_epochs):\n",
    "            nodes[n]['model'].partial_fit(federated_dataset[n]) \n",
    "    \n",
    "    # fedavg aggregating\n",
    "        \n",
    "    \n",
    "            print(n, nodes[n]['model'].cluster_centers_)\n",
    "    cluster_w = np.zeros((n_clusters, space_dim)).copy()\n",
    "    #cluster_w = np.zeros((n_clusters, dim))\n",
    "    order_memory = {}\n",
    "    for i, n in enumerate(nodes_id):\n",
    "        ref_order_model = nodes[nodes_id\n",
    "                                [0]]['model']\n",
    "        if len(nodes_id) > 1:\n",
    "            comp_model = nodes[n]['model']\n",
    "            order = pairwise_distances_argmin(ref_order_model.cluster_centers_,\n",
    "                                              comp_model.cluster_centers_,\n",
    "                                                 metric=distance_metric)\n",
    "            \n",
    "        else:\n",
    "            order = np.arange(n_clusters)  # special case where only one node is considered\n",
    "        order_memory[n] = order\n",
    "        print(order)\n",
    "        for c in range(n_clusters):\n",
    "        \n",
    "            if i == 0:\n",
    "                \n",
    "                cluster_w[c] += ref_order_model.cluster_centers_[c]* nodes[n]['weigths']\n",
    "            else:\n",
    "                cluster_w[c] += nodes[n]['model'].cluster_centers_[order][c] * nodes[n]['weigths']\n",
    "            \n",
    "    for c in range(n_clusters):        \n",
    "        cluster_w[c] = cluster_w[c]/len(nodes_id)\n",
    "\n",
    "    # update \n",
    "    for n in nodes_id:\n",
    "        nodes[n]['model'].cluster_centers_ = cluster_w.copy()[order]\n",
    "        print(n, nodes[n]['model'].cluster_centers_, '\\n', cluster_w)\n",
    "        # update history (predict on global dataset)\n",
    "    predicted = nodes[n]['model'].predict(X)\n",
    "    history[\"obj\"].append(nodes[n]['model'].score(X))\n",
    "    \n",
    "    if len(np.unique(predicted)) == 1:\n",
    "        db_score = 0\n",
    "    else:\n",
    "        db_score = davies_bouldin_score(X,predicted)\n",
    "    history[\"db\"].append(db_score)\n",
    "    order = pairwise_distances_argmin(cluster_centers,\n",
    "                                  nodes[n]['model'].cluster_centers_,\n",
    "                                     metric=distance_metric)\n",
    "    euclidian_dist = np.sum(np.square(nodes[n]['model'].cluster_centers_[order] - cluster_centers), axis=1)\n",
    "    history[\"euclidian\"].append(np.sqrt(euclidian_dist).tolist())\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cb7f3",
   "metadata": {},
   "source": [
    "### 3. result display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "markers = [\"v\", \"<\", \"^\", \">\"]\n",
    "for cluster, col, m in zip(range(n_clusters), colors, markers):\n",
    "    my_members = mbk_local_labels == cluster\n",
    "    cluster_center = mbk_local.cluster_centers_[cluster]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], \n",
    "             marker='.', c=col)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, n in enumerate(nodes.keys()):\n",
    "        plt.plot(nodes[n]['model'].cluster_centers_[cluster,0],\n",
    "                nodes[n]['model'].cluster_centers_[cluster,1],\n",
    "                 marker=m,  label=str(n), ms=10, mew=1, markerfacecolor=col)\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=8, mew=2,label=\"local Kmean\")\n",
    "    \n",
    "    plt.title(\"local mini batch k means vs federated k means\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ecd3b",
   "metadata": {},
   "source": [
    "I decided to plot all nodes after final round, to make sure each client has the same clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a40c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = pairwise_distances_argmin(mbk_local.cluster_centers_,\n",
    "                                  nodes[n]['model'].cluster_centers_,\n",
    "                                     metric=distance_metric)\n",
    "\n",
    "mbk_local.cluster_centers_, nodes[n]['model'].cluster_centers_[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537135be",
   "metadata": {},
   "source": [
    "### Display plots of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc80ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(history[\"obj\"])\n",
    "plt.xlabel(\"rounds\")\n",
    "plt.title(\"Federated K Means: obj function\")\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(np.mean(np.array(history[\"euclidian\"]), axis=1))\n",
    "plt.title(\"Federated K Means: \\nmean of distance between\\n K-means centers\\n and actual clusters center\")\n",
    "plt.xlabel(\"rounds\")\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(history[\"db\"])\n",
    "plt.title(\"Federated K Means: Davie Boulin\")\n",
    "plt.xlabel(\"epochs x rounds\")\n",
    "\n",
    "plt.subplot(2,3, 4)\n",
    "plt.title(\"local  k means: obj function\")\n",
    "plt.plot(history_local[\"obj\"])\n",
    "plt.xlabel(\"rounds\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=1.5, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"local  k means:\\n mean of euclidian distance\\n between cluster center \")\n",
    "plt.plot(np.mean(history_local[\"euclidian\"], axis=1))\n",
    "plt.xlabel(\"rounds\")\n",
    "\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "\n",
    "plt.title(\"local  k means:\\n davies bouldin score\")\n",
    "plt.plot(history_local[\"db\"])\n",
    "plt.xlabel(\"rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac03f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.plot(np.array(history[\"euclidian\"]), label=cluster_centers)\n",
    "plt.title(\"Distance to each cluster center\\n and the one predicted by federated k means\")\n",
    "plt.xlabel(\"rounds\")\n",
    "plt.ylabel(\"euclidian distance\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_local[\"euclidian\"], label=cluster_centers)\n",
    "plt.title(\"Distance to each cluster center\\n and the one predicted by local k means\")\n",
    "plt.xlabel(\"rounds\")\n",
    "plt.ylabel(\"euclidian distance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4160f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495db1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
