{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Differential Privacy with OPACUS on Fed-BioMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how `opacus` (https://opacus.ai/) can be used in Fed-BioMed. Opacus is a library which allows to train PyTorch models with differential privacy. We will train the basic MNIST example using two nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Fed-BioMed Environment\n",
    "\n",
    "### Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default)\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node run`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model and parameters\n",
    "\n",
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we are going to define the model using opacus for differential privacy. For this example, we are going to use the function `make_private` from `opacus.privacy_engine`. Two hyperparameters should be defined:\n",
    "* `noise_multiplier`: The ratio of the standard deviation of the Gaussian noise to the L2-sensitivity of the function to which the noise is added (How much noise to add)\n",
    "* `max_grad_norm`: The maximum norm of the per-sample gradients. Any gradient with norm higher than this will be clipped to this value.\n",
    "\n",
    "It is worth noting that in order to use the opacus `PrivacyEngine` class we need to properly define as training plan attributes a `model`, a `dataloader` and an `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"import torch.nn.functional as F\",\n",
    "                \"from opacus import PrivacyEngine\",]\n",
    "        \n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        model = nn.Sequential(nn.Conv2d(1, 32, 3, 1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(32, 64, 3, 1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(2),\n",
    "                                  nn.Dropout(0.25),\n",
    "                                  nn.Flatten(),\n",
    "                                  nn.Linear(9216, 128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.Linear(128, 10),\n",
    "                                  nn.LogSoftmax(dim=1))\n",
    "        return model\n",
    "    \n",
    "\n",
    "    \n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. For instance, the privacy parameters should be passed here.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48,\n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False, \n",
    "    'dp_args': \n",
    "        {\n",
    "            \"type\": \"local\", \n",
    "            \"sigma\": 0.4, \n",
    "            \"clip\": 0.005\n",
    "        },\n",
    "    'batch_maxnum': 50 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `rounds` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 17:11:35,223 fedbiomed INFO - Messaging researcher_ed5f291a-9d17-4734-bb0d-4842bfc12fc4 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f786820a3d0>\n",
      "2022-09-26 17:11:35,267 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2022-09-26 17:11:45,309 fedbiomed INFO - Node selected for training -> node_379d7d28-e23d-497a-86c9-3279544cdc35\n",
      "/user/scansiz/home/anaconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "2022-09-26 17:11:45,359 fedbiomed DEBUG - Model file has been saved: /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0015/my_model_da386f2a-80a2-4880-bc4b-10c4c6f983fc.py\n",
      "2022-09-26 17:11:45,393 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0015/my_model_da386f2a-80a2-4880-bc4b-10c4c6f983fc.py successful, with status code 201\n",
      "2022-09-26 17:11:45,641 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0015/aggregated_params_init_dd490c26-c8f3-4abe-9bf5-7c0cbbaecfef.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 3\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `rounds` are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 17:26:43,320 fedbiomed INFO - Sampled nodes in round 0 ['node_379d7d28-e23d-497a-86c9-3279544cdc35']\n",
      "2022-09-26 17:26:43,321 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_379d7d28-e23d-497a-86c9-3279544cdc35 \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: Perform training with the arguments: {'researcher_id': 'researcher_ed5f291a-9d17-4734-bb0d-4842bfc12fc4', 'job_id': '1081a665-0753-4e5e-b3e1-01b80aa84104', 'training_args': scheme:\n",
      "{'optimizer_args': {'rules': [<class 'dict'>], 'required': True, 'default': {}}, 'batch_size': {'rules': [<class 'int'>], 'required': True, 'default': 48}, 'epochs': {'rules': [<class 'int'>], 'required': True, 'default': 1}, 'dry_run': {'rules': [<class 'bool'>], 'required': True, 'default': False}, 'batch_maxnum': {'rules': [<class 'int'>], 'required': True, 'default': 100}, 'test_ratio': {'rules': [<class 'float'>, <function TrainingArgs._test_ratio_hook at 0x7f77ab551c10>], 'required': False, 'default': 0.0}, 'test_on_local_updates': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'test_on_global_updates': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'test_metric': {'rules': [<function TrainingArgs._metric_validation_hook at 0x7f77ab5519d0>], 'required': False, 'default': None}, 'test_metric_args': {'rules': [<class 'dict'>], 'required': False, 'default': {}}, 'log_interval': {'rules': [<class 'int'>], 'required': False, 'default': 10}, 'fedprox_mu': {'rules': [<function TrainingArgs._fedprox_mu_validator at 0x7f77ab551af0>], 'required': False, 'default': None}, 'use_gpu': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'dp_args': {'rules': [<function TrainingArgs._validate_dp_args at 0x7f77ab551e50>], 'required': True, 'default': None}}\n",
      "value:\n",
      "{'batch_size': 48, 'optimizer_args': {'lr': 0.001}, 'epochs': 1, 'dry_run': False, 'dp_args': {'type': 'local', 'sigma': 0.4, 'clip': 0.005}, 'batch_maxnum': 50, 'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'log_interval': 10, 'fedprox_mu': None, 'use_gpu': False}, 'training': True, 'model_args': {}, 'command': 'train', 'training_plan_url': 'http://localhost:8844/media/uploads/2022/09/26/my_model_da386f2a-80a2-4880-bc4b-10c4c6f983fc.py', 'params_url': 'http://localhost:8844/media/uploads/2022/09/26/aggregated_params_init_dd490c26-c8f3-4abe-9bf5-7c0cbbaecfef.pt', 'training_plan_class': 'MyTrainingPlan', 'training_data': {'node_379d7d28-e23d-497a-86c9-3279544cdc35': ['dataset_39e90f3c-b6c7-4352-8bfc-d1abcfe0bdc7']}} \n",
      " -----------------------------------------------------------------\n",
      "2022-09-26 17:26:43,323 fedbiomed DEBUG - researcher_ed5f291a-9d17-4734-bb0d-4842bfc12fc4\n",
      "2022-09-26 17:26:43,538 fedbiomed INFO - \u001b[1mWARNING\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_379d7d28-e23d-497a-86c9-3279544cdc35\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m There is no validation activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for validation will be ignored\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-09-26 17:26:48,577 fedbiomed INFO - \u001b[1mERROR\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_379d7d28-e23d-497a-86c9-3279544cdc35\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m Cannot train model in round: unsupported operand type(s) for *: 'int' and 'NoneType'\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-09-26 17:26:58,403 fedbiomed INFO - Error message received during training: FB312: Node stopped in SIGTERM signal handler\n",
      "2022-09-26 17:26:58,404 fedbiomed ERROR - FB408: node did not answer during training (node = node_379d7d28-e23d-497a-86c9-3279544cdc35)\n",
      "2022-09-26 17:26:58,405 fedbiomed CRITICAL - FB407: list of nodes became empty when training (no node has answered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Fed-BioMed researcher stopped due to exception:\n",
      "FB407: list of nodes became empty when training (no node has answered)\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "FedbiomedSilentTerminationError",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available in `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available in `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a little testing routine to extract the accuracy metrics on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def testing_Accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    device = 'cpu'\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100* correct/len(data_loader.dataset)\n",
    "\n",
    "    return(test_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from fedbiomed.researcher.environ import environ\n",
    "import os\n",
    "\n",
    "local_mnist = os.path.join(environ['TMP_DIR'], 'local_mnist')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "test_set = datasets.MNIST(root = local_mnist, download = True, train = False, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed_model = exp.training_plan().model()\n",
    "fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "acc_federated = testing_Accuracy(fed_model, test_loader)\n",
    "\n",
    "print('\\nAccuracy federated training:  {:.4f}'.format(acc_federated[1]))\n",
    "\n",
    "print('\\nError federated training:  {:.4f}'.format(acc_federated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
