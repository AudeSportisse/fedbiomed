{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and Central DP with Fed-BioMed: MONAI 2d image registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/), trained with Differential Privacy (DP). We are going to compare results of:\n",
    "* non private training\n",
    "* train with Local Differential Privacy (LDP)\n",
    "* train with Central Differential Privacy (CDP)\n",
    "\n",
    "In order to enforce differential privacy during training (both local and central) we will rely on the Opcaus library (https://opacus.ai/). \n",
    "\n",
    "## Image Registration\n",
    "\n",
    "Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.\n",
    "\n",
    "In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ).\n",
    "Goal of the notebook is to train a model given moving images and fixed images (recalibrated images). \n",
    "\n",
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`\n",
    "\n",
    "## Creating MedNIST nodes\n",
    "\n",
    "MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.\n",
    "\n",
    "To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:\n",
    "\n",
    "https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view\n",
    "\n",
    "The dataset owned by each client has structure:\n",
    "\n",
    "\n",
    "└── client_*/\n",
    "\n",
    "    ├── AbdomenCT/\n",
    "    \n",
    "    └── BreastMRI/\n",
    "    \n",
    "    └── CXR/\n",
    "    \n",
    "    └── ChestCT/\n",
    "    \n",
    "    └── Hand/\n",
    "    \n",
    "    └── HeadCT/   \n",
    "\n",
    "To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. \n",
    "After activating the fedbiomed network with the commands\n",
    "\n",
    "`source ./scripts/fedbiomed_environment network`\n",
    "\n",
    "and \n",
    "\n",
    "`./scripts/fedbiomed_run network`\n",
    "\n",
    "we create a first node by using the commands\n",
    "\n",
    "`source ./scripts/fedbiomed_environment node`\n",
    "\n",
    "`./scripts/fedbiomed_run node start`\n",
    "\n",
    "We then poulate the node with the data of first client:\n",
    "\n",
    "`./scripts/fedbiomed_run node add`\n",
    "\n",
    "We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. \n",
    "Assign tag `mednist` to the data when asked.\n",
    "\n",
    "We can further check that the data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "\n",
    "Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Fed-BioMed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the reseracher enviroment with the command `source ./scripts/fedbiomed_environment researcher`, and open the Jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag `mednist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed.\n",
    "We first import the necessary modules from `fedbiomed` and `monai` libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the training plan. Note that we use the standard `TorchTrainingPlan` natively provided in Fed-BioMed. We reuse the `MedNISTDataset` data loader defined in the original MONAI tutorial, which is returned by the method `training_data`, which also implements the data parsing from the nodes `dataset_path`. We should also properly define the `training_routine`, following the MONAI tutorial. According to the MONAI tutorial, the model is the `GlobalNet` and the loss is `MSELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Union, List\n",
    "\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    "    EnsureTypeD,\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config, USE_COMPILED\n",
    "from monai.networks.nets import GlobalNet\n",
    "from monai.networks.blocks import Warp\n",
    "from monai.apps import MedNISTDataset\n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "        \n",
    "    # Dependencies for training plan\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"import numpy as np\",\n",
    "            \"import monai\",\n",
    "            \"from torch.nn import MSELoss\",\n",
    "            \"from monai.utils import set_determinism, first\",\n",
    "            \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n",
    "            \"from monai.data import DataLoader, Dataset, CacheDataset\",\n",
    "            \"from monai.networks.nets import GlobalNet\",\n",
    "            \"from monai.config import USE_COMPILED\",\n",
    "            \"from monai.networks.blocks import Warp\",\n",
    "            \"from monai.apps import MedNISTDataset\",\n",
    "            \"from opacus.validators import ModuleValidator\"]\n",
    "        return deps \n",
    "    \n",
    "    # Model for training\n",
    "    def init_model(self):\n",
    "        \n",
    "        # Define model related attributes \n",
    "        self.image_loss = MSELoss()\n",
    "        if USE_COMPILED:\n",
    "            self.warp_layer = Warp(3, \"border\")\n",
    "        else:\n",
    "            self.warp_layer = Warp(\"bilinear\", \"border\")\n",
    "        \n",
    "        # Define model \n",
    "        model = GlobalNet(image_size=(64, 64),\n",
    "                          spatial_dims=2,\n",
    "                          in_channels=2,  # moving and fixed\n",
    "                          num_channel_initial=16,\n",
    "                          depth=3)\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    # Optimizer for training\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        optimizer = torch.optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def training_data(self, batch_size = 20):\n",
    "        # Custom torch Dataloader for MedNIST data\n",
    "        data_path = self.dataset_path\n",
    "        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n",
    "        MedNISTDataset.dataset_folder_name = \"\"\n",
    "        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n",
    "        training_datadict = [\n",
    "            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n",
    "            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n",
    "        ]\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n",
    "                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n",
    "                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n",
    "                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n",
    "                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n",
    "                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "            ]\n",
    "        )\n",
    "        train_ds = CacheDataset(data=training_datadict, transform=train_transforms,\n",
    "                                cache_rate=1.0, num_workers=0)\n",
    "        dl = self.MednistDataLoader(train_ds)\n",
    "        \n",
    "        return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    def training_step(self, moving, fixed):\n",
    "        ddf = self.model().forward(torch.cat((moving, fixed), dim=1))\n",
    "        pred_image = self.warp_layer(moving, ddf)\n",
    "        loss = self.image_loss(pred_image, fixed)\n",
    "        return loss\n",
    "    \n",
    "    class MednistDataLoader(monai.data.Dataset):\n",
    "        # Custom DataLoader that inherits from monai's Dataset object\n",
    "        def __init__(self, dataset):\n",
    "            self.dataset = dataset\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.dataset[idx][\"moving_hand\"],\n",
    "                    self.dataset[idx][\"fixed_hand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we import the required modules for running any experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 08:42:40,183 fedbiomed INFO - Component environment:\n",
      "2022-09-21 08:42:40,185 fedbiomed INFO - type = ComponentType.RESEARCHER\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-private training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train our model in a non-private way. We set the model and training parameters. In particular, we are going to perform 2 epochs over 3 rounds for this experiment. Moreover the training is performed on ~26% of the locally available training data. We are also trying to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 16, \n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-5\n",
    "    },\n",
    "    'use_gpu': True,\n",
    "    'epochs': 4, \n",
    "    'dry_run': False\n",
    "}\n",
    "\n",
    "tags =  ['#MEDNIST', '#dataset']\n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment can be now defined, by providing the `mednist` tag, and running the local training on nodes with model defined in `model_path`, standard `aggregator` (FedAvg) and `client_selection_strategy` (all nodes used). Federated learning is going to be perfomed through 3 optimization rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 08:42:44,526 fedbiomed INFO - Messaging researcher_2e0d74d9-4e02-4710-ad66-38fae6b2f290 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f82c852c190>\n",
      "2022-09-21 08:42:44,538 fedbiomed INFO - Searching dataset with data tags: ['#MEDNIST', '#dataset'] for all nodes\n",
      "2022-09-21 08:42:54,575 fedbiomed INFO - Node selected for training -> node_97621f4d-cef4-4a50-83e9-873c100efeb2\n",
      "monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n",
      "2022-09-21 08:42:54,633 fedbiomed DEBUG - Model file has been saved: /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0105/my_model_51280191-14f2-43e7-870e-ed01042e3dbf.py\n",
      "2022-09-21 08:42:54,812 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0105/my_model_51280191-14f2-43e7-870e-ed01042e3dbf.py successful, with status code 201\n",
      "2022-09-21 08:42:55,001 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0105/aggregated_params_init_00963359-26e9-4518-8453-42d89ff8a16c.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 08:43:06,202 fedbiomed INFO - Sampled nodes in round 0 ['node_97621f4d-cef4-4a50-83e9-873c100efeb2']\n",
      "2022-09-21 08:43:06,205 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_97621f4d-cef4-4a50-83e9-873c100efeb2 \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: Perform training with the arguments: {'researcher_id': 'researcher_2e0d74d9-4e02-4710-ad66-38fae6b2f290', 'job_id': 'e5d4ad75-1a00-4f6b-91fa-2d4609858d81', 'training_args': scheme:\n",
      "{'optimizer_args': {'rules': [<class 'dict'>], 'required': True, 'default': {}}, 'batch_size': {'rules': [<class 'int'>], 'required': True, 'default': 48}, 'epochs': {'rules': [<class 'int'>], 'required': True, 'default': 1}, 'dry_run': {'rules': [<class 'bool'>], 'required': True, 'default': False}, 'batch_maxnum': {'rules': [<class 'int'>], 'required': True, 'default': 100}, 'test_ratio': {'rules': [<class 'float'>, <function TrainingArgs._test_ratio_hook at 0x7f820d0e45e0>], 'required': False, 'default': 0.0}, 'test_on_local_updates': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'test_on_global_updates': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'test_metric': {'rules': [<function TrainingArgs._metric_validation_hook at 0x7f820d0e43a0>], 'required': False, 'default': None}, 'test_metric_args': {'rules': [<class 'dict'>], 'required': False, 'default': {}}, 'log_interval': {'rules': [<class 'int'>], 'required': False, 'default': 10}, 'fedprox_mu': {'rules': [<function TrainingArgs._fedprox_mu_validator at 0x7f820d0e44c0>], 'required': False, 'default': None}, 'use_gpu': {'rules': [<class 'bool'>], 'required': False, 'default': False}, 'dp_args': {'rules': [<function TrainingArgs._validate_dp_args at 0x7f820d0e4820>], 'required': False, 'default': None}}\n",
      "value:\n",
      "{'batch_size': 16, 'optimizer_args': {'lr': 1e-05}, 'use_gpu': True, 'epochs': 4, 'dry_run': False, 'batch_maxnum': 100, 'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'log_interval': 10, 'fedprox_mu': None, 'dp_args': None}, 'training': True, 'model_args': {}, 'command': 'train', 'training_plan_url': 'http://localhost:8844/media/uploads/2022/09/21/my_model_51280191-14f2-43e7-870e-ed01042e3dbf.py', 'params_url': 'http://localhost:8844/media/uploads/2022/09/21/aggregated_params_init_00963359-26e9-4518-8453-42d89ff8a16c.pt', 'training_plan_class': 'MyTrainingPlan', 'training_data': {'node_97621f4d-cef4-4a50-83e9-873c100efeb2': ['dataset_fc388c19-eab3-4f66-914d-4937ffa50054']}} \n",
      " -----------------------------------------------------------------\n",
      "2022-09-21 08:43:06,207 fedbiomed DEBUG - researcher_2e0d74d9-4e02-4710-ad66-38fae6b2f290\n",
      "2022-09-21 08:43:06,569 fedbiomed INFO - \u001b[1mWARNING\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_97621f4d-cef4-4a50-83e9-873c100efeb2\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m There is no validation activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for validation will be ignored\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-09-21 08:43:15,113 fedbiomed INFO - \u001b[1mERROR\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_97621f4d-cef4-4a50-83e9-873c100efeb2\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m Cannot train model in round: 'NoneType' object has no attribute 'before_training'\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-09-21 08:43:21,260 fedbiomed INFO - Downloading model params after training on node_97621f4d-cef4-4a50-83e9-873c100efeb2 - from \n",
      "2022-09-21 08:43:21,262 fedbiomed ERROR - FB604: repository error : bad URL when downloading file node_params_750729f6-068c-4801-8474-1ece30a114b1.pt(details :Invalid URL '': No scheme supplied. Perhaps you meant http://? )\n",
      "2022-09-21 08:43:21,262 fedbiomed ERROR - Cannot download model parameter from node node_97621f4d-cef4-4a50-83e9-873c100efeb2, probably because Node stops working (details: FB604: repository error : bad URL when downloading file node_params_750729f6-068c-4801-8474-1ece30a114b1.pt(details :Invalid URL '': No scheme supplied. Perhaps you meant http://? ))\n",
      "2022-09-21 08:43:21,263 fedbiomed ERROR - FB408: node did not answer during training (node = node_97621f4d-cef4-4a50-83e9-873c100efeb2)\n",
      "2022-09-21 08:43:21,264 fedbiomed CRITICAL - FB407: list of nodes became empty when training (no node has answered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Fed-BioMed researcher stopped due to exception:\n",
      "FB407: list of nodes became empty when training (no node has answered)\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "FedbiomedSilentTerminationError",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform DP training (both local and central) we need to provide to the model and training schemes:\n",
    "* `clip`: defining the maximal L2 norm of gradients\n",
    "* `sigma`: defining the strenght of Gaussian noise to be added (either to gradients in case of LDP or to the final local model in case of CDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDP\n",
    "\n",
    "### Dimensioning the training parameters with LDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "\n",
    "req = Requests()\n",
    "query_nodes = req.list()\n",
    "# min_dataset_size = 1000 #min([xx[i][0]['shape'][0] for i in xx]) #see training data in model\n",
    "# tot_dataset_size = 1000*len([xx[i][0]['shape'][0] for i in xx]) #sum([xx[i][0]['shape'][0] for i in xx]) #see training data in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dataset_size = min([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model\n",
    "tot_dataset_size = sum([dataset['shape'][0] for i in query_nodes for dataset in query_nodes[i] if dataset['tags'] == ['#MEDNIST', '#dataset']]) #see training data in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = training_args['batch_size']/min_dataset_size\n",
    "sigma = 0.4\n",
    "clip = 0.005\n",
    "delta = .1/min_dataset_size\n",
    "max_epsilon = 10.\n",
    "max_N = int(1e2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.privacy.rdp_accountant import get_iterations, compute_rdp\n",
    "\n",
    "N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = N/(training_args['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_args['epochs']*rounds<=max_rounds, 'Number of rounds not compatible with privacy budget'\n",
    "\n",
    "print(f'The maximal number of FL rounds for ({max_epsilon},{delta})-LDP training is {max_rounds}')\n",
    "print('The selected number of FL rounds, '+str(rounds)+\n",
    "      ',implies ('+str(eps_list[training_args['epochs']*rounds-1])+','+str(delta)+',)-LDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to repeat the same training but with private SGD: at each epoch gradients are clipped and perturbed according to the provided privacy parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update training parameters for LDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform DP-training we should provide an additional argument to training: the dictionalry `'DP_args'` containing necessary parameters for DP. If we want to perform LDP, we should specify: `'type' : 'local'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "LDP = {'dp_args': {'type' : 'local', 'sigma': sigma, 'clip': clip}}\n",
    "model_args.update(LDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the LDP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_LDP = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_LDP.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDP\n",
    "\n",
    "### Dimensioning the training parameters with CDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_clients = len([query_nodes[i][0]['shape'][0] for i in query_nodes])\n",
    "\n",
    "# Here we use the same parameters as LDP to evaluate the number of rounds, \n",
    "# since we are performing record-level DP\n",
    "\n",
    "q = training_args['batch_size']/min_dataset_size \n",
    "sigma = 0.4#/(np.sqrt(num_clients)*training_args['batch_size'])\n",
    "clip = 0.005\n",
    "delta = .1/min_dataset_size\n",
    "max_epsilon = 10.\n",
    "max_N = int(1e2)\n",
    "\n",
    "N, eps_list = get_iterations(delta, sigma, q, max_epsilon, max_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = N/(training_args['epochs'])\n",
    "print(max_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rounds<=max_rounds, 'Number of rounds not compatible with privacy budget'\n",
    "\n",
    "print(f'The maximal number of allowed rounds for ({max_epsilon},{delta})-CDP training is {max_rounds}')\n",
    "print(f'The selected number of training rounds, '+str(rounds)+\n",
    "      ',implies ('+str(eps_list[rounds-1])+','+str(delta)+',)-CDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update training parameters for CDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform CDP, we should update the `'DP_args'` dictionary by setting:  `'type' : 'central'`. Otherwise we are going to keep the same privacy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDP = {'dp_args': {'type' : 'central', 'sigma': sigma/np.sqrt(num_clients), 'clip': clip}}\n",
    "model_args.update(CDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the CDP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_CDP = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_CDP.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to test and compare locally the three final federated models on an independent testing partition.\n",
    "The test dataset is available at this link:\n",
    "\n",
    "https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib -q\n",
    "!pip install gdown -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print_config()\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the testing dataset on the local temporary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from fedbiomed.researcher.environ import environ\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\n",
    "\n",
    "resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\n",
    "base_dir = tmp_dir.name\n",
    "test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n",
    "\n",
    "gdown.download(resource, test_file, quiet=False)\n",
    "\n",
    "zf = zipfile.ZipFile(test_file)\n",
    "\n",
    "for file in zf.infolist():\n",
    "    zf.extract(file, base_dir)\n",
    "    \n",
    "data_dir = os.path.join(base_dir, \"MedNIST_testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine our custom dataloader (defined previously in  the `TrainingPlan`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "import monai\n",
    "\n",
    "class MednistDataLoader(monai.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.dataset[idx][\"moving_hand\"],\n",
    "                self.dataset[idx][\"fixed_hand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the testing data loader and pairs of moving vs fixed hands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if you have one + enough memory available\n",
    "#\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "# recreate model\n",
    "model = GlobalNet(\n",
    "    image_size=(64, 64),\n",
    "    spatial_dims=2,\n",
    "    in_channels=2,  # moving and fixed\n",
    "    num_channel_initial=16,\n",
    "    depth=3).to(device)\n",
    "\n",
    "if USE_COMPILED:\n",
    "    warp_layer = Warp(3, \"border\").to(device)\n",
    "else:\n",
    "    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n",
    "\n",
    "MedNISTDataset.dataset_folder_name = \"\"\n",
    "test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\n",
    "testing_datadict = [\n",
    "    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n",
    "    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n",
    "]\n",
    "test_transforms = Compose(\n",
    "            [\n",
    "                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n",
    "                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n",
    "                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n",
    "                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n",
    "                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "            ]\n",
    "        )\n",
    "val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n",
    "                      cache_rate=1.0, num_workers=0)\n",
    "val_dl = MednistDataLoader(val_ds)\n",
    "val_loader = DataLoader(val_dl, batch_size=16, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the federated models we need to create model instances and assign to it the models parameters estimated at the last federated optimization rounds. Then, we generate predictions of the transformation between pairs. In addition, we evaluate the structural similarity index for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics -q\n",
    "\n",
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "\n",
    "# Non private training\n",
    "model = exp.training_plan().model()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "# training with LDP\n",
    "model_LDP = exp_LDP.training_plan().model()\n",
    "model_LDP.load_state_dict(exp_LDP.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "# training with CDP\n",
    "model_CDP = exp_CDP.training_plan().model()\n",
    "model_CDP.load_state_dict(exp_CDP.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "for moving, fixed in val_loader:\n",
    "    # Non private training\n",
    "    ddf = model(torch.cat((moving, fixed), dim=1))\n",
    "    pred_image = warp_layer(moving, ddf)\n",
    "    \n",
    "    # training with LDP\n",
    "    ddf_LDP = model_LDP(torch.cat((moving, fixed), dim=1))\n",
    "    pred_image_LDP = warp_layer(moving, ddf_LDP)\n",
    "    \n",
    "    # training with CDP\n",
    "    ddf_CDP = model_CDP(torch.cat((moving, fixed), dim=1))\n",
    "    pred_image_CDP = warp_layer(moving, ddf_CDP)\n",
    "    \n",
    "    # ssim predicted vs ground truth\n",
    "    # Non private training\n",
    "    SSIM = structural_similarity_index_measure(pred_image, fixed)\n",
    "    # training with LDP\n",
    "    SSIM_LDP = structural_similarity_index_measure(pred_image_LDP, fixed)\n",
    "    # training with CDP\n",
    "    SSIM_CDP = structural_similarity_index_measure(pred_image_CDP, fixed)\n",
    "    \n",
    "    break\n",
    "\n",
    "fixed_image = fixed.detach().cpu().numpy()[:, 0]\n",
    "moving_image = moving.detach().cpu().numpy()[:, 0]\n",
    "pred_image = pred_image.detach().cpu().numpy()[:, 0]\n",
    "pred_image_LDP = pred_image_LDP.detach().cpu().numpy()[:, 0]\n",
    "pred_image_CDP = pred_image_CDP.detach().cpu().numpy()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> Results for non-private training')\n",
    "print(f'SSIM = {SSIM}')\n",
    "\n",
    "print('---> Results for training with LDP')\n",
    "print(f'SSIM = {SSIM_LDP})')\n",
    "\n",
    "print('---> Results for training with CDP')\n",
    "print(f'SSIM = {SSIM_CDP})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print some example of predictions of all models from the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "batch_size = 10\n",
    "plt.subplots(batch_size, 5, figsize=(12, 25))\n",
    "for b in range(batch_size):\n",
    "    # moving image\n",
    "    plt.subplot(batch_size, 5, b * 5 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"moving image\")\n",
    "    plt.imshow(moving_image[b], cmap=\"gray\")\n",
    "    # fixed image\n",
    "    plt.subplot(batch_size, 5, b * 5 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"fixed image\")\n",
    "    plt.imshow(fixed_image[b], cmap=\"gray\")\n",
    "    # warped moving\n",
    "    plt.subplot(batch_size, 5, b * 5 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"predicted image\")\n",
    "    plt.imshow(pred_image[b], cmap=\"gray\")\n",
    "    # warped moving LDP\n",
    "    plt.subplot(batch_size, 5, b * 5 + 4)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"predicted image (LDP)\")\n",
    "    plt.imshow(pred_image_LDP[b], cmap=\"gray\")\n",
    "    # warped moving CDP\n",
    "    plt.subplot(batch_size, 5, b * 5 + 5)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"predicted image (CDP)\")\n",
    "    plt.imshow(pred_image_CDP[b], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
