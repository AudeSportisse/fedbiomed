{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 15:12:50,071 fedbiomed INFO - Component environment:\n",
      "2021-12-17 15:12:50,072 fedbiomed INFO - - type = ComponentType.RESEARCHER\n",
      "2021-12-17 15:12:51,016 fedbiomed INFO - Messaging researcher_860a6809-2bc7-47ed-8f94-423486e71805 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x106bf64f0>\n",
      "2021-12-17 15:12:51,125 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2021-12-17 15:12:51,142 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'command': 'list'}\n",
      "2021-12-17 15:12:51,144 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'command': 'list'}\n",
      "2021-12-17 15:13:01,147 fedbiomed INFO - \n",
      " Node: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f | Number of Datasets: 1 \n",
      "+---------+-------------+-------------+---------------+--------------------+\n",
      "| name    | data_type   | tags        | description   | shape              |\n",
      "+=========+=============+=============+===============+====================+\n",
      "| mednist | images      | ['mednist'] | mednist       | [14738, 3, 64, 64] |\n",
      "+---------+-------------+-------------+---------------+--------------------+\n",
      "\n",
      "2021-12-17 15:13:01,148 fedbiomed INFO - \n",
      " Node: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 | Number of Datasets: 1 \n",
      "+---------+-------------+-------------+---------------+--------------------+\n",
      "| name    | data_type   | tags        | description   | shape              |\n",
      "+=========+=============+=============+===============+====================+\n",
      "| mednist | images      | ['mednist'] | mednist       | [14736, 3, 64, 64] |\n",
      "+---------+-------------+-------------+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f': [{'name': 'mednist',\n",
       "   'data_type': 'images',\n",
       "   'tags': ['mednist'],\n",
       "   'description': 'mednist',\n",
       "   'shape': [14738, 3, 64, 64]}],\n",
       " 'node_a6c386ed-2df1-40d9-9bdd-89c7606c8581': [{'name': 'mednist',\n",
       "   'data_type': 'images',\n",
       "   'tags': ['mednist'],\n",
       "   'description': 'mednist',\n",
       "   'shape': [14736, 3, 64, 64]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the client up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 3 (images) to add MedNIST to the client\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MedNIST is contained\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node run`. Wait until you get `Connected with result code 0`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file = tmp_dir_model.name + '/class_export_mednist.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    "    EnsureTypeD,\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config, USE_COMPILED\n",
    "from monai.networks.nets import GlobalNet\n",
    "from monai.networks.blocks import Warp\n",
    "from monai.apps import MedNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : write **only** the code to export in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/tmp/tmpw4t84yo8/class_export_mednist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Union, List\n",
    "\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    "    EnsureTypeD,\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config, USE_COMPILED\n",
    "from monai.networks.nets import GlobalNet\n",
    "from monai.networks.blocks import Warp\n",
    "from monai.apps import MedNISTDataset\n",
    "\n",
    "# Here we define the model to be used. \n",
    "class MyMonaiTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, kwargs):\n",
    "        super(MyMonaiTrainingPlan, self).__init__()\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"import numpy as np\",\n",
    "                \"import os\",\n",
    "                \"from torch.nn import MSELoss\",\n",
    "                \"from typing import Union, List\",\n",
    "                \"from monai.utils import set_determinism, first\",\n",
    "                \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n",
    "                \"from monai.data import DataLoader, Dataset, CacheDataset\",\n",
    "                \"from monai.config import print_config, USE_COMPILED\",\n",
    "                \"from monai.networks.nets import GlobalNet\",\n",
    "                \"from monai.networks.blocks import Warp\",\n",
    "                \"from monai.apps import MedNISTDataset\",]\n",
    "        self.add_dependency(deps)\n",
    "        #self.num_class =  kwargs['num_class']\n",
    "        #self.device = torch.device(\"cuda:0\")\n",
    "        self.model = GlobalNet(\n",
    "            image_size=(64, 64),\n",
    "            spatial_dims=2,\n",
    "            in_channels=2,  # moving and fixed\n",
    "            num_channel_initial=16,\n",
    "            depth=3).to(self.device)\n",
    "        self.image_loss = MSELoss()\n",
    "        if USE_COMPILED:\n",
    "            self.warp_layer = Warp(3, \"border\").to(self.device)\n",
    "        else:\n",
    "            self.warp_layer = Warp(\"bilinear\", \"border\").to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), 1e-5)\n",
    "        \n",
    "    def training_data(self, batch_size = 20):\n",
    "        # Custom torch Dataloader for MedNIST data\n",
    "        data_path = self.dataset_path.split('MedNIST/')[1]\n",
    "        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n",
    "        training_datadict = [\n",
    "            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n",
    "            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n",
    "        ]\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n",
    "                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n",
    "                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n",
    "                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n",
    "                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "            ]\n",
    "        )\n",
    "        train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,\n",
    "                                cache_rate=1.0, num_workers=4)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 2}\n",
    "        train_loader = DataLoader(train_ds, **train_kwargs)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    # to be modified?\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, moving, fixed):\n",
    "        ddf = self.forward(torch.cat((moving, fixed), dim=1))\n",
    "        pred_image = self.warp_layer(moving, ddf)\n",
    "        loss = self.image_loss(pred_image, fixed)\n",
    "        return loss\n",
    "    \n",
    "    def training_routine(self,\n",
    "                         epochs: int = 2,\n",
    "                         log_interval: int = 10,\n",
    "                         lr: Union[int, float] = 1e-3,\n",
    "                         batch_size: int = 48,\n",
    "                         batch_maxnum: int = 0,\n",
    "                         dry_run: bool = False,\n",
    "                         monitor=None):\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # use_cuda = torch.cuda.is_available()\n",
    "        # device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            training_data = self.training_data(batch_size=batch_size)\n",
    "            for batch_idx,batch_data in enumerate(training_data):\n",
    "                self.optimizer.zero_grad()\n",
    "                moving = batch_data[\"moving_hand\"].to(self.device)\n",
    "                fixed = batch_data[\"fixed_hand\"].to(self.device)\n",
    "                res = self.training_step(moving, fixed)\n",
    "                res.backward()\n",
    "                #loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # do not take into account more than batch_maxnum\n",
    "                # batches from the dataset\n",
    "                if (batch_maxnum > 0) and (batch_idx >= batch_maxnum):\n",
    "                    #print('Reached {} batches for this epoch, ignore remaining data'.format(batch_maxnum))\n",
    "                    logger.debug('Reached {} batches for this epoch, ignore remaining data'.format(batch_maxnum))\n",
    "                    break\n",
    "\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(moving),\n",
    "                        len(training_data.dataset),\n",
    "                        100 * batch_idx / len(training_data),\n",
    "                        res.item()))\n",
    "\n",
    "                    # Send scalar values via general/feedback topic\n",
    "                    if monitor is not None:\n",
    "                        monitor.add_scalar('Loss', res.item(), batch_idx, epoch)\n",
    "\n",
    "                    if dry_run:\n",
    "                        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the client side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the client side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {'USE_COMPILED': True}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 20, \n",
    "    'lr': 1e-5, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of client ID with `clients`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `rounds` rounds, applying the `client_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 15:13:14,842 fedbiomed INFO - Searching dataset with data tags: ['mednist'] for all nodes\n",
      "2021-12-17 15:13:14,856 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'tags': ['mednist'], 'command': 'search'}\n",
      "2021-12-17 15:13:14,861 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'tags': ['mednist'], 'command': 'search'}\n",
      "2021-12-17 15:13:24,860 fedbiomed INFO - Node selected for training -> node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f\n",
      "2021-12-17 15:13:24,861 fedbiomed INFO - Node selected for training -> node_a6c386ed-2df1-40d9-9bdd-89c7606c8581\n",
      "2021-12-17 15:13:24,862 fedbiomed INFO - Checking data quality of federated datasets...\n",
      "/Users/balelli/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/monai/networks/blocks/warp.py:65: UserWarning: monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n",
      "  warnings.warn(\"monai.networks.blocks.Warp: Using PyTorch native grid_sample.\")\n",
      "2021-12-17 15:13:24,986 fedbiomed DEBUG - torchnn saved model filename: /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/tmpjca85p0a/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['mednist']\n",
    "rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 #clients=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyMonaiTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `rounds` are done for all the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 15:13:27,617 fedbiomed INFO - Sampled nodes in round 0 ['node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f', 'node_a6c386ed-2df1-40d9-9bdd-89c7606c8581']\n",
      "2021-12-17 15:13:27,620 fedbiomed INFO - Send message to node node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f - {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'model_args': {'USE_COMPILED': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'model_class': 'MyMonaiTrainingPlan', 'training_data': {'node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f': ['dataset_13af71e1-82a2-428d-8799-411874784dbb']}}\n",
      "2021-12-17 15:13:27,622 fedbiomed DEBUG - researcher_860a6809-2bc7-47ed-8f94-423486e71805\n",
      "2021-12-17 15:13:27,626 fedbiomed INFO - Send message to node node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 - {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'model_args': {'USE_COMPILED': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'model_class': 'MyMonaiTrainingPlan', 'training_data': {'node_a6c386ed-2df1-40d9-9bdd-89c7606c8581': ['dataset_53f0d322-3403-4789-883a-a92eb4f6e34c']}}\n",
      "2021-12-17 15:13:27,633 fedbiomed DEBUG - researcher_860a6809-2bc7-47ed-8f94-423486e71805\n",
      "2021-12-17 15:13:27,640 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'model_args': {'USE_COMPILED': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'model_class': 'MyMonaiTrainingPlan', 'training_data': {'node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f': ['dataset_13af71e1-82a2-428d-8799-411874784dbb']}}\n",
      "2021-12-17 15:13:27,656 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / DEBUG - Message received: {'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'model_args': {'USE_COMPILED': True}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'model_class': 'MyMonaiTrainingPlan', 'training_data': {'node_a6c386ed-2df1-40d9-9bdd-89c7606c8581': ['dataset_53f0d322-3403-4789-883a-a92eb4f6e34c']}}\n",
      "2021-12-17 15:13:27,668 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / DEBUG - [TASKS QUEUE] Item:{'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'training_data': {'node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f': ['dataset_13af71e1-82a2-428d-8799-411874784dbb']}, 'model_args': {'USE_COMPILED': True}, 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'model_class': 'MyMonaiTrainingPlan', 'command': 'train'}\n",
      "2021-12-17 15:13:27,670 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / DEBUG - [TASKS QUEUE] Item:{'researcher_id': 'researcher_860a6809-2bc7-47ed-8f94-423486e71805', 'job_id': '38893627-7f3a-479c-9cde-f8489e298d6a', 'params_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_298789a9-be40-4a2d-aa9a-bb52df16bdff.pt', 'training_args': {'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}, 'training_data': {'node_a6c386ed-2df1-40d9-9bdd-89c7606c8581': ['dataset_53f0d322-3403-4789-883a-a92eb4f6e34c']}, 'model_args': {'USE_COMPILED': True}, 'model_url': 'http://localhost:8844/media/uploads/2021/12/17/my_model_4bc2380e-71b5-474c-b4ae-2c21e24a01b3.py', 'model_class': 'MyMonaiTrainingPlan', 'command': 'train'}\n",
      "2021-12-17 15:13:28,017 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / INFO - {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x1546c2160>, 'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}\n",
      "2021-12-17 15:13:28,019 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / DEBUG - Dataset_path/Users/balelli/Downloads/MedNIST_2/MedNIST\n",
      "2021-12-17 15:13:28,021 fedbiomed INFO - log from: node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f / ERROR - Cannot train model in round: list index out of range\n",
      "2021-12-17 15:13:28,039 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / INFO - {'monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x14ee8fee0>, 'batch_size': 20, 'lr': 1e-05, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 250}\n",
      "2021-12-17 15:13:28,041 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / DEBUG - Dataset_path/Users/balelli/Downloads/MedNIST_1/MedNIST\n",
      "2021-12-17 15:13:28,043 fedbiomed INFO - log from: node_a6c386ed-2df1-40d9-9bdd-89c7606c8581 / ERROR - Cannot train model in round: list index out of range\n",
      "2021-12-17 15:13:37,671 fedbiomed INFO - Downloading model params after training on node_4f0f6a9a-a2d6-4e17-a448-7c853bcf561f - from \n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '': No schema supplied. Perhaps you meant http://?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a33583383f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/fedbiomed/researcher/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sync)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sampled nodes in round '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Trigger training round on sampled nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0manswering_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_nodes_training_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# refining/normalizing model weigths received from nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/fedbiomed/researcher/job.py\u001b[0m in \u001b[0;36mstart_nodes_training_round\u001b[0;34m(self, round)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# TODO : handle error depending on status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading model params after training on \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'node_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' - from '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'my_model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# TODO: could choose completely different name/structure for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/fedbiomed/common/repository.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, url, filename)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mwhich\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtemporary\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msaved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fedbiomed-researcher/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available in `exp.training_replies` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies.keys())\n",
    "\n",
    "print(\"\\nList the clients for the last training round and their timings : \")\n",
    "round_data = exp.training_replies[rounds - 1].data\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies[rounds - 1].dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available in `exp.aggregated_params` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()\n",
    "\n",
    "data_dir = '/Users/balelli/Downloads/MedNIST_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(x for x in os.listdir(data_dir)\n",
    "                     if os.path.isdir(os.path.join(data_dir, x)))\n",
    "num_class = len(class_names)\n",
    "image_files = [\n",
    "    [\n",
    "        os.path.join(data_dir, class_names[i], x)\n",
    "        for x in os.listdir(os.path.join(data_dir, class_names[i]))\n",
    "    ]\n",
    "    for i in range(num_class)\n",
    "]\n",
    "\n",
    "num_each = [len(image_files[i]) for i in range(num_class)]\n",
    "image_files_list = []\n",
    "\n",
    "image_class = []\n",
    "for i in range(num_class):\n",
    "    image_files_list.extend(image_files[i])\n",
    "    image_class.extend([i] * num_each[i])\n",
    "num_total = len(image_class)\n",
    "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
    "\n",
    "print(f\"Total image count: {num_total}\")\n",
    "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
    "print(f\"Label names: {class_names}\")\n",
    "print(f\"Label counts: {num_each}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(image_files_list)\n",
    "indices = np.arange(length)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "test_split = int(0.1 * length)\n",
    "test_indices = indices[:test_split]\n",
    "\n",
    "test_x = [image_files_list[i] for i in test_indices]\n",
    "test_y = [image_class[i] for i in test_indices]\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])\n",
    "\n",
    "y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\n",
    "y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=num_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]\n",
    "\n",
    "\n",
    "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=1,\n",
    "                    out_channels=num_class).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "max_epochs = 4\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exp.model_instance\n",
    "model.load_state_dict(exp.aggregated_params[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[0].to(device),\n",
    "            test_data[1].to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    y_true, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
