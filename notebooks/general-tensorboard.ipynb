{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node run`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MyTrainingPlan, self).__init__(model_args)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\"\n",
    "               ]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 2, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:11:26,304 fedbiomed INFO - Component environment:\n",
      "2022-03-18 16:11:26,306 fedbiomed INFO - type = ComponentType.RESEARCHER\n",
      "2022-03-18 16:11:26,706 fedbiomed INFO - Messaging researcher_96a37edc-2ba8-47d7-aa8e-33679104e4b2 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7f5ae26de0a0>\n",
      "2022-03-18 16:11:26,745 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2022-03-18 16:11:36,759 fedbiomed INFO - Node selected for training -> node_19ef0050-617d-4624-bbce-207469edf883\n",
      "2022-03-18 16:11:36,794 fedbiomed DEBUG - Model file has been saved: /home/scansiz/Desktop/Inria/development/fedbiomed/var/experiments/Experiment_0133/my_model_095bee68-3c71-4631-b4e4-b6240023330e.py\n",
      "2022-03-18 16:11:36,833 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/Desktop/Inria/development/fedbiomed/var/experiments/Experiment_0133/my_model_095bee68-3c71-4631-b4e4-b6240023330e.py successful, with status code 201\n",
      "2022-03-18 16:11:37,128 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/Desktop/Inria/development/fedbiomed/var/experiments/Experiment_0133/aggregated_params_init_daf78187-86a3-4745-b8d1-a4ba74cb3882.pt successful, with status code 201\n",
      "2022-03-18 16:11:37,130 fedbiomed INFO - Removing tensorboard logs from previous experiment\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 3\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None,\n",
    "                 tensorboard=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard to see loss value after every iteration during training. It is normal to see empty screen. After you run the experiment you will be able to see the changes on the dashboard. Notebook will refresh results in every 30 seconds. You can also click refresh button to see current training steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "tensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:11:37,141 fedbiomed INFO - Sampled nodes in round 0 ['node_19ef0050-617d-4624-bbce-207469edf883']\n",
      "2022-03-18 16:11:37,143 fedbiomed INFO - Send message to node node_19ef0050-617d-4624-bbce-207469edf883 - {'researcher_id': 'researcher_96a37edc-2ba8-47d7-aa8e-33679104e4b2', 'job_id': 'b8e3273e-b9fd-4d89-8dc2-3ae34569ca47', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 2, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2022/03/18/my_model_095bee68-3c71-4631-b4e4-b6240023330e.py', 'params_url': 'http://localhost:8844/media/uploads/2022/03/18/aggregated_params_init_daf78187-86a3-4745-b8d1-a4ba74cb3882.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_19ef0050-617d-4624-bbce-207469edf883': ['dataset_ba55374f-ddc3-4f5d-8bb6-deac79c459ee']}}\n",
      "2022-03-18 16:11:37,143 fedbiomed DEBUG - researcher_96a37edc-2ba8-47d7-aa8e-33679104e4b2\n",
      "2022-03-18 16:11:38,049 fedbiomed INFO - log from: node_19ef0050-617d-4624-bbce-207469edf883 / INFO - training with arguments {'history_monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7f2bdf2f5880>, 'node_args': {'gpu': False, 'gpu_num': None, 'gpu_only': False}, 'batch_size': 48, 'lr': 0.001, 'epochs': 2, 'dry_run': False, 'batch_maxnum': 100}\n",
      "2022-03-18 16:11:50,900 fedbiomed INFO - log from: node_19ef0050-617d-4624-bbce-207469edf883 / INFO - Actual/True values (y_true) has more than two levels, using multiclass `weighted` calculation for the metric RECALL\n",
      "2022-03-18 16:11:50,916 fedbiomed INFO - \u001b[1mTESTING BEFORE TRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Completed: 12000/12000 (100%) \n",
      " \t\t\t\t\t RECALL: \u001b[1m0.090083\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:11:52,474 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 480/48000 (1%) \n",
      " \t\t\t\t\t Loss: \u001b[1m1.581026\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:11:54,147 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 960/48000 (2%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.793630\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:11:55,678 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 1440/48000 (3%) \n",
      " \t\t\t\t\t Loss: \u001b[1m1.036878\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:11:57,091 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 1920/48000 (4%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.370937\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:11:58,544 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 2400/48000 (5%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.706465\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:00,097 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 2880/48000 (6%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.315601\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:01,404 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 3360/48000 (7%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.385467\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:02,689 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 3840/48000 (8%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.544615\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:03,992 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 1 | Completed: 4320/48000 (9%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.215874\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:06,602 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 480/48000 (1%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.255903\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:07,908 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 960/48000 (2%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.238337\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:09,395 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 1440/48000 (3%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.202976\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:10,769 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 1920/48000 (4%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.289897\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:11,974 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 2400/48000 (5%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.214859\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:13,251 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 2880/48000 (6%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.477324\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:14,451 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 3360/48000 (7%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.399044\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:15,670 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 3840/48000 (8%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.249383\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:16,875 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Epoch: 2 | Completed: 4320/48000 (9%) \n",
      " \t\t\t\t\t Loss: \u001b[1m0.255545\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:30,173 fedbiomed INFO - \u001b[1mTESTING AFTER TRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_19ef0050-617d-4624-bbce-207469edf883 \n",
      "\t\t\t\t\t Completed: 12000/12000 (100%) \n",
      " \t\t\t\t\t ACCURACY: \u001b[1m0.965750\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2022-03-18 16:12:30,525 fedbiomed INFO - log from: node_19ef0050-617d-4624-bbce-207469edf883 / INFO - results uploaded successfully \n",
      "2022-03-18 16:12:37,227 fedbiomed INFO - Downloading model params after training on node_19ef0050-617d-4624-bbce-207469edf883 - from http://localhost:8844/media/uploads/2022/03/18/node_params_a6d244d7-eeaa-44a6-a887-7793423ea279.pt\n",
      "2022-03-18 16:12:37,304 fedbiomed DEBUG - upload (HTTP GET request) of file node_params_0d7e40db-2ff9-4e1b-b083-8057f0c9d02f.pt successful, with status code 200\n",
      "2022-03-18 16:12:37,344 fedbiomed INFO - Nodes that successfully reply in round 0 ['node_19ef0050-617d-4624-bbce-207469edf883']\n",
      "2022-03-18 16:12:37,654 fedbiomed DEBUG - upload (HTTP POST request) of file /home/scansiz/Desktop/Inria/development/fedbiomed/var/experiments/Experiment_0133/aggregated_params_5439c419-45fe-4857-864f-78e150f8fba2.pt successful, with status code 201\n",
      "2022-03-18 16:12:37,660 fedbiomed INFO - Saved aggregated params for round 0 in /home/scansiz/Desktop/Inria/development/fedbiomed/var/experiments/Experiment_0133/aggregated_params_5439c419-45fe-4857-864f-78e150f8fba2.pt\n",
      "2022-03-18 16:12:37,662 fedbiomed INFO - Sampled nodes in round 1 ['node_19ef0050-617d-4624-bbce-207469edf883']\n",
      "2022-03-18 16:12:37,665 fedbiomed INFO - Send message to node node_19ef0050-617d-4624-bbce-207469edf883 - {'researcher_id': 'researcher_96a37edc-2ba8-47d7-aa8e-33679104e4b2', 'job_id': 'b8e3273e-b9fd-4d89-8dc2-3ae34569ca47', 'training_args': {'batch_size': 48, 'lr': 0.001, 'epochs': 2, 'dry_run': False, 'batch_maxnum': 100}, 'model_args': {}, 'command': 'train', 'model_url': 'http://localhost:8844/media/uploads/2022/03/18/my_model_095bee68-3c71-4631-b4e4-b6240023330e.py', 'params_url': 'http://localhost:8844/media/uploads/2022/03/18/aggregated_params_5439c419-45fe-4857-864f-78e150f8fba2.pt', 'model_class': 'MyTrainingPlan', 'training_data': {'node_19ef0050-617d-4624-bbce-207469edf883': ['dataset_ba55374f-ddc3-4f5d-8bb6-deac79c459ee']}}\n",
      "2022-03-18 16:12:37,668 fedbiomed DEBUG - researcher_96a37edc-2ba8-47d7-aa8e-33679104e4b2\n",
      "2022-03-18 16:12:37,894 fedbiomed INFO - log from: node_19ef0050-617d-4624-bbce-207469edf883 / INFO - training with arguments {'history_monitor': <fedbiomed.node.history_monitor.HistoryMonitor object at 0x7f2bdf2f5fd0>, 'node_args': {'gpu': False, 'gpu_num': None, 'gpu_only': False}, 'batch_size': 48, 'lr': 0.001, 'epochs': 2, 'dry_run': False, 'batch_maxnum': 100}\n",
      "2022-03-18 16:12:47,844 fedbiomed INFO - log from: node_19ef0050-617d-4624-bbce-207469edf883 / CRITICAL - Node stopped in signal_handler, probably by user decision (Ctrl C)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:12:50,417 fedbiomed CRITICAL - Fed-BioMed researcher stopped due to keyboard interrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Fed-BioMed researcher stopped due to keyboard interrupt\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display current values please click refresh button on the TensorBoard screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional : searching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "\n",
    "r = Requests()\n",
    "data = r.search(tags)\n",
    "\n",
    "import pandas as pd\n",
    "for node_id in data.keys():\n",
    "    print('\\n','Data for ', node_id, '\\n\\n', pd.DataFrame(data[node_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Feel free to try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
