{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee99bdb4",
   "metadata": {},
   "source": [
    "Related to user story: [SP11-Item04: General Data Wrapper PoC](https://gitlab.inria.fr/fedbiomed/fedbiomed/-/issues/164)\n",
    "\n",
    "## Tabular dataset\n",
    "\n",
    "Workflow of data pre processing:\n",
    "\n",
    "1. Columns name should be shared with the researcher\n",
    "2. Data format file to be filled by clinicians.\n",
    "3. Specify if missing data are allowed for a given columns (Exception). The file will be used for data verification during FL pre-processing,\n",
    "4. Outlier verification for quantitative data, continuous and discrete, and for dates (Critical warning),\n",
    "5. Missing data imputation by local mean (or optional NN), or majority voting for discrete labels. Give warnings when missing data are found (for verification a posteriori).\n",
    "6. Give critical warning when too many missing are found (>50%),\n",
    "7. Verify that number of available data is greater then minimum required (Error)\n",
    "\n",
    "Critical warnings have different levels of disclosure to the researcher (1) only the warning, 2) type of warning, 3) type of warning and column affected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b334c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-2.4.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33ae4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. load  a single view dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17059269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class ExcelSignatures(Enum):\n",
    "    XLSX = (b'\\x50\\x4B\\x05\\x06', 2, -22, 4)\n",
    "    LSX1 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 512, 8)\n",
    "    LSX2 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 1536, 8)\n",
    "    LSX3 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 2048, 8)\n",
    "    \n",
    "    def __init__(self, sig, whence, offset, size):\n",
    "        self._sig = sig\n",
    "        self._whence = whence\n",
    "        self._offset = offset\n",
    "        self._size = size\n",
    "\n",
    "    @property \n",
    "    def signature(self) -> bytes:\n",
    "        return self._sig\n",
    "    \n",
    "    @property\n",
    "    def whence(self) -> int:\n",
    "        return self._whence\n",
    "    \n",
    "    @property\n",
    "    def offset(self) -> int:\n",
    "        return self._offset\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self._size\n",
    "\n",
    "\n",
    "\n",
    "def load_tabular_datasets(path:str) -> Dict[str, pd.DataFrame]:\n",
    "    tabular_datasets = {}\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        print('directory found')\n",
    "        _is_folder = True\n",
    "        \n",
    "        _tabular_data_files = os.listdir(path)\n",
    "    else:\n",
    "        print('file found')\n",
    "        _is_folder = False\n",
    "        _tabular_data_files = (path,)\n",
    "        \n",
    "    for tabular_data_file in _tabular_data_files:\n",
    "        if _is_folder:\n",
    "            tabular_data_file = os.path.join(path, tabular_data_file)\n",
    "        \n",
    "        _is_excel = excel_sniffer(tabular_data_file)\n",
    "        _csv_delimiter, _csv_header = csv_sniffer(tabular_data_file)\n",
    "        _view_name = os.path.basename(tabular_data_file)\n",
    "        if _is_excel:\n",
    "            tabular_datasets[_view_name] = load_excel_file(tabular_data_file)\n",
    "        elif _csv_delimiter is not None:\n",
    "            tabular_datasets[_view_name] = load_csv_file(tabular_data_file,\n",
    "                                                               _csv_delimiter, \n",
    "                                                               _csv_header)\n",
    "        else:\n",
    "            print(f'warning: cannot parse {tabular_data_file}: not a tabular data file')\n",
    "        \n",
    "    return tabular_datasets\n",
    "\n",
    "def load_csv_file(path:str, delimiter:str, header:int) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataframe = pd.read_csv(path, delimiter=delimiter, header=header)\n",
    "    except csv.Error as err:\n",
    "        print('err', err, 'in file', path)\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "#https://stackoverflow.com/questions/23515791/how-to-check-the-uploaded-file-is-csv-or-xls-in-python/23515973\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_excel_file(path:str, sheet_name: Union[str, int]=0) -> pd.DataFrame:\n",
    "    \"\"\"May rely on openpyxl package\"\"\"\n",
    "    #with open(path, 'r') as excl:\n",
    "    #    _c = csv.DictReader(excl, dialect=csv.excel_tab)\n",
    "    #    _delimiter = _c.dialect.delimiter\n",
    "    \n",
    "    dataframe = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def excel_sniffer(path: str) -> bool:\n",
    "    \n",
    "    for excel_sig in ExcelSignatures:\n",
    "        with open(path, 'rb') as f:\n",
    "            f.seek(excel_sig.offset, excel_sig.whence)\n",
    "            bytes = f.read(excel_sig.size)\n",
    "\n",
    "            if bytes == excel_sig.signature:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "\n",
    "def csv_sniffer(path:str) :\n",
    "        \n",
    "    with open(path, 'r') as csvfile:\n",
    "        try:\n",
    "            # do some operation on file using sniffer to make sure considered file\n",
    "            # is a CSV file\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline())\n",
    "            delimiter = dialect.delimiter\n",
    "            dialect.lineterminator\n",
    "            has_header = csv.Sniffer().has_header(csvfile.readline())\n",
    "            if has_header:\n",
    "                header = 0\n",
    "            else:\n",
    "                header = None\n",
    "        except (csv.Error, UnicodeDecodeError) as err:\n",
    "            delimiter, header = None, None\n",
    "            print('err', err, 'in file', path)\n",
    "    return delimiter, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "750c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (3.0.9)\r\n",
      "Requirement already satisfied: et-xmlfile in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from openpyxl) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n",
      "err 'utf-8' codec can't decode byte 0x8c in position 15: invalid start byte in file ../../Exceltest.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Exceltest.xlsx':    ID   Age Eligibility\n",
       " 0    1   45           Y\n",
       " 1    2   45           Y\n",
       " 2    3   33           N\n",
       " 3    4   54           Y\n",
       " 4    5   45           Y\n",
       " 5    6   54         NaN\n",
       " 6    7   34           N\n",
       " 7    8   54         NaN\n",
       " 8    9   45         NaN\n",
       " 9   10   44           Y}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tabular_datasets('../../Exceltest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "single_view_dataset = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ed673dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test7/file1':      a   e   i   o      0      1      2      3                 time  pressure  \\\n",
       " 0   48  98  65   5  False   True  False  False  2018-01-01 00:00:00  0.088082   \n",
       " 1   87  83  13  70   True  False   True  False  2018-01-01 01:00:00  0.774788   \n",
       " 2   46  73  81  96  False  False  False   True  2018-01-01 02:00:00  0.514092   \n",
       " 3   84  45  81  39  False   True   True   True  2018-01-01 03:00:00  0.832881   \n",
       " 4   94  84   0  15  False   True  False  False  2018-01-01 04:00:00  0.696152   \n",
       " ..  ..  ..  ..  ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95  14  66  25  64   True   True   True  False  2018-01-04 23:00:00  0.295578   \n",
       " 96  91  81  48  53  False  False   True   True  2018-01-05 00:00:00  0.474322   \n",
       " 97  15  82  12  51   True   True   True   True  2018-01-05 01:00:00  0.927511   \n",
       " 98  51  18   4  52  False  False   True   True  2018-01-05 02:00:00  0.494798   \n",
       " 99  51  70  63  77  False  False   True  False  2018-01-05 03:00:00  0.316395   \n",
       " \n",
       "         sp02  a.1  e.1  i.1  o.1 gender blood type                  pkey  \n",
       " 0   0.360430   90   63   60    8    MAN          A  zmixzrgvxrjqxoe sluk  \n",
       " 1   0.072426   15   20   45   10    MAN          O  vrzahnpfluspdcbfnaqt  \n",
       " 2   0.538551   30    2    6   22  WOMAN          A  pnrepvmrxqabdlvisclv  \n",
       " 3   0.761962   42   70    7   79  WOMAN         AB  gwj luzejwdxzsiljxzd  \n",
       " 4   0.389385   65   90   12   90    MAN          B  jjdvcnofivbqhirxzdyo  \n",
       " ..       ...  ...  ...  ...  ...    ...        ...                   ...  \n",
       " 95  0.027930   13   41   93   19  WOMAN          A  hrvepmqjn llgbzplshv  \n",
       " 96  0.545006   91   41   99   71  WOMAN          B  wroevwyuamxibzshlxxh  \n",
       " 97  0.059205   38    7   73   47    MAN          B  ywadcykylymkdtzfctpg  \n",
       " 98  0.125428   38   11   97    3    MAN          O  ruchbfa zwgenxslegrl  \n",
       " 99  0.772293   49   74   36   42  WOMAN         AB  zeapltpxuvuibfybxcll  \n",
       " \n",
       " [100 rows x 18 columns],\n",
       " 'test7/contatct':     discrete       city                  pkey\n",
       " 0       64.0      Lille  qpqorfhylu gmfjy bdj\n",
       " 1       26.0      Lille  kkmjozalfyirgsire ui\n",
       " 2       61.0      Paris  ezfasuuycdda foisjte\n",
       " 3       29.0      Paris  faxiqkt xggzmwzoidbg\n",
       " 4       99.0      Lille  znwhlj rwzdutnagwasy\n",
       " ..       ...        ...                   ...\n",
       " 95       9.0      Paris  zeqhcikzdodus jn qjf\n",
       " 96      98.0  Marseille  iicthcvfmkajbvr gzir\n",
       " 97      21.0      Lille  ztjakcsk bhjoksdz lm\n",
       " 98      42.0  Marseille   sabunaa opt vpulnxj\n",
       " 99       3.0      Paris  qmbexyexvgromrm admu\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'test7/file2':         0      1      2      3                 time        pH  \\\n",
       " 0   False   True  False   True  2018-01-01 00:00:00  0.023107   \n",
       " 1    True  False  False  False  2018-01-01 01:00:00       NaN   \n",
       " 2   False   True  False   True  2018-01-01 02:00:00  0.407279   \n",
       " 3    True   True   True   True  2018-01-01 03:00:00  0.536301   \n",
       " 4   False   True   True   True  2018-01-01 04:00:00  0.749443   \n",
       " ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95   True   True   True  False  2018-01-04 23:00:00       NaN   \n",
       " 96   True  False  False  False  2018-01-05 00:00:00  0.388389   \n",
       " 97  False   True   True  False  2018-01-05 01:00:00  0.889067   \n",
       " 98   True   True  False  False  2018-01-05 02:00:00  0.402979   \n",
       " 99   True  False   True  False  2018-01-05 03:00:00  0.667349   \n",
       " \n",
       "                     pkey  \n",
       " 0   kkmjozalfyirgsire ui  \n",
       " 1   xkdawggpnuulcewuoyzz  \n",
       " 2   khuulhwgwnjggrfoefce  \n",
       " 3   xxysdmwwmjsmyhaswfdb  \n",
       " 4   ldejfuij mnbnf wwmms  \n",
       " ..                   ...  \n",
       " 95  wrmdecb s pohtmrcdj   \n",
       " 96  whmwrpvqmerdpwwzxasf  \n",
       " 97  pnrepvmrxqabdlvisclv  \n",
       " 98  iicthcvfmkajbvr gzir  \n",
       " 99  kiejdmbuih awhuifwwd  \n",
       " \n",
       " [100 rows x 7 columns]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_view_dataframe =  load_tabular_datasets('test7')\n",
    "multi_view_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d533e99",
   "metadata": {},
   "source": [
    "Data format file to be filled by clinicians (step 2 int he workflow):\n",
    "\n",
    "Data format file will be a dictionary specifying the type: \n",
    "* for single view datasets:\n",
    "```{<feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}```\n",
    " * for multiview datatset\n",
    "```{{<view_name>: <feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}}```\n",
    "\n",
    "where\n",
    "* `<view_name>` is the name of the view\n",
    "* `<feature_name>` is the name of the feature\n",
    "* `<data_type>` can be categorical or continuous or missing_data or datetime\n",
    "* `<value_taken>` is the type of the value (eg int, char, float, signed, unsigned ...)\n",
    "* `<value_range>` represent either a list of bounds, an upper or a lower bound, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134b8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create data format file\n",
    "\n",
    "import numpy as np\n",
    "import enum\n",
    "from enum import Enum, auto\n",
    "import datetime\n",
    "\n",
    "# the use of Enum classes will prevent incorrect combination of values\n",
    "class QuantitativeDataType(Enum):\n",
    "    CONTINUOUS = [float, np.float64]\n",
    "    DISCRETE = [int, np.int64]\n",
    "\n",
    "class CategoricalDataType(Enum):\n",
    "    BOOLEAN = [bool]\n",
    "    NUMERICAL = [float, int, np.float64, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "class KeyDataType(Enum):\n",
    "    NUMERICAL = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    DATETIME = \"DATETIME\"\n",
    "    \n",
    "class DataType(Enum):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    KEY = [KeyDataType.NUMERICAL,\n",
    "           KeyDataType.CHARACTER,\n",
    "           KeyDataType.DATETIME]\n",
    "    QUANTITATIVE = [QuantitativeDataType.CONTINUOUS,\n",
    "                   QuantitativeDataType.DISCRETE]\n",
    "    CATEGORICAL = [CategoricalDataType.BOOLEAN,\n",
    "                  CategoricalDataType.NUMERICAL,\n",
    "                  CategoricalDataType.CHARACTER]\n",
    "    #MISSING = 'MISSING'\n",
    "    #DATETIME = 'DATETIME'\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "    UNKNOWN = 'UNKNOWN'\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in DataType.__members__.items())\n",
    "\n",
    "class MissingValueAllowedDefault(Enum):\n",
    "    KEY = False\n",
    "    QUANTITATIVE = True\n",
    "    CATEGORICAL = True\n",
    "    DATETIME = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in MissingValueAllowedDefault.__members__.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8d35b2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'bool'>\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "str(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type(\n",
    "                  #avail_data_types: enum.EnumMeta,\n",
    "                  d_format: Enum,\n",
    "                  d_type: type) ->  Tuple[Enum, List[Union[type, str]]]:\n",
    "    present_d_types = []\n",
    "    sub_d_type_format = None\n",
    "    for avail_data_type in DataType:\n",
    "        if d_format is avail_data_type:\n",
    "            sub_dtypes = avail_data_type.value\n",
    "            if not isinstance(sub_dtypes, str) and hasattr(sub_dtypes, '__getitem__') and isinstance(sub_dtypes[0], Enum):\n",
    "                # check if dtype has subtypes\n",
    "                #(eg if datatype is QUANTITATIVE, subtype will be CONTINOUS or DISCRETE)\n",
    "                for sub_dtype in sub_dtypes:\n",
    "                    if any(d_type == t for t in tuple(sub_dtype.value)):\n",
    "                        present_d_types.append(d_type)\n",
    "                        sub_d_type_format = sub_dtype\n",
    "                        print(sub_dtype, d_type)\n",
    "            else:\n",
    "                \n",
    "                present_d_types.append(sub_dtypes)\n",
    "                sub_d_type_format = sub_dtypes\n",
    "    return  sub_d_type_format, present_d_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00bd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "691fe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "df = pd.DataFrame({'w': [1, 2, 3, 4,  'jj', None]})\n",
    "print(check_missing_data(df['w']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f354b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936ac8f0",
   "metadata": {},
   "source": [
    "CLI details:\n",
    "\n",
    "1. open  csv file\n",
    "2. for each columns in file ask type of variable or if variable should be excluded\n",
    "3. automatically detect the type given values in columns \n",
    "4. ask for each columns if missing data are allowed\n",
    "\n",
    "\n",
    "eg :\n",
    "\n",
    "assume a column is of type discrete with integers\n",
    "\n",
    "\n",
    "1. user select it is quantitative\n",
    "2. then system will label it as quantitative-discrete\n",
    "\n",
    "\n",
    "**Question** : do we want an auto selection parameter choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d328b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_msg() -> str:\n",
    "    msg_yes_or_no_question = '1) YES\\n2) NO\\n'   \n",
    "    return msg_yes_or_no_question\n",
    "\n",
    "def parse_yes_no_msg(resp: str) -> bool:\n",
    "    \"\"\"implements logic to parse yes or no msg\"\"\"\n",
    "    yes_or_no_question_key = {'1': True,\n",
    "                    '2': False}\n",
    "    return yes_or_no_question_key.get(resp)\n",
    "\n",
    "def get_data_type_selection_msg(available_data_type:List[Enum]) ->Tuple[str, int]:\n",
    "    \n",
    "    n_available_data_type = len(available_data_type)\n",
    "    msg = ''\n",
    "\n",
    "    \n",
    "    for i, dtype in enumerate(available_data_type):\n",
    "        msg += '%d) %s \\n' %  (i+1, dtype.name)\n",
    "    \n",
    "    ignoring_key = i+2  # add ingoring entry\n",
    "    msg += '%d) ignore this column\\n' % (ignoring_key)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return msg, ignoring_key\n",
    "\n",
    "def unique(iterable: Iterator) -> int:\n",
    "    \"\"\"returns number of unique values\"\"\"\n",
    "    return len(set(iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "800e755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI for clinicians for setting up data format file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b6238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_user_dataframe_format_file(dataset: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \n",
    "    dataset_columns = dataset.columns\n",
    "    dataset_columns_length = len(dataset_columns)\n",
    "    data_format_file = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    available_data_type = [d_type for d_type in DataType]  # get all available data types\n",
    "    \n",
    "    for n_feature, feature in enumerate(dataset_columns):\n",
    "        print(f'displaying first 10 values of feature {feature} (n_feature: {n_feature+1}/{dataset_columns_length})')\n",
    "        #print(tabulate(dataset[feature].head(10).values()))\n",
    "        pprint.pprint(dataset[feature].head(10))  # print first 10 lines of feature value\n",
    "        print(f'number of differents samples: {unique(dataset[feature])} / total of samples: {dataset[feature].shape[0]}')\n",
    "        \n",
    "        msg_data_type_selection, ignoring_id = get_data_type_selection_msg(available_data_type)\n",
    "        msg_data_type_selection = f'specify data type for {feature}:\\n' + msg_data_type_selection\n",
    "        \n",
    "        # ask user about data type\n",
    "        data_format_id = get_user_input(msg_data_type_selection,\n",
    "                                       \n",
    "                                       n_answers=dataset_columns_length+1)\n",
    "        \n",
    "        if int(data_format_id) > ignoring_id - 1:\n",
    "            # case where user decide to ingore column: go to next iteration (next feature)\n",
    "            print(f\"Ignoring feature {feature}\")\n",
    "            continue\n",
    "        else:\n",
    "            # case where user selected a data type: add data type and info to the format file\n",
    "            data_format = available_data_type[int(data_format_id)-1]\n",
    "            data_type = dataset[feature].dtype\n",
    "            n_data_type, types = get_data_type(data_format, data_type)\n",
    "\n",
    "        # KEY and DATETIME type \n",
    "        if data_format is DataType.KEY or data_format is DataType.DATETIME:  \n",
    "            is_missing_values_allowed = False\n",
    "        else: \n",
    "            # ask user if missing values are allowed for this specific variable\n",
    "            msg_yes_or_no_question = get_yes_no_msg()\n",
    "            msg_yes_or_no_question = f'Allow {feature} to have missing values:\\n' + msg_yes_or_no_question\n",
    "            missing_values_user_selection = get_user_input(msg_yes_or_no_question,\n",
    "                                                        n_answers=2)\n",
    "            is_missing_values_allowed = parse_yes_no_msg(missing_values_user_selection)\n",
    "            \n",
    "            data_format_file[feature] = {'data_format': data_format.name,\n",
    "                                         'data_type': n_data_type.name,\n",
    "                                         'values': str(data_type),\n",
    "                                         'is_missing_values': is_missing_values_allowed}\n",
    "            \n",
    "    return data_format_file\n",
    "            \n",
    "def get_user_input(msg:str,  n_answers:int) -> str:\n",
    "    \"\"\"\"\"\"\n",
    "    is_column_parsed = False\n",
    "    while not is_column_parsed:\n",
    "        #data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "        resp = input(msg)\n",
    "        if resp.isdigit() and int(resp) <= n_answers and int(resp)>0:\n",
    "            # check if value passed by user is correct (if it is integer,\n",
    "            # and whithin range [1, n_available_data_type])\n",
    "            is_column_parsed = True\n",
    "\n",
    "        else:\n",
    "            print(f'error ! {resp} value not understood')\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLI to use when dataset is available\n",
    "\n",
    "\n",
    "def get_from_user_multi_view_dataset_fromat_file(datasets: Dict[str, pd.DataFrame])-> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "    data_format_files = {}\n",
    "    \n",
    "    for tabular_data_file in datasets.keys():\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(f\"+++++++ Now parsing view: {tabular_data_file} +++++++\")\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        data_format_file = get_from_user_dataframe_format_file(datasets[tabular_data_file])\n",
    "        \n",
    "        _file_name = os.path.basename(tabular_data_file)\n",
    "        data_format_files[_file_name] = data_format_file\n",
    "        \n",
    "    return data_format_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8acbe629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: /user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature CDRSB.bl (n_feature: 1/16)\n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "6    4\n",
      "7    0\n",
      "8    3\n",
      "9    2\n",
      "Name: CDRSB.bl, dtype: int64\n",
      "number of differents samples: 8 / total of samples: 1000\n",
      "specify data type for CDRSB.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "CategoricalDataType.NUMERICAL int64\n",
      "Allow CDRSB.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "displaying first 10 values of feature ADAS11.bl (n_feature: 2/16)\n",
      "0     8\n",
      "1     0\n",
      "2     8\n",
      "3     3\n",
      "4     0\n",
      "5    10\n",
      "6    12\n",
      "7     2\n",
      "8     8\n",
      "9    11\n",
      "Name: ADAS11.bl, dtype: int64\n",
      "number of differents samples: 28 / total of samples: 1000\n",
      "specify data type for ADAS11.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "2\n",
      "QuantitativeDataType.DISCRETE int64\n",
      "Allow ADAS11.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "displaying first 10 values of feature MMSE.bl (n_feature: 3/16)\n",
      "0    27.0\n",
      "1    30.0\n",
      "2    24.0\n",
      "3    29.0\n",
      "4    30.0\n",
      "5    27.0\n",
      "6    24.0\n",
      "7    30.0\n",
      "8    28.0\n",
      "9    27.0\n",
      "Name: MMSE.bl, dtype: float64\n",
      "number of differents samples: 11 / total of samples: 1000\n",
      "specify data type for MMSE.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "CategoricalDataType.NUMERICAL float64\n",
      "Allow MMSE.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "3\n",
      "error ! 3 value not understood\n",
      "Allow MMSE.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "displaying first 10 values of feature RAVLT.immediate.bl (n_feature: 4/16)\n",
      "0    23.739439\n",
      "1    64.933800\n",
      "2    36.987722\n",
      "3    50.314425\n",
      "4    57.217830\n",
      "5    37.209473\n",
      "6    37.216348\n",
      "7    46.391300\n",
      "8    34.576231\n",
      "9    19.897128\n",
      "Name: RAVLT.immediate.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for RAVLT.immediate.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "2\n",
      "QuantitativeDataType.CONTINUOUS float64\n",
      "Allow RAVLT.immediate.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "displaying first 10 values of feature RAVLT.learning.bl (n_feature: 5/16)\n",
      "0    4.0\n",
      "1    9.0\n",
      "2    3.0\n",
      "3    5.0\n",
      "4    9.0\n",
      "5    2.0\n",
      "6    6.0\n",
      "7    6.0\n",
      "8    3.0\n",
      "9    1.0\n",
      "Name: RAVLT.learning.bl, dtype: float64\n",
      "number of differents samples: 13 / total of samples: 1000\n",
      "specify data type for RAVLT.learning.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "CategoricalDataType.NUMERICAL float64\n",
      "Allow RAVLT.learning.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "displaying first 10 values of feature RAVLT.forgetting.bl (n_feature: 6/16)\n",
      "0    5.821573\n",
      "1    4.001653\n",
      "2    6.876316\n",
      "3    4.733481\n",
      "4    7.225401\n",
      "5    8.421522\n",
      "6    6.137385\n",
      "7    5.128675\n",
      "8    4.023993\n",
      "9    4.142775\n",
      "Name: RAVLT.forgetting.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for RAVLT.forgetting.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature RAVLT.forgetting.bl\n",
      "displaying first 10 values of feature FAQ.bl (n_feature: 7/16)\n",
      "0     3\n",
      "1     0\n",
      "2     0\n",
      "3     3\n",
      "4     0\n",
      "5     4\n",
      "6    12\n",
      "7     0\n",
      "8    10\n",
      "9     6\n",
      "Name: FAQ.bl, dtype: int64\n",
      "number of differents samples: 20 / total of samples: 1000\n",
      "specify data type for FAQ.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "2\n",
      "QuantitativeDataType.DISCRETE int64\n",
      "Allow FAQ.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "displaying first 10 values of feature WholeBrain.bl (n_feature: 8/16)\n",
      "0    0.684331\n",
      "1    0.735892\n",
      "2    0.738731\n",
      "3    0.696179\n",
      "4    0.841806\n",
      "5    0.708125\n",
      "6    0.649689\n",
      "7    0.759112\n",
      "8    0.773193\n",
      "9    0.654161\n",
      "Name: WholeBrain.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for WholeBrain.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature WholeBrain.bl\n",
      "displaying first 10 values of feature Ventricles.bl (n_feature: 9/16)\n",
      "0    0.012699\n",
      "1    0.012803\n",
      "2    0.030492\n",
      "3    0.032797\n",
      "4    0.004030\n",
      "5    0.026286\n",
      "6    0.026068\n",
      "7    0.018997\n",
      "8    0.005643\n",
      "9    0.038188\n",
      "Name: Ventricles.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for Ventricles.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature Ventricles.bl\n",
      "displaying first 10 values of feature Hippocampus.bl (n_feature: 10/16)\n",
      "0    0.003786\n",
      "1    0.004866\n",
      "2    0.004300\n",
      "3    0.004720\n",
      "4    0.006820\n",
      "5    0.005262\n",
      "6    0.004314\n",
      "7    0.006479\n",
      "8    0.005609\n",
      "9    0.003478\n",
      "Name: Hippocampus.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for Hippocampus.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature Hippocampus.bl\n",
      "displaying first 10 values of feature MidTemp.bl (n_feature: 11/16)\n",
      "0    0.012678\n",
      "1    0.015071\n",
      "2    0.012419\n",
      "3    0.012312\n",
      "4    0.016948\n",
      "5    0.013632\n",
      "6    0.012296\n",
      "7    0.014903\n",
      "8    0.011644\n",
      "9    0.012776\n",
      "Name: MidTemp.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for MidTemp.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature MidTemp.bl\n",
      "displaying first 10 values of feature Entorhinal.bl (n_feature: 12/16)\n",
      "0    0.002214\n",
      "1    0.003041\n",
      "2    0.002316\n",
      "3    0.002593\n",
      "4    0.002896\n",
      "5    0.002358\n",
      "6    0.001747\n",
      "7    0.002881\n",
      "8    0.002748\n",
      "9    0.001833\n",
      "Name: Entorhinal.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for Entorhinal.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature Entorhinal.bl\n",
      "displaying first 10 values of feature ABETA.MEDIAN.bl (n_feature: 13/16)\n",
      "0    154.016065\n",
      "1    211.573206\n",
      "2    163.637668\n",
      "3    182.256297\n",
      "4    247.997479\n",
      "5    174.159343\n",
      "6    200.303050\n",
      "7    219.381922\n",
      "8    195.466999\n",
      "9    147.780038\n",
      "Name: ABETA.MEDIAN.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for ABETA.MEDIAN.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature ABETA.MEDIAN.bl\n",
      "displaying first 10 values of feature PTAU.MEDIAN.bl (n_feature: 14/16)\n",
      "0     67.970509\n",
      "1      5.451168\n",
      "2     66.704378\n",
      "3     47.091893\n",
      "4     -5.997140\n",
      "5     39.930517\n",
      "6     30.004188\n",
      "7     39.929520\n",
      "8    103.082202\n",
      "9     50.473814\n",
      "Name: PTAU.MEDIAN.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for PTAU.MEDIAN.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature PTAU.MEDIAN.bl\n",
      "displaying first 10 values of feature TAU.MEDIAN.bl (n_feature: 15/16)\n",
      "0    132.571916\n",
      "1     33.787719\n",
      "2    110.049924\n",
      "3    138.690457\n",
      "4    -61.573234\n",
      "5    109.317598\n",
      "6     -0.372409\n",
      "7    114.830917\n",
      "8    138.734948\n",
      "9     57.337968\n",
      "Name: TAU.MEDIAN.bl, dtype: float64\n",
      "number of differents samples: 1000 / total of samples: 1000\n",
      "specify data type for TAU.MEDIAN.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "2\n",
      "QuantitativeDataType.CONTINUOUS float64\n",
      "Allow TAU.MEDIAN.bl to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "displaying first 10 values of feature AGE (n_feature: 16/16)\n",
      "0    75.0\n",
      "1    67.0\n",
      "2    63.0\n",
      "3    75.0\n",
      "4    65.0\n",
      "5    66.0\n",
      "6    69.0\n",
      "7    68.0\n",
      "8    76.0\n",
      "9    73.0\n",
      "Name: AGE, dtype: float64\n",
      "number of differents samples: 44 / total of samples: 1000\n",
      "specify data type for AGE:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "CategoricalDataType.NUMERICAL float64\n",
      "Allow AGE to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_format_file = get_from_user_multi_view_dataset_fromat_file(single_view_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec5f1",
   "metadata": {},
   "source": [
    "data_fromat_ref (read only)\n",
    "\n",
    "CLI editer data_format_file\n",
    "\n",
    "review : \n",
    "- specify lower / upper bound NUMERICAL\n",
    "- Specify categorical (BOOLEAN, CHARACTER, NUMERICAL)\n",
    "\n",
    "- save different categorical values \n",
    " a posteriori ex SEX -> male or female, NOT FEMALE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65a22aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: test7/file1 +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature a (n_feature: 1/18)\n",
      "0    48\n",
      "1    87\n",
      "2    46\n",
      "3    84\n",
      "4    94\n",
      "5    18\n",
      "6    15\n",
      "7    30\n",
      "8    54\n",
      "9    46\n",
      "Name: a, dtype: int64\n",
      "number of differents samples: 57 / total of samples: 100\n",
      "specify data type for a:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "2\n",
      "QuantitativeDataType.DISCRETE int64\n",
      "Allow a to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "displaying first 10 values of feature e (n_feature: 2/18)\n",
      "0    98\n",
      "1    83\n",
      "2    73\n",
      "3    45\n",
      "4    84\n",
      "5     5\n",
      "6    44\n",
      "7    55\n",
      "8    37\n",
      "9     8\n",
      "Name: e, dtype: int64\n",
      "number of differents samples: 65 / total of samples: 100\n",
      "specify data type for e:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature e\n",
      "displaying first 10 values of feature i (n_feature: 3/18)\n",
      "0    65\n",
      "1    13\n",
      "2    81\n",
      "3    81\n",
      "4     0\n",
      "5    57\n",
      "6    14\n",
      "7    98\n",
      "8    13\n",
      "9    89\n",
      "Name: i, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for i:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature i\n",
      "displaying first 10 values of feature o (n_feature: 4/18)\n",
      "0     5\n",
      "1    70\n",
      "2    96\n",
      "3    39\n",
      "4    15\n",
      "5    28\n",
      "6    29\n",
      "7    82\n",
      "8    19\n",
      "9    21\n",
      "Name: o, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for o:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature o\n",
      "displaying first 10 values of feature 0 (n_feature: 5/18)\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "7     True\n",
      "8     True\n",
      "9    False\n",
      "Name: 0, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature 0\n",
      "displaying first 10 values of feature 1 (n_feature: 6/18)\n",
      "0     True\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6    False\n",
      "7     True\n",
      "8     True\n",
      "9     True\n",
      "Name: 1, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature 1\n",
      "displaying first 10 values of feature 2 (n_feature: 7/18)\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "7    False\n",
      "8    False\n",
      "9     True\n",
      "Name: 2, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 2:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature 2\n",
      "displaying first 10 values of feature 3 (n_feature: 8/18)\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4    False\n",
      "5     True\n",
      "6     True\n",
      "7    False\n",
      "8    False\n",
      "9     True\n",
      "Name: 3, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 3:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature 3\n",
      "displaying first 10 values of feature time (n_feature: 9/18)\n",
      "0    2018-01-01 00:00:00\n",
      "1    2018-01-01 01:00:00\n",
      "2    2018-01-01 02:00:00\n",
      "3    2018-01-01 03:00:00\n",
      "4    2018-01-01 04:00:00\n",
      "5    2018-01-01 05:00:00\n",
      "6    2018-01-01 06:00:00\n",
      "7    2018-01-01 07:00:00\n",
      "8    2018-01-01 08:00:00\n",
      "9    2018-01-01 09:00:00\n",
      "Name: time, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for time:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "4\n",
      "displaying first 10 values of feature pressure (n_feature: 10/18)\n",
      "0    0.088082\n",
      "1    0.774788\n",
      "2    0.514092\n",
      "3    0.832881\n",
      "4    0.696152\n",
      "5    0.166896\n",
      "6    0.740141\n",
      "7    0.067837\n",
      "8    0.342615\n",
      "9    0.777026\n",
      "Name: pressure, dtype: float64\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pressure:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature pressure\n",
      "displaying first 10 values of feature sp02 (n_feature: 11/18)\n",
      "0    0.360430\n",
      "1    0.072426\n",
      "2    0.538551\n",
      "3    0.761962\n",
      "4    0.389385\n",
      "5    0.200984\n",
      "6    0.765184\n",
      "7    0.258049\n",
      "8    0.736926\n",
      "9    0.253232\n",
      "Name: sp02, dtype: float64\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for sp02:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature sp02\n",
      "displaying first 10 values of feature a.1 (n_feature: 12/18)\n",
      "0    90\n",
      "1    15\n",
      "2    30\n",
      "3    42\n",
      "4    65\n",
      "5    21\n",
      "6    60\n",
      "7     0\n",
      "8    33\n",
      "9    64\n",
      "Name: a.1, dtype: int64\n",
      "number of differents samples: 62 / total of samples: 100\n",
      "specify data type for a.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature a.1\n",
      "displaying first 10 values of feature e.1 (n_feature: 13/18)\n",
      "0    63\n",
      "1    20\n",
      "2     2\n",
      "3    70\n",
      "4    90\n",
      "5    94\n",
      "6    33\n",
      "7    94\n",
      "8    71\n",
      "9    65\n",
      "Name: e.1, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for e.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature e.1\n",
      "displaying first 10 values of feature i.1 (n_feature: 14/18)\n",
      "0    60\n",
      "1    45\n",
      "2     6\n",
      "3     7\n",
      "4    12\n",
      "5    77\n",
      "6    75\n",
      "7    50\n",
      "8    94\n",
      "9    15\n",
      "Name: i.1, dtype: int64\n",
      "number of differents samples: 67 / total of samples: 100\n",
      "specify data type for i.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature i.1\n",
      "displaying first 10 values of feature o.1 (n_feature: 15/18)\n",
      "0     8\n",
      "1    10\n",
      "2    22\n",
      "3    79\n",
      "4    90\n",
      "5     2\n",
      "6    85\n",
      "7    64\n",
      "8    47\n",
      "9    30\n",
      "Name: o.1, dtype: int64\n",
      "number of differents samples: 69 / total of samples: 100\n",
      "specify data type for o.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature o.1\n",
      "displaying first 10 values of feature gender (n_feature: 16/18)\n",
      "0      MAN\n",
      "1      MAN\n",
      "2    WOMAN\n",
      "3    WOMAN\n",
      "4      MAN\n",
      "5      MAN\n",
      "6    WOMAN\n",
      "7    WOMAN\n",
      "8    WOMAN\n",
      "9      MAN\n",
      "Name: gender, dtype: object\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for gender:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature gender\n",
      "displaying first 10 values of feature blood type (n_feature: 17/18)\n",
      "0      A\n",
      "1      O\n",
      "2      A\n",
      "3     AB\n",
      "4      B\n",
      "5     AB\n",
      "6      O\n",
      "7      B\n",
      "8    NaN\n",
      "9      A\n",
      "Name: blood type, dtype: object\n",
      "number of differents samples: 5 / total of samples: 100\n",
      "specify data type for blood type:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "Ignoring feature blood type\n",
      "displaying first 10 values of feature pkey (n_feature: 18/18)\n",
      "0    zmixzrgvxrjqxoe sluk\n",
      "1    vrzahnpfluspdcbfnaqt\n",
      "2    pnrepvmrxqabdlvisclv\n",
      "3    gwj luzejwdxzsiljxzd\n",
      "4    jjdvcnofivbqhirxzdyo\n",
      "5    e fshtkhnlimpczypnoe\n",
      "6    qe nlikallf znokwdhk\n",
      "7    kgenxxkftoeqtrnoteaq\n",
      "8    abghippigyxzaxtejumu\n",
      "9    ezfasuuycdda foisjte\n",
      "Name: pkey, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pkey:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "1\n",
      "KeyDataType.CHARACTER object\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: test7/contatct +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature discrete (n_feature: 1/3)\n",
      "0    64.0\n",
      "1    26.0\n",
      "2    61.0\n",
      "3    29.0\n",
      "4    99.0\n",
      "5     5.0\n",
      "6    71.0\n",
      "7    99.0\n",
      "8    90.0\n",
      "9    36.0\n",
      "Name: discrete, dtype: float64\n",
      "number of differents samples: 70 / total of samples: 100\n",
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "error ! 6 value not understood\n",
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "error ! 6 value not understood\n",
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "6\n",
      "error ! 6 value not understood\n",
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error ! 5 value not understood\n",
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "CategoricalDataType.NUMERICAL float64\n",
      "Allow discrete to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "displaying first 10 values of feature city (n_feature: 2/3)\n",
      "0        Lille\n",
      "1        Lille\n",
      "2        Paris\n",
      "3        Paris\n",
      "4        Lille\n",
      "5        Lille\n",
      "6        Paris\n",
      "7        Paris\n",
      "8    Marseille\n",
      "9        Lille\n",
      "Name: city, dtype: object\n",
      "number of differents samples: 3 / total of samples: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7956/2921833459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_view_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7956/2412665332.py\u001b[0m in \u001b[0;36mget_from_user_multi_view_dataset_fromat_file\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"+++++++ Now parsing view: {tabular_data_file} +++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_dataframe_format_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtabular_data_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabular_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7956/2260902348.py\u001b[0m in \u001b[0;36mget_from_user_dataframe_format_file\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# ask user about data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         data_format_id = get_user_input(msg_data_type_selection,\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                        n_answers=dataset_columns_length+1)\n",
      "\u001b[0;32m/tmp/ipykernel_7956/2260902348.py\u001b[0m in \u001b[0;36mget_user_input\u001b[0;34m(msg, n_answers)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_column_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#data_format_id = input(f'specify data type for {feature}:\\n' + msg )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_answers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# check if value passed by user is correct (if it is integer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "data_format_file = get_from_user_multi_view_dataset_fromat_file(multi_view_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e57d180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(data_format_file['pseudo_adni_mod.csv'].keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76cb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data format file\n",
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "with open(json_file_name, \"w\") as format_file:\n",
    "    json.dump(data_format_file, format_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]], path: str):\n",
    "    # save `format_file_ref` into a JSON file\n",
    "    with open(path, \"w\") as format_file:\n",
    "        json.dump(format_file_ref, format_file)\n",
    "    print(f\"Model successfully saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_format_file_ref(path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    # retrieve data format file\n",
    "    with open(path, \"r\") as format_file:\n",
    "        format_file_ref = json.load(format_file)\n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36e3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "\n",
    "format_file = load_format_file_ref(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82331c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f18285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca66943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(data_format: str,\n",
    "                  action: str,\n",
    "                  available_categorical_data_type: List[Enum],\n",
    "                  msg:str) -> Tuple[Dict[str, Any], bool]:\n",
    "    \n",
    "    \n",
    "    is_cancelled = False\n",
    "    if data_format == DataType.CATEGORICAL.name:\n",
    "        if action == '1':\n",
    "            new_field = ask_for_data_type(available_categorical_data_type,\n",
    "                                          msg)\n",
    "        elif action =='2':\n",
    "            new_field = ask_for_categorical_values()\n",
    "        elif action == '3':\n",
    "            print('operation cancelled')\n",
    "            new_field = None\n",
    "            is_cancelled = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_field = None\n",
    "    else:\n",
    "        # for QUANTITATIVE, DATETIME and KEY data types\n",
    "        \n",
    "        if action == '1':\n",
    "            new_field =  ask_for_lower_bound()\n",
    "        elif action == '2':\n",
    "            new_field = ask_for_upper_bound()\n",
    "        elif action == '3':\n",
    "            new_field = None\n",
    "            print('operation cancelled')\n",
    "            is_cancelled = True\n",
    "        else:\n",
    "            new_field = None\n",
    "    \n",
    "    return new_field, is_cancelled\n",
    "\n",
    "\n",
    "def isfloat(value:str) ->bool:\n",
    "    \"\"\"checks if string represents a float or int\"\"\"\n",
    "    is_float = True\n",
    "    try:\n",
    "        float(value)\n",
    "    except ValueError as e:\n",
    "        is_float = False\n",
    "    return is_float\n",
    "    \n",
    "        \n",
    "def ask_for_lower_bound() -> Dict[str, float]:\n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        lower_bound = input('enter lower bound')\n",
    "        if isfloat(lower_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "    return {'lower_bound': float(lower_bound)}\n",
    "\n",
    "def ask_for_upper_bound() -> Dict[str, float]:\n",
    "    \n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        upper_bound = input('enter upper bound')\n",
    "        if isfloat(upper_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "    return {'upper_bound': float(upper_bound)}\n",
    "\n",
    "def ask_for_data_type(\n",
    "                      available_categorical_data_type: List[Enum], \n",
    "                      msg: str) -> Dict[str, Any]:\n",
    "    # edit categorical datatype\n",
    "    data_type_selection = get_user_input(msg, 4)\n",
    "    updates = None\n",
    "    if data_type_selection != '4':\n",
    "        new_data_type = available_categorical_data_type[int(data_type_selection) - 1]\n",
    "        new_values = list(map(lambda x: str(x), new_data_type.value))\n",
    "        updates = {'data_type': new_data_type.name, 'values': new_values}\n",
    "    return updates\n",
    "\n",
    "\n",
    "def ask_for_categorical_values() -> Dict[str, Any]:\n",
    "    possible_values = input('enter possible values (separated by \",\")')\n",
    "    possible_values = possible_values.split(\",\")\n",
    "    return {'categorical_values': possible_values}\n",
    "\n",
    "\n",
    "\n",
    "def edit_feature_format_file_ref(feature_content: Dict[str, Any],\n",
    "                                  feature_name: str,\n",
    "                                  available_categorical_data_type: List[Enum],\n",
    "                                  messages: Dict[str, str],\n",
    "                                  ignore_keystroke: int) -> Dict[str, Any]:\n",
    "    \"\"\"Edits a specific feature that belongs to a specific view within a format file\"\"\"\n",
    "    \n",
    "\n",
    "    _is_feature_unparsed = True  \n",
    "    _is_cancelled = False  # whether parsing of current column has been cancelled or not\n",
    "    _is_first_edit = True\n",
    "    \n",
    "    # iterate over number of feature contained in view, and ask for each feature if changes are needed\n",
    "    while _is_feature_unparsed:\n",
    "        if _is_cancelled or not _is_first_edit:\n",
    "            _f_answer = True\n",
    "        else:\n",
    "            _f_answer = get_user_input(f\"Edit variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "            # ask if user wants to edit feature names\n",
    "            _f_answer = parse_yes_no_msg(_f_answer)\n",
    "            _is_operation_cancelled = False  # for cancelling feature edition\n",
    "            _is_first_edit = False\n",
    "        if _f_answer:\n",
    "            # case where user wants to edit the current feature\n",
    "            \n",
    "            _msg = messages['edit']\n",
    "            if feature_content.get('data_format') == DataType.CATEGORICAL.name:\n",
    "                # case if feature is a categorical variable\n",
    "                 _msg += messages['categorical_edit']\n",
    "            else:\n",
    "                #case if feature is a quantitative variable\n",
    "                _msg += messages['quantitative_edit']\n",
    "                \n",
    "            _msg += messages['ignore']\n",
    "            _edit_selection = get_user_input(_msg, 3)\n",
    "            \n",
    "            _edited_field, _is_cancelled = select_action(feature_content.get('data_format'),\n",
    "                                                          _edit_selection,\n",
    "                                                          available_categorical_data_type,\n",
    "                                                          messages['data_type_select'])\n",
    "            \n",
    "            if not _is_cancelled:\n",
    "                # if user has not cancelled field edition\n",
    "                if _edited_field is not None:\n",
    "                    feature_content.update(_edited_field)\n",
    "             \n",
    "                _c_answer = get_user_input(f\"Continue Editing variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "                _is_feature_unparsed = parse_yes_no_msg(_c_answer)\n",
    "            else:\n",
    "                _is_feature_unparsed = False\n",
    "                \n",
    "        else:\n",
    "            _is_feature_unparsed = False\n",
    "            \n",
    "    return feature_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed0d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \n",
    "    # CLI for editing `format_file_ref`, a file containing information about each variable\n",
    "    # in a tabular dataset\n",
    "    print(f'Now editing format file ref')\n",
    "    \n",
    "    ## variables initialization\n",
    "    available_categorical_data_types = [t for t in CategoricalDataType]\n",
    "    _file_names = list(format_file_ref.keys())\n",
    "    _n_tot_files = len(_file_names)\n",
    "    \n",
    "    ## messages definition\n",
    "    _data_type_selection_msg, ign_key = get_data_type_selection_msg(available_categorical_data_types)\n",
    "    \n",
    "    _messages = {\n",
    "        'yes_or_no': get_yes_no_msg(),\n",
    "        'data_type_select':  _data_type_selection_msg,\n",
    "        'edit': 'Which field should be modified?\\n',\n",
    "        'quantitative_edit': '1)lower_bound\\n2)upper_bound\\n',\n",
    "        'categorical_edit': '1)data_type (categorical variable only)\\n2)values taken (categorical variable only)\\n',\n",
    "        'ignore': '3)cancel operation\\n'\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # iterate over name of files (ie views)\n",
    "    for i_file in range(_n_tot_files):\n",
    "        # ask for each file if user wants to edt it\n",
    "        _answer = get_user_input(f\"Edit file: {_file_names[i_file]}?\\n\" + _messages['yes_or_no'], 2)\n",
    "        _answer = parse_yes_no_msg(_answer)\n",
    "        \n",
    "        if _answer:\n",
    "            # case where user wants to modify current view scheme\n",
    "            _file_content = format_file_ref[_file_names[i_file]]  # get file (ie view) content\n",
    "            \n",
    "            ## variables initialization for parsing current view\n",
    "            _features_names = list(_file_content.keys())\n",
    "            _n_tot_feature = len(_features_names)\n",
    "            \n",
    "            # iterate over features found in view\n",
    "            for i_feature in range(_n_tot_feature):\n",
    "                feature_name = _features_names[i_feature]\n",
    "                feature_content = _file_content[feature_name]\n",
    "                feature_content = edit_feature_format_file_ref(feature_content,\n",
    "                                                               feature_name,\n",
    "                                                               available_categorical_data_types,\n",
    "                                                               _messages,\n",
    "                                                               ign_key)\n",
    "            format_file_ref[_file_names[i_file]].update({feature_name: feature_content})\n",
    "            \n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95895e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now editing format file ref\n",
      "Edit file: pseudo_adni_mod.csv?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Edit variable: CDRSB.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: ADAS11.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: MMSE.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: RAVLT.immediate.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: RAVLT.learning.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: FAQ.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: TAU.MEDIAN.bl?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: AGE?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_format_file_ref(format_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ba372",
   "metadata": {},
   "source": [
    "## tabular data sanity check using file format ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2a9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "\n",
    "def join_muti_view_dataset(multi_view_dataset: pd.DataFrame, primary_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    _views_name = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "    \n",
    "    joined_dataframe = multi_view_dataset[_views_name[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_name)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_name[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_name[x]))\n",
    "        \n",
    "        #df['file1'].join(df['file2'].set_index('pkey'), on='pkey', rsuffix='.file2')\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"Fin\"\"\"\n",
    "    primary_key = None\n",
    "    for view_name in views_names:\n",
    "        file_content = format_file[view_name]\n",
    "        d_format = file_content.get('data_format')\n",
    "\n",
    "        if d_format == DataType.KEY.name:\n",
    "            if primary_key is None:\n",
    "                primary_key = view_name\n",
    "                print(f'found primary key {view_name}')\n",
    "            else:\n",
    "                print('error: found 2 primary keys')\n",
    "    return primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68eedfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rename_variables_before_joining() missing 1 required positional argument: 'views_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6768/3821142122.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tabular_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpre_parsed_dataset_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename_variables_before_joining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpre_parsed_dataset_to_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: rename_variables_before_joining() missing 1 required positional argument: 'views_name'"
     ]
    }
   ],
   "source": [
    "dataset_to_check = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd657d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary key None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>views</th>\n",
       "      <th colspan=\"16\" halign=\"left\">pseudo_adni_mod.csv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_name</th>\n",
       "      <th>CDRSB.bl</th>\n",
       "      <th>ADAS11.bl</th>\n",
       "      <th>MMSE.bl</th>\n",
       "      <th>RAVLT.immediate.bl</th>\n",
       "      <th>RAVLT.learning.bl</th>\n",
       "      <th>RAVLT.forgetting.bl</th>\n",
       "      <th>FAQ.bl</th>\n",
       "      <th>WholeBrain.bl</th>\n",
       "      <th>Ventricles.bl</th>\n",
       "      <th>Hippocampus.bl</th>\n",
       "      <th>MidTemp.bl</th>\n",
       "      <th>Entorhinal.bl</th>\n",
       "      <th>ABETA.MEDIAN.bl</th>\n",
       "      <th>PTAU.MEDIAN.bl</th>\n",
       "      <th>TAU.MEDIAN.bl</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.739439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.821573</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.684331</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>154.016065</td>\n",
       "      <td>67.970509</td>\n",
       "      <td>132.571916</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>64.933800</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.001653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735892</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>211.573206</td>\n",
       "      <td>5.451168</td>\n",
       "      <td>33.787719</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.987722</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.876316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738731</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>163.637668</td>\n",
       "      <td>66.704378</td>\n",
       "      <td>110.049924</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>50.314425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.733481</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.032797</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.012312</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>182.256297</td>\n",
       "      <td>47.091893</td>\n",
       "      <td>138.690457</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.217830</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.225401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841806</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>247.997479</td>\n",
       "      <td>-5.997140</td>\n",
       "      <td>-61.573234</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>61.896022</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.663102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767153</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.012879</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>231.706787</td>\n",
       "      <td>24.632786</td>\n",
       "      <td>87.065806</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.083170</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.241477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.695168</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.012534</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>146.949187</td>\n",
       "      <td>57.588115</td>\n",
       "      <td>121.985248</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.289059</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.437600</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.628691</td>\n",
       "      <td>0.041537</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>181.805672</td>\n",
       "      <td>55.052669</td>\n",
       "      <td>157.229102</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.650504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.669603</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.714763</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>178.824412</td>\n",
       "      <td>69.412821</td>\n",
       "      <td>103.238647</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.089863</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.703384</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.691858</td>\n",
       "      <td>0.030349</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>180.781989</td>\n",
       "      <td>32.978001</td>\n",
       "      <td>54.780563</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "views        pseudo_adni_mod.csv                                       \\\n",
       "feature_name            CDRSB.bl ADAS11.bl MMSE.bl RAVLT.immediate.bl   \n",
       "0                            1.0       8.0    27.0          23.739439   \n",
       "1                            0.0       0.0    30.0          64.933800   \n",
       "2                            0.0       8.0    24.0          36.987722   \n",
       "3                            0.0       3.0    29.0          50.314425   \n",
       "4                            0.0       0.0    30.0          57.217830   \n",
       "..                           ...       ...     ...                ...   \n",
       "995                          1.0       2.0    29.0          61.896022   \n",
       "996                          0.0       1.0    29.0          62.083170   \n",
       "997                          3.0      14.0    24.0          22.289059   \n",
       "998                          0.0      13.0    26.0          31.650504   \n",
       "999                          0.0      15.0    28.0          29.089863   \n",
       "\n",
       "views                                                                    \\\n",
       "feature_name RAVLT.learning.bl RAVLT.forgetting.bl FAQ.bl WholeBrain.bl   \n",
       "0                          4.0            5.821573    3.0      0.684331   \n",
       "1                          9.0            4.001653    0.0      0.735892   \n",
       "2                          3.0            6.876316    0.0      0.738731   \n",
       "3                          5.0            4.733481    3.0      0.696179   \n",
       "4                          9.0            7.225401    0.0      0.841806   \n",
       "..                         ...                 ...    ...           ...   \n",
       "995                        8.0            1.663102    0.0      0.767153   \n",
       "996                        8.0            5.241477    1.0      0.695168   \n",
       "997                        2.0            5.437600    7.0      0.628691   \n",
       "998                        2.0            1.669603    4.0      0.714763   \n",
       "999                        3.0            7.703384    4.0      0.691858   \n",
       "\n",
       "views                                                               \\\n",
       "feature_name Ventricles.bl Hippocampus.bl MidTemp.bl Entorhinal.bl   \n",
       "0                 0.012699       0.003786   0.012678      0.002214   \n",
       "1                 0.012803       0.004866   0.015071      0.003041   \n",
       "2                 0.030492       0.004300   0.012419      0.002316   \n",
       "3                 0.032797       0.004720   0.012312      0.002593   \n",
       "4                 0.004030       0.006820   0.016948      0.002896   \n",
       "..                     ...            ...        ...           ...   \n",
       "995               0.011417       0.005209   0.012879      0.002208   \n",
       "996               0.011908       0.004641   0.012534      0.002197   \n",
       "997               0.041537       0.003478   0.010870      0.001939   \n",
       "998               0.020461       0.004713   0.013989      0.001981   \n",
       "999               0.030349       0.004237   0.011439      0.002419   \n",
       "\n",
       "views                                                            \n",
       "feature_name ABETA.MEDIAN.bl PTAU.MEDIAN.bl TAU.MEDIAN.bl   AGE  \n",
       "0                 154.016065      67.970509    132.571916  75.0  \n",
       "1                 211.573206       5.451168     33.787719  67.0  \n",
       "2                 163.637668      66.704378    110.049924  63.0  \n",
       "3                 182.256297      47.091893    138.690457  75.0  \n",
       "4                 247.997479      -5.997140    -61.573234  65.0  \n",
       "..                       ...            ...           ...   ...  \n",
       "995               231.706787      24.632786     87.065806  76.0  \n",
       "996               146.949187      57.588115    121.985248  77.0  \n",
       "997               181.805672      55.052669    157.229102  74.0  \n",
       "998               178.824412      69.412821    103.238647  64.0  \n",
       "999               180.781989      32.978001     54.780563  65.0  \n",
       "\n",
       "[1000 rows x 16 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# extract views names\n",
    "views_names = list(format_file.keys())\n",
    "views_names\n",
    "\n",
    "\n",
    "# llok for primary key\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(dataset_to_check, views_names)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "if primary_key is not None:\n",
    "    multi_df_to_check = join_muti_view_dataset(multi_df_to_check)\n",
    "    \n",
    "multi_df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15a12eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary key None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ff5d5",
   "metadata": {},
   "source": [
    "## data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca423d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "887f090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) YES\\n2) NO\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_yes_or_no_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2460bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85714c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d161776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "ll\n",
      "please add new feature name:\n",
      "ll\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "4\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "kk\n",
      "please add new feature name:\n",
      "vkeof\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "is_views_finished = False\n",
    "\n",
    "\n",
    "views_format_file = {}\n",
    "\n",
    "while not is_views_finished:\n",
    "    is_features_finished = False\n",
    "    resp = input('do you want to add a new view (file)?\\n' + msg_yes_or_no_question)\n",
    "    resp = yes_or_no_question_key.get(resp)\n",
    "    if not resp:\n",
    "        is_views_finished = True\n",
    "        print('process done')\n",
    "        continue\n",
    "    new_view = input('please add new view name:\\n')\n",
    "    while not is_features_finished:\n",
    "        feature_format_file = {}\n",
    "        new_feature = input('please add new feature name:\\n')\n",
    "        feature_format_file[new_feature] = {}\n",
    "        is_column_parsed = False\n",
    "        try:\n",
    "            while not is_column_parsed:\n",
    "                data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "                if data_format_id.isdigit() and int(data_format_id) <= n_available_data_type+1:\n",
    "                    # check if value passed by user is correct (if it is integer,\n",
    "                    # and whithin range [1, n_available_data_type])\n",
    "                    is_column_parsed = True\n",
    "                \n",
    "                else:\n",
    "                    print(f'error ! {data_format_id} value not understood')\n",
    "                    \n",
    "        except KeyboardInterrupt as e:\n",
    "            print('stopping now' + str(e))\n",
    "        resp = input('do you want to add a new variable (feature) ?' + msg_yes_or_no_question)\n",
    "        resp = yes_or_no_question_key.get(resp)\n",
    "        if not resp:\n",
    "            is_features_finished = True\n",
    "            print('process done')\n",
    "            continue\n",
    "    views_format_file[new_view] = feature_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee0e0cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'datetime64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9387/2903691535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'datetime64'"
     ]
    }
   ],
   "source": [
    "type(np.datetime64(\"2018-01-01\"))\n",
    "import datetime\n",
    "type(datetime.datetime(2018, 1, 1))\n",
    "\n",
    "pd.datetime64[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24be197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = type(pd.to_datetime('13000101', format='%Y%m%d', errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52c37736",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Series(pd.date_range(\"1/1/2011\", freq=\"H\", periods=3)).dtype\n",
    "\n",
    "t =type(t).type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b13acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(t == t1 for t1 in [pd.Timestamp, pd.Timedelta, pd.Period, datetime.datetime,np.datetime64] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb165f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9881289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDRSB.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'ADAS11.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'MMSE.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.immediate.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.learning.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.forgetting.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'FAQ.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'WholeBrain.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Ventricles.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Hippocampus.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'MidTemp.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Entorhinal.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'ABETA.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'PTAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'TAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'AGE': {'data_type': <QuantitativeDataType.DISCRETE: [<class 'int'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb7ad297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.dtype[float64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(dataset[feature].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dbaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_add_numeric_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_arith_method',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_binop',\n",
       " '_cacher',\n",
       " '_can_hold_na',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_from_arguments',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_convert',\n",
       " '_convert_dtypes',\n",
       " '_data',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_duplicated',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_from_mgr',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_index_resolvers',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_get_values_tuple',\n",
       " '_get_with',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_index',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_init_dict',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_cached',\n",
       " '_is_copy',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_item_cache',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_map_values',\n",
       " '_maybe_update_cacher',\n",
       " '_memory_usage',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_name',\n",
       " '_needs_reindex_multi',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_indexer',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_replace_single',\n",
       " '_repr_data_resource_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_labels',\n",
       " '_set_name',\n",
       " '_set_value',\n",
       " '_set_values',\n",
       " '_set_with',\n",
       " '_set_with_engine',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'array',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'autocorr',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'clip',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'factorize',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'hasnans',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'interpolate',\n",
       " 'is_monotonic',\n",
       " 'is_monotonic_decreasing',\n",
       " 'is_monotonic_increasing',\n",
       " 'is_unique',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'item',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memory_usage',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'ravel',\n",
       " 'rdiv',\n",
       " 'rdivmod',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'repeat',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'searchsorted',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_frame',\n",
       " 'to_hdf',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_list',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_sql',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unique',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae52ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
