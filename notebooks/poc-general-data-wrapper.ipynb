{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee99bdb4",
   "metadata": {},
   "source": [
    "Related to user story: [SP11-Item04: General Data Wrapper PoC](https://gitlab.inria.fr/fedbiomed/fedbiomed/-/issues/164)\n",
    "\n",
    "## Tabular dataset\n",
    "\n",
    "Workflow of data pre processing:\n",
    "\n",
    "1. Columns name should be shared with the researcher\n",
    "2. Data format file to be filled by clinicians.\n",
    "3. Specify if missing data are allowed for a given columns (Exception). The file will be used for data verification during FL pre-processing,\n",
    "4. Outlier verification for quantitative data, continuous and discrete, and for dates (Critical warning),\n",
    "5. Missing data imputation by local mean (or optional NN), or majority voting for discrete labels. Give warnings when missing data are found (for verification a posteriori).\n",
    "6. Give critical warning when too many missing are found (>50%),\n",
    "7. Verify that number of available data is greater then minimum required (Error)\n",
    "\n",
    "Critical warnings have different levels of disclosure to the researcher (1) only the warning, 2) type of warning, 3) type of warning and column affected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b334c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-2.4.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ae4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. load  a single view dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional, Callable\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17059269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class ExcelSignatures(Enum):\n",
    "    XLSX = (b'\\x50\\x4B\\x05\\x06', 2, -22, 4)\n",
    "    LSX1 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 512, 8)\n",
    "    LSX2 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 1536, 8)\n",
    "    LSX3 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 2048, 8)\n",
    "    \n",
    "    def __init__(self, sig, whence, offset, size):\n",
    "        self._sig = sig\n",
    "        self._whence = whence\n",
    "        self._offset = offset\n",
    "        self._size = size\n",
    "\n",
    "    @property \n",
    "    def signature(self) -> bytes:\n",
    "        return self._sig\n",
    "    \n",
    "    @property\n",
    "    def whence(self) -> int:\n",
    "        return self._whence\n",
    "    \n",
    "    @property\n",
    "    def offset(self) -> int:\n",
    "        return self._offset\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self._size\n",
    "\n",
    "\n",
    "\n",
    "def load_tabular_datasets(path:str) -> Dict[str, pd.DataFrame]:\n",
    "    tabular_datasets = {}\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        print('directory found')\n",
    "        _is_folder = True\n",
    "        \n",
    "        _tabular_data_files = os.listdir(path)\n",
    "    else:\n",
    "        print('file found')\n",
    "        _is_folder = False\n",
    "        _tabular_data_files = (path,)\n",
    "        \n",
    "    for tabular_data_file in _tabular_data_files:\n",
    "        if _is_folder:\n",
    "            tabular_data_file = os.path.join(path, tabular_data_file)\n",
    "        \n",
    "        _is_excel = excel_sniffer(tabular_data_file)\n",
    "        _csv_delimiter, _csv_header = csv_sniffer(tabular_data_file)\n",
    "        _view_name = os.path.basename(tabular_data_file)\n",
    "        if _is_excel:\n",
    "            tabular_datasets[_view_name] = load_excel_file(tabular_data_file)\n",
    "        elif _csv_delimiter is not None:\n",
    "            tabular_datasets[_view_name] = load_csv_file(tabular_data_file,\n",
    "                                                               _csv_delimiter, \n",
    "                                                               _csv_header)\n",
    "        else:\n",
    "            print(f'warning: cannot parse {tabular_data_file}: not a tabular data file')\n",
    "        \n",
    "    return tabular_datasets\n",
    "\n",
    "def load_csv_file(path:str, delimiter:str, header:int) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataframe = pd.read_csv(path, delimiter=delimiter, header=header)\n",
    "    except csv.Error as err:\n",
    "        print('err', err, 'in file', path)\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "#https://stackoverflow.com/questions/23515791/how-to-check-the-uploaded-file-is-csv-or-xls-in-python/23515973\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_excel_file(path:str, sheet_name: Union[str, int]=0) -> pd.DataFrame:\n",
    "    \"\"\"May rely on openpyxl package\"\"\"\n",
    "    #with open(path, 'r') as excl:\n",
    "    #    _c = csv.DictReader(excl, dialect=csv.excel_tab)\n",
    "    #    _delimiter = _c.dialect.delimiter\n",
    "    \n",
    "    dataframe = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def excel_sniffer(path: str) -> bool:\n",
    "    \n",
    "    for excel_sig in ExcelSignatures:\n",
    "        with open(path, 'rb') as f:\n",
    "            f.seek(excel_sig.offset, excel_sig.whence)\n",
    "            bytes = f.read(excel_sig.size)\n",
    "\n",
    "            if bytes == excel_sig.signature:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "\n",
    "def csv_sniffer(path:str) :\n",
    "        \n",
    "    with open(path, 'r') as csvfile:\n",
    "        try:\n",
    "            # do some operation on file using sniffer to make sure considered file\n",
    "            # is a CSV file\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline())\n",
    "            delimiter = dialect.delimiter\n",
    "            dialect.lineterminator\n",
    "            has_header = csv.Sniffer().has_header(csvfile.readline())\n",
    "            if has_header:\n",
    "                header = 0\n",
    "            else:\n",
    "                header = None\n",
    "        except (csv.Error, UnicodeDecodeError) as err:\n",
    "            delimiter, header = None, None\n",
    "            print('err', err, 'in file', path)\n",
    "    return delimiter, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "750c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (3.0.9)\r\n",
      "Requirement already satisfied: et-xmlfile in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from openpyxl) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n",
      "err 'utf-8' codec can't decode byte 0x8c in position 15: invalid start byte in file ../../Exceltest.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Exceltest.xlsx':    ID   Age Eligibility\n",
       " 0    1   45           Y\n",
       " 1    2   45           Y\n",
       " 2    3   33           N\n",
       " 3    4   54           Y\n",
       " 4    5   45           Y\n",
       " 5    6   54         NaN\n",
       " 6    7   34           N\n",
       " 7    8   54         NaN\n",
       " 8    9   45         NaN\n",
       " 9   10   44           Y}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tabular_datasets('../../Exceltest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "single_view_dataset = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed673dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1':      a   e   i   o      0      1      2      3                 time  pressure  \\\n",
       " 0   48  98  65   5  False   True  False  False  2018-01-01 00:00:00  0.088082   \n",
       " 1   87  83  13  70   True  False   True  False  2018-01-01 01:00:00  0.774788   \n",
       " 2   46  73  81  96  False  False  False   True  2018-01-01 02:00:00  0.514092   \n",
       " 3   84  45  81  39  False   True   True   True  2018-01-01 03:00:00  0.832881   \n",
       " 4   94  84   0  15  False   True  False  False  2018-01-01 04:00:00  0.696152   \n",
       " ..  ..  ..  ..  ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95  14  66  25  64   True   True   True  False  2018-01-04 23:00:00  0.295578   \n",
       " 96  91  81  48  53  False  False   True   True  2018-01-05 00:00:00  0.474322   \n",
       " 97  15  82  12  51   True   True   True   True  2018-01-05 01:00:00  0.927511   \n",
       " 98  51  18   4  52  False  False   True   True  2018-01-05 02:00:00  0.494798   \n",
       " 99  51  70  63  77  False  False   True  False  2018-01-05 03:00:00  0.316395   \n",
       " \n",
       "         sp02  a.1  e.1  i.1  o.1 gender blood type                  pkey  \n",
       " 0   0.360430   90   63   60    8    MAN          A  zmixzrgvxrjqxoe sluk  \n",
       " 1   0.072426   15   20   45   10    MAN          O  vrzahnpfluspdcbfnaqt  \n",
       " 2   0.538551   30    2    6   22  WOMAN          A  pnrepvmrxqabdlvisclv  \n",
       " 3   0.761962   42   70    7   79  WOMAN         AB  gwj luzejwdxzsiljxzd  \n",
       " 4   0.389385   65   90   12   90    MAN          B  jjdvcnofivbqhirxzdyo  \n",
       " ..       ...  ...  ...  ...  ...    ...        ...                   ...  \n",
       " 95  0.027930   13   41   93   19  WOMAN          A  hrvepmqjn llgbzplshv  \n",
       " 96  0.545006   91   41   99   71  WOMAN          B  wroevwyuamxibzshlxxh  \n",
       " 97  0.059205   38    7   73   47    MAN          B  ywadcykylymkdtzfctpg  \n",
       " 98  0.125428   38   11   97    3    MAN          O  ruchbfa zwgenxslegrl  \n",
       " 99  0.772293   49   74   36   42  WOMAN         AB  zeapltpxuvuibfybxcll  \n",
       " \n",
       " [100 rows x 18 columns],\n",
       " 'contatct':     discrete       city                  pkey\n",
       " 0       64.0      Lille  qpqorfhylu gmfjy bdj\n",
       " 1       26.0      Lille  kkmjozalfyirgsire ui\n",
       " 2       61.0      Paris  ezfasuuycdda foisjte\n",
       " 3       29.0      Paris  faxiqkt xggzmwzoidbg\n",
       " 4       99.0      Lille  znwhlj rwzdutnagwasy\n",
       " ..       ...        ...                   ...\n",
       " 95       9.0      Paris  zeqhcikzdodus jn qjf\n",
       " 96      98.0  Marseille  iicthcvfmkajbvr gzir\n",
       " 97      21.0      Lille  ztjakcsk bhjoksdz lm\n",
       " 98      42.0  Marseille   sabunaa opt vpulnxj\n",
       " 99       3.0      Paris  qmbexyexvgromrm admu\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'file2':         0      1      2      3                 time        pH  \\\n",
       " 0   False   True  False   True  2018-01-01 00:00:00  0.023107   \n",
       " 1    True  False  False  False  2018-01-01 01:00:00       NaN   \n",
       " 2   False   True  False   True  2018-01-01 02:00:00  0.407279   \n",
       " 3    True   True   True   True  2018-01-01 03:00:00  0.536301   \n",
       " 4   False   True   True   True  2018-01-01 04:00:00  0.749443   \n",
       " ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95   True   True   True  False  2018-01-04 23:00:00       NaN   \n",
       " 96   True  False  False  False  2018-01-05 00:00:00  0.388389   \n",
       " 97  False   True   True  False  2018-01-05 01:00:00  0.889067   \n",
       " 98   True   True  False  False  2018-01-05 02:00:00  0.402979   \n",
       " 99   True  False   True  False  2018-01-05 03:00:00  0.667349   \n",
       " \n",
       "                     pkey  \n",
       " 0   kkmjozalfyirgsire ui  \n",
       " 1   xkdawggpnuulcewuoyzz  \n",
       " 2   khuulhwgwnjggrfoefce  \n",
       " 3   xxysdmwwmjsmyhaswfdb  \n",
       " 4   ldejfuij mnbnf wwmms  \n",
       " ..                   ...  \n",
       " 95  wrmdecb s pohtmrcdj   \n",
       " 96  whmwrpvqmerdpwwzxasf  \n",
       " 97  pnrepvmrxqabdlvisclv  \n",
       " 98  iicthcvfmkajbvr gzir  \n",
       " 99  kiejdmbuih awhuifwwd  \n",
       " \n",
       " [100 rows x 7 columns]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_view_dataframe =  load_tabular_datasets('test7')\n",
    "multi_view_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d533e99",
   "metadata": {},
   "source": [
    "Data format file to be filled by clinicians (step 2 int he workflow):\n",
    "\n",
    "Data format file will be a dictionary specifying the type: \n",
    "* for single view datasets:\n",
    "```{<feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}```\n",
    " * for multiview datatset\n",
    "```{{<view_name>: <feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}}```\n",
    "\n",
    "where\n",
    "* `<view_name>` is the name of the view\n",
    "* `<feature_name>` is the name of the feature\n",
    "* `<data_type>` can be categorical or continuous or missing_data or datetime\n",
    "* `<value_taken>` is the type of the value (eg int, char, float, signed, unsigned ...)\n",
    "* `<value_range>` represent either a list of bounds, an upper or a lower bound, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "134b8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create data format file\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import enum\n",
    "import datetime\n",
    "\n",
    "# the use of Enum classes will prevent incorrect combination of values\n",
    "class QuantitativeDataType(Enum):\n",
    "    CONTINUOUS = [float, np.float64]\n",
    "    DISCRETE = [int, np.int64]\n",
    "\n",
    "class CategoricalDataType(Enum):\n",
    "    BOOLEAN = [bool]\n",
    "    NUMERICAL = [float, int, np.float64, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "class KeyDataType(Enum):\n",
    "    NUMERICAL = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "\n",
    "\n",
    "class CustomDataType(Enum):\n",
    "    \"\"\"for demo purpose: here a custom datatype\"\"\"\n",
    "    DISCRETE = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "    \n",
    "class DataType(Enum):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # what about \n",
    "    KEY = KeyDataType\n",
    "    QUANTITATIVE = QuantitativeDataType\n",
    "    CATEGORICAL = CategoricalDataType\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "    CUSTOM = CustomDataType  # custom data type (should be defined by user)\n",
    "    UNKNOWN = 'UNKNOWN'\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in DataType.__members__.items())\n",
    "\n",
    "class MissingValueAllowedDefault(Enum):\n",
    "    KEY = False\n",
    "    QUANTITATIVE = True\n",
    "    CATEGORICAL = True\n",
    "    DATETIME = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in MissingValueAllowedDefault.__members__.items())\n",
    "    \n",
    "    \n",
    "class DataTypeProperties(Enum):\n",
    "    \"\"\"Data Type possible modification (whithin CLI editing)\"\"\"\n",
    "    CATEGORICAL = (False, False, True, False, True)\n",
    "    QUANTITATIVE = (True, True, False, False, True)\n",
    "    DATETIME = (True, True, False, True, False)\n",
    "    UNKNOWN = (False, False, False, False, True)\n",
    "    CUSTOM = (True, True, True, False, True)\n",
    "    KEY = (True, True, False, True, False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 lower_bound: bool,\n",
    "                 upper_bound: bool,\n",
    "                 set_of_values: bool,\n",
    "                 date_format:bool,\n",
    "                 allow_missing_values: bool):\n",
    "        self._lower_bound = lower_bound\n",
    "        self._upper_bound = upper_bound\n",
    "        self._set_of_values = set_of_values\n",
    "        self._date_format = date_format\n",
    "        self._allow_missing_values = allow_missing_values\n",
    "    \n",
    "    @property\n",
    "    def lower_bound(self):\n",
    "        return self._lower_bound\n",
    "    \n",
    "    @property\n",
    "    def upper_bound(self):\n",
    "        return self._upper_bound\n",
    "    \n",
    "    @property\n",
    "    def set_of_values(self):\n",
    "        return self._set_of_values\n",
    "    \n",
    "    @property\n",
    "    def date_format(self):\n",
    "        return self._date_format\n",
    "    \n",
    "    @property\n",
    "    def allow_missing_values(self):\n",
    "        return self._allow_missing_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d35b2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.impute_missing_values_mode(data: Union[pandas.core.series.Series, pandas.core.frame.DataFrame])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImputationMethods.MODE_IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7e67bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(next(iter(DataType.DATETIME.value)), Enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type(\n",
    "                  #avail_data_types: enum.EnumMeta,\n",
    "                  d_format: Enum,\n",
    "                  d_type: type) ->  Tuple[Enum, List[Union[type, str]]]:\n",
    "    # varibales initialisation\n",
    "    present_d_types = []\n",
    "    sub_d_type_format = d_format\n",
    "    \n",
    "    \n",
    "    for avail_data_type in DataType:\n",
    "        if d_format is avail_data_type:\n",
    "            sub_dtypes = avail_data_type.value\n",
    "            #if not isinstance(sub_dtypes, str) and hasattr(sub_dtypes, '__getitem__') and isinstance(sub_dtypes[0], Enum):\n",
    "            if not isinstance(sub_dtypes, str):\n",
    "                # check if dtype has subtypes\n",
    "                #(eg if datatype is QUANTITATIVE, subtype will be CONTINOUS or DISCRETE)\n",
    "                if isinstance(next(iter(sub_dtypes)), Enum):\n",
    "                    \n",
    "                    for sub_dtype in sub_dtypes:\n",
    "                        if any(d_type == t for t in tuple(sub_dtype.value)):\n",
    "                            present_d_types.append(d_type)\n",
    "                            sub_d_type_format = sub_dtype\n",
    "                            print(sub_dtype, d_type)\n",
    "                else:\n",
    "                    # case where datatype doesnot have subtypes, eg DATETIME\n",
    "                    if any(d_type == t for t in sub_dtypes):\n",
    "                        present_d_types.append(d_type)\n",
    "                    sub_d_type_format = d_format\n",
    "            else:\n",
    "                # case where d_format is a string of character\n",
    "                sub_d_type_format = d_format\n",
    "    print(sub_d_type_format, '|', present_d_types)\n",
    "    return  sub_d_type_format, present_d_types\n",
    "\n",
    "\n",
    "def find_data_type(data_format_name: str, data_type_name: str=None) -> List[Enum]:\n",
    "    \"\"\"Retrieves from a given data_format and data_type,\n",
    "    the corresponding Enum class describing data\"\"\"\n",
    "    \n",
    "    ## varible initialisation\n",
    "    data_type = None\n",
    "    _is_data_format_unrecognized = True\n",
    "    _is_data_type_unrecognized = True\n",
    "    \n",
    "    _available_data_types = [t for t in DataType]\n",
    "    \n",
    "    for a_data_type in _available_data_types:\n",
    "        if data_format_name == a_data_type.name:\n",
    "            _is_data_format_unrecognized = False\n",
    "            data_type = a_data_type\n",
    "            \n",
    "            for sub_type in a_data_type.value:\n",
    "                \n",
    "                if data_type_name is not None and  isinstance(sub_type, Enum):\n",
    "                    # check if sub data type exist (it shouldnot if variable is UNKNOWN)\n",
    "                    if data_type_name == sub_type.name:\n",
    "                        \n",
    "                        _is_data_type_unrecognized = False\n",
    "                        data_type = sub_type\n",
    "                \n",
    "                    \n",
    "                else:\n",
    "                    _is_data_type_unrecognized = False\n",
    "    # check for data formt file consistancy error\n",
    "    if any((_is_data_format_unrecognized, _is_data_type_unrecognized)):\n",
    "        if _is_data_format_unrecognized:\n",
    "            raise ValueError(f'error: {data_format_name} not recognized as a valid data type')\n",
    "        else:\n",
    "            raise ValueError(f'error {data_type_name} not recognized as a valid data type')\n",
    "            \n",
    "    return data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00bd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691fe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "df = pd.DataFrame({'w': [1, 2, 3, 4,  'jj', None]})\n",
    "print(check_missing_data(df['w']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f354b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936ac8f0",
   "metadata": {},
   "source": [
    "CLI details:\n",
    "\n",
    "1. open  csv file\n",
    "2. for each columns in file ask type of variable or if variable should be excluded\n",
    "3. automatically detect the type given values in columns \n",
    "4. ask for each columns if missing data are allowed\n",
    "\n",
    "\n",
    "eg :\n",
    "\n",
    "assume a column is of type discrete with integers\n",
    "\n",
    "\n",
    "1. user select it is quantitative\n",
    "2. then system will label it as quantitative-discrete\n",
    "\n",
    "\n",
    "**Question** : do we want an auto selection parameter choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d328b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_msg() -> str:\n",
    "    msg_yes_or_no_question = '1) YES\\n2) NO\\n'   \n",
    "    return msg_yes_or_no_question\n",
    "\n",
    "def parse_yes_no_msg(resp: str) -> bool:\n",
    "    \"\"\"implements logic to parse yes or no msg\"\"\"\n",
    "    yes_or_no_question_key = {'1': True,\n",
    "                    '2': False}\n",
    "    return yes_or_no_question_key.get(resp)\n",
    "\n",
    "def get_data_type_selection_msg(available_data_type:List[Enum],\n",
    "                               ign_msg: str = 'ignore this column') ->Tuple[str, int]:\n",
    "    \n",
    "    \n",
    "    n_available_data_type = len(available_data_type)\n",
    "    msg = ''\n",
    "\n",
    "    \n",
    "    for i, dtype in enumerate(available_data_type):\n",
    "        msg += '%d) %s \\n' %  (i+1, dtype.name)\n",
    "    \n",
    "    ignoring_key = i+2  # add ingoring entry\n",
    "    msg += f'%d) %s\\n' % (ignoring_key, ign_msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return msg, ignoring_key\n",
    "\n",
    "def unique(iterable: Iterator, number: bool = False) -> int:\n",
    "    \"\"\"returns number of unique values\"\"\"\n",
    "    set_of_values = set(iterable)\n",
    "    if number:\n",
    "        return len(set_of_values)\n",
    "    else:\n",
    "        return set_of_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2442268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# imputation methods\n",
    "\n",
    "def impute_missing_values_mean(data):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            for col in data.columns:\n",
    "                if (data[col].isnull().sum()>0):\n",
    "                    if any(data[col].dtype in x2 for x2 in  [x.value for x in QuantitativeDataType]):\n",
    "                        data[col].fillna(value=data[col].mean(),inplace=True)\n",
    "        else:\n",
    "            data = data.fillna(data.mean())\n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in loading data file')\n",
    "    \n",
    " #Categorical Data\n",
    "def impute_missing_values_mode(data: Union[pd.Series, pd.DataFrame]):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            for col in data.columns:\n",
    "                if (data[col].isnull().sum()>0):\n",
    "                    categorical_data_type = [x.value for x in CategoricalDataType]\n",
    "                    data_type_condition = any(data[col].dtype in x2 for x2 in categorical_data_type)\n",
    "                    if data_type_condition:\n",
    "                        print(col)\n",
    "                        data[col].fillna(value=data[col].mode()[0],inplace=True)\n",
    "                        \n",
    "        else:\n",
    "            data = data.fillna(data.value_counts().index[0])\n",
    "            \n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - mode')\n",
    "        \n",
    "    # Impute missing values with KNN            \n",
    "\n",
    "def impute_missing_values_knn(data,k=2):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            missing_cols = data.columns[data.isnull().any()]\n",
    "            if len(missing_cols)>0:\n",
    "                    imputer =KNNImputer(n_neighbors=k)\n",
    "                    data = pd.DataFrame(imputer.fit_transform(data),columns=data.columns) \n",
    "        else:\n",
    "            imputer =KNNImputer(n_neighbors=k)\n",
    "            data =pd.DataFrame( (imputer.fit_transform(np.array(data).reshape(1,-1))).reshape(-1,1),columns=[data.name])                    \n",
    "\n",
    "        return data  \n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - knn')\n",
    "\n",
    "#Impute missing values with Interpolate\n",
    "def impute_missing_values_interpolate(data):\n",
    "    try:\n",
    "        data_filled = data.interpolate()\n",
    "            \n",
    "        return data_filled\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - interpolate')\n",
    "        \n",
    "\n",
    "class ImputationMethods(Enum):\n",
    "    MEAN_IMPUTATION = partial(impute_missing_values_mean)\n",
    "    MODE_IMPUTATION = partial(impute_missing_values_mode)\n",
    "    KNN_IMPUTATION = partial(impute_missing_values_knn)\n",
    "    INTERPOLATION_IMPUTATION = partial(impute_missing_values_interpolate)\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        \"\"\"method avoiding to specify `value` when using an enum class\"\"\"\n",
    "        self.value(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a158a595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = get_data_imputation_methods_msg()\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "800e755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI for clinicians for setting up data format file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96b6238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_imputation_methods_msg() -> Tuple[str, Dict[str, str]]:\n",
    "    msg = 'Please select the following method for filling missing values (if some are found)\\n'\n",
    "    \n",
    "    #available_methods = [method for ethod in ImputationMethods]\n",
    "    select_action = {}\n",
    "    for i, method in enumerate(ImputationMethods):\n",
    "        \n",
    "        msg += '%d) %s\\n' % (i+1, method.name)\n",
    "        select_action[str(i)] = method.name\n",
    "\n",
    "    # ignore key\n",
    "    msg += '%d) No method\\n' % (i + 2)\n",
    "    select_action[str(i+1)] = None\n",
    "    return msg, select_action\n",
    "    \n",
    "def no_methods(*kwargs):\n",
    "    return None\n",
    "\n",
    "def set_imputation_method(user_action: str, available_methods: Dict[str, Any]):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def get_from_user_dataframe_format_file(dataset: pd.DataFrame) -> Dict[str, Any]:\n",
    "    ##\n",
    "    # variable initialisation\n",
    "    data_format_file = {}\n",
    "    \n",
    "    dataset_columns = dataset.columns\n",
    "    dataset_columns_length = len(dataset_columns)\n",
    "    \n",
    "    \n",
    "    available_data_type = [d_type for d_type in DataType]  # get all available data types\n",
    "    \n",
    "    for n_feature, feature in enumerate(dataset_columns):\n",
    "        print(f'displaying first 10 values of feature {feature} (n_feature: {n_feature+1}/{dataset_columns_length})')\n",
    "        #print(tabulate(dataset[feature].head(10).values()))\n",
    "        pprint.pprint(dataset[feature].head(10))  # print first 10 lines of feature value\n",
    "        print(f'number of differents samples: {unique(dataset[feature], number=True)} / total of samples: {dataset[feature].shape[0]}')\n",
    "        \n",
    "        msg_data_type_selection, ignoring_id = get_data_type_selection_msg(available_data_type)\n",
    "        msg_data_type_selection = f'specify data type for {feature}:\\n' + msg_data_type_selection\n",
    "        \n",
    "        # ask user about data type\n",
    "        data_format_id = get_user_input(msg_data_type_selection,\n",
    "                                       \n",
    "                                       n_answers=ignoring_id)\n",
    "        \n",
    "        if int(data_format_id) > ignoring_id - 1:\n",
    "            # case where user decide to ingore column: go to next iteration (next feature)\n",
    "            print(f\"Ignoring feature {feature}\")\n",
    "            continue\n",
    "        else:\n",
    "            # case where user selected a data type: add data type and info to the format file\n",
    "            data_format = available_data_type[int(data_format_id)-1]\n",
    "            data_type = dataset[feature].dtype\n",
    "            n_data_type, types = get_data_type(data_format, data_type)\n",
    "            \n",
    "        # KEY and DATETIME type \n",
    "        if data_format is DataType.KEY or data_format is DataType.DATETIME:  \n",
    "            # for these data type, missing values are disabled by default\n",
    "            is_missing_values_allowed = False\n",
    "        else: \n",
    "            # ask user if missing values are allowed for this specific variable\n",
    "            ## message definition\n",
    "            msg_yes_or_no_question = get_yes_no_msg()\n",
    "            msg_data_imputation_methods, data_imputation_methods = get_data_imputation_methods_msg()\n",
    "            n_data_imputation_method = len(data_imputation_methods)\n",
    "            msg_yes_or_no_question = f'Allow {feature} to have missing values:\\n' + msg_yes_or_no_question\n",
    "            \n",
    "            missing_values_user_selection = get_user_input(msg_yes_or_no_question,\n",
    "                                                        n_answers=2)\n",
    "            is_missing_values_allowed = parse_yes_no_msg(missing_values_user_selection)\n",
    "            \n",
    "            if is_missing_values_allowed:\n",
    "                # let user select amputation method if missing data are allowed\n",
    "                amputation_meth_user_selection = get_user_input(msg_data_imputation_methods,\n",
    "                                                                n_answers=n_data_imputation_method)\n",
    "                \n",
    "                amputation_meth_selected = data_imputation_methods.get(amputation_meth_user_selection)\n",
    "            else:\n",
    "                # no amputation is required (because missing values are not required)\n",
    "                amputation_meth_selected = None\n",
    "                \n",
    "        data_format_file[feature] = {'data_format': data_format.name,\n",
    "                                     'data_type': n_data_type.name,\n",
    "                                     'values': str(data_type),\n",
    "                                     'is_missing_values': is_missing_values_allowed,\n",
    "                                     'data_amputation_method': amputation_meth_selected\n",
    "                                    }\n",
    "            \n",
    "    return data_format_file\n",
    "            \n",
    "def get_user_input(msg:str,  n_answers:int) -> str:\n",
    "    \"\"\"\"\"\"\n",
    "    is_column_parsed = False\n",
    "    while not is_column_parsed:\n",
    "        #data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "        resp = input(msg)\n",
    "        if resp.isdigit() and int(resp) <= n_answers and int(resp)>0:\n",
    "            # check if value passed by user is correct (if it is integer,\n",
    "            # and whithin range [1, n_available_data_type])\n",
    "            is_column_parsed = True\n",
    "\n",
    "        else:\n",
    "            print(f'error ! {resp} value not understood')\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "185958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLI to use when dataset is available\n",
    "\n",
    "\n",
    "def get_from_user_multi_view_dataset_fromat_file(datasets: Dict[str, pd.DataFrame])-> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "    data_format_files = {}\n",
    "    \n",
    "    for tabular_data_file in datasets.keys():\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(f\"+++++++ Now parsing view: {tabular_data_file} +++++++\")\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        data_format_file = get_from_user_dataframe_format_file(datasets[tabular_data_file])\n",
    "        if data_format_file:\n",
    "            # (above condition avoids adding empty views)\n",
    "            _file_name = os.path.basename(tabular_data_file)\n",
    "            data_format_files[_file_name] = data_format_file\n",
    "        \n",
    "    return data_format_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8acbe629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_view_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7316/552618136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_view_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'single_view_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data_format_file = get_from_user_multi_view_dataset_fromat_file(single_view_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec5f1",
   "metadata": {},
   "source": [
    "data_fromat_ref (read only)\n",
    "\n",
    "CLI editer data_format_file\n",
    "\n",
    "review : \n",
    "- specify lower / upper bound NUMERICAL\n",
    "- Specify categorical (BOOLEAN, CHARACTER, NUMERICAL)\n",
    "\n",
    "- save different categorical values \n",
    " a posteriori ex SEX -> male or female, NOT FEMALE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b874ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: file1 +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature a (n_feature: 1/18)\n",
      "0    48\n",
      "1    87\n",
      "2    46\n",
      "3    84\n",
      "4    94\n",
      "5    18\n",
      "6    15\n",
      "7    30\n",
      "8    54\n",
      "9    46\n",
      "Name: a, dtype: int64\n",
      "number of differents samples: 57 / total of samples: 100\n",
      "specify data type for a:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature a\n",
      "displaying first 10 values of feature e (n_feature: 2/18)\n",
      "0    98\n",
      "1    83\n",
      "2    73\n",
      "3    45\n",
      "4    84\n",
      "5     5\n",
      "6    44\n",
      "7    55\n",
      "8    37\n",
      "9     8\n",
      "Name: e, dtype: int64\n",
      "number of differents samples: 65 / total of samples: 100\n",
      "specify data type for e:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature e\n",
      "displaying first 10 values of feature i (n_feature: 3/18)\n",
      "0    65\n",
      "1    13\n",
      "2    81\n",
      "3    81\n",
      "4     0\n",
      "5    57\n",
      "6    14\n",
      "7    98\n",
      "8    13\n",
      "9    89\n",
      "Name: i, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for i:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature i\n",
      "displaying first 10 values of feature o (n_feature: 4/18)\n",
      "0     5\n",
      "1    70\n",
      "2    96\n",
      "3    39\n",
      "4    15\n",
      "5    28\n",
      "6    29\n",
      "7    82\n",
      "8    19\n",
      "9    21\n",
      "Name: o, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for o:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature o\n",
      "displaying first 10 values of feature 0 (n_feature: 5/18)\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "7     True\n",
      "8     True\n",
      "9    False\n",
      "Name: 0, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 0\n",
      "displaying first 10 values of feature 1 (n_feature: 6/18)\n",
      "0     True\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6    False\n",
      "7     True\n",
      "8     True\n",
      "9     True\n",
      "Name: 1, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "3\n",
      "CategoricalDataType.BOOLEAN bool\n",
      "CategoricalDataType.BOOLEAN | [dtype('bool')]\n",
      "Allow 1 to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Please select the following method for filling missing values (if some are found)\n",
      "1) MEAN_IMPUTATION\n",
      "2) MODE_IMPUTATION\n",
      "3) KNN_IMPUTATION\n",
      "4) INTERPOLATION_IMPUTATION\n",
      "5) No method\n",
      "1\n",
      "displaying first 10 values of feature 2 (n_feature: 7/18)\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "7    False\n",
      "8    False\n",
      "9     True\n",
      "Name: 2, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 2:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 2\n",
      "displaying first 10 values of feature 3 (n_feature: 8/18)\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4    False\n",
      "5     True\n",
      "6     True\n",
      "7    False\n",
      "8    False\n",
      "9     True\n",
      "Name: 3, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 3:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 3\n",
      "displaying first 10 values of feature time (n_feature: 9/18)\n",
      "0    2018-01-01 00:00:00\n",
      "1    2018-01-01 01:00:00\n",
      "2    2018-01-01 02:00:00\n",
      "3    2018-01-01 03:00:00\n",
      "4    2018-01-01 04:00:00\n",
      "5    2018-01-01 05:00:00\n",
      "6    2018-01-01 06:00:00\n",
      "7    2018-01-01 07:00:00\n",
      "8    2018-01-01 08:00:00\n",
      "9    2018-01-01 09:00:00\n",
      "Name: time, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for time:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "4\n",
      "DataType.DATETIME | [dtype('O')]\n",
      "displaying first 10 values of feature pressure (n_feature: 10/18)\n",
      "0    0.088082\n",
      "1    0.774788\n",
      "2    0.514092\n",
      "3    0.832881\n",
      "4    0.696152\n",
      "5    0.166896\n",
      "6    0.740141\n",
      "7    0.067837\n",
      "8    0.342615\n",
      "9    0.777026\n",
      "Name: pressure, dtype: float64\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pressure:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature pressure\n",
      "displaying first 10 values of feature sp02 (n_feature: 11/18)\n",
      "0    0.360430\n",
      "1    0.072426\n",
      "2    0.538551\n",
      "3    0.761962\n",
      "4    0.389385\n",
      "5    0.200984\n",
      "6    0.765184\n",
      "7    0.258049\n",
      "8    0.736926\n",
      "9    0.253232\n",
      "Name: sp02, dtype: float64\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for sp02:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature sp02\n",
      "displaying first 10 values of feature a.1 (n_feature: 12/18)\n",
      "0    90\n",
      "1    15\n",
      "2    30\n",
      "3    42\n",
      "4    65\n",
      "5    21\n",
      "6    60\n",
      "7     0\n",
      "8    33\n",
      "9    64\n",
      "Name: a.1, dtype: int64\n",
      "number of differents samples: 62 / total of samples: 100\n",
      "specify data type for a.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature a.1\n",
      "displaying first 10 values of feature e.1 (n_feature: 13/18)\n",
      "0    63\n",
      "1    20\n",
      "2     2\n",
      "3    70\n",
      "4    90\n",
      "5    94\n",
      "6    33\n",
      "7    94\n",
      "8    71\n",
      "9    65\n",
      "Name: e.1, dtype: int64\n",
      "number of differents samples: 61 / total of samples: 100\n",
      "specify data type for e.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature e.1\n",
      "displaying first 10 values of feature i.1 (n_feature: 14/18)\n",
      "0    60\n",
      "1    45\n",
      "2     6\n",
      "3     7\n",
      "4    12\n",
      "5    77\n",
      "6    75\n",
      "7    50\n",
      "8    94\n",
      "9    15\n",
      "Name: i.1, dtype: int64\n",
      "number of differents samples: 67 / total of samples: 100\n",
      "specify data type for i.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature i.1\n",
      "displaying first 10 values of feature o.1 (n_feature: 15/18)\n",
      "0     8\n",
      "1    10\n",
      "2    22\n",
      "3    79\n",
      "4    90\n",
      "5     2\n",
      "6    85\n",
      "7    64\n",
      "8    47\n",
      "9    30\n",
      "Name: o.1, dtype: int64\n",
      "number of differents samples: 69 / total of samples: 100\n",
      "specify data type for o.1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature o.1\n",
      "displaying first 10 values of feature gender (n_feature: 16/18)\n",
      "0      MAN\n",
      "1      MAN\n",
      "2    WOMAN\n",
      "3    WOMAN\n",
      "4      MAN\n",
      "5      MAN\n",
      "6    WOMAN\n",
      "7    WOMAN\n",
      "8    WOMAN\n",
      "9      MAN\n",
      "Name: gender, dtype: object\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for gender:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "73\n",
      "error ! 73 value not understood\n",
      "specify data type for gender:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "3\n",
      "CategoricalDataType.CHARACTER object\n",
      "CategoricalDataType.CHARACTER | [dtype('O')]\n",
      "Allow gender to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Please select the following method for filling missing values (if some are found)\n",
      "1) MEAN_IMPUTATION\n",
      "2) MODE_IMPUTATION\n",
      "3) KNN_IMPUTATION\n",
      "4) INTERPOLATION_IMPUTATION\n",
      "5) No method\n",
      "2\n",
      "displaying first 10 values of feature blood type (n_feature: 17/18)\n",
      "0      A\n",
      "1      O\n",
      "2      A\n",
      "3     AB\n",
      "4      B\n",
      "5     AB\n",
      "6      O\n",
      "7      B\n",
      "8    NaN\n",
      "9      A\n",
      "Name: blood type, dtype: object\n",
      "number of differents samples: 5 / total of samples: 100\n",
      "specify data type for blood type:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature blood type\n",
      "displaying first 10 values of feature pkey (n_feature: 18/18)\n",
      "0    zmixzrgvxrjqxoe sluk\n",
      "1    vrzahnpfluspdcbfnaqt\n",
      "2    pnrepvmrxqabdlvisclv\n",
      "3    gwj luzejwdxzsiljxzd\n",
      "4    jjdvcnofivbqhirxzdyo\n",
      "5    e fshtkhnlimpczypnoe\n",
      "6    qe nlikallf znokwdhk\n",
      "7    kgenxxkftoeqtrnoteaq\n",
      "8    abghippigyxzaxtejumu\n",
      "9    ezfasuuycdda foisjte\n",
      "Name: pkey, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pkey:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "1\n",
      "KeyDataType.CHARACTER object\n",
      "KeyDataType.DATETIME object\n",
      "KeyDataType.DATETIME | [dtype('O'), dtype('O')]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: contatct +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature discrete (n_feature: 1/3)\n",
      "0    64.0\n",
      "1    26.0\n",
      "2    61.0\n",
      "3    29.0\n",
      "4    99.0\n",
      "5     5.0\n",
      "6    71.0\n",
      "7    99.0\n",
      "8    90.0\n",
      "9    36.0\n",
      "Name: discrete, dtype: float64\n",
      "number of differents samples: 70 / total of samples: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specify data type for discrete:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature discrete\n",
      "displaying first 10 values of feature city (n_feature: 2/3)\n",
      "0        Lille\n",
      "1        Lille\n",
      "2        Paris\n",
      "3        Paris\n",
      "4        Lille\n",
      "5        Lille\n",
      "6        Paris\n",
      "7        Paris\n",
      "8    Marseille\n",
      "9        Lille\n",
      "Name: city, dtype: object\n",
      "number of differents samples: 3 / total of samples: 100\n",
      "specify data type for city:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature city\n",
      "displaying first 10 values of feature pkey (n_feature: 3/3)\n",
      "0    qpqorfhylu gmfjy bdj\n",
      "1    kkmjozalfyirgsire ui\n",
      "2    ezfasuuycdda foisjte\n",
      "3    faxiqkt xggzmwzoidbg\n",
      "4    znwhlj rwzdutnagwasy\n",
      "5    ki vn mtcclzwskgaewy\n",
      "6    exrmjfolirbtkgjswcin\n",
      "7    jsgohdb eieenvdwjopr\n",
      "8    kzrybjgjxm rde xqmra\n",
      "9    ruchbfa zwgenxslegrl\n",
      "Name: pkey, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pkey:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature pkey\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: file2 +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature 0 (n_feature: 1/7)\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "5     True\n",
      "6    False\n",
      "7    False\n",
      "8    False\n",
      "9     True\n",
      "Name: 0, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 0\n",
      "displaying first 10 values of feature 1 (n_feature: 2/7)\n",
      "0     True\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4     True\n",
      "5    False\n",
      "6     True\n",
      "7    False\n",
      "8     True\n",
      "9     True\n",
      "Name: 1, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 1:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 1\n",
      "displaying first 10 values of feature 2 (n_feature: 3/7)\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6    False\n",
      "7    False\n",
      "8     True\n",
      "9    False\n",
      "Name: 2, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 2:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 2\n",
      "displaying first 10 values of feature 3 (n_feature: 4/7)\n",
      "0     True\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6     True\n",
      "7    False\n",
      "8     True\n",
      "9     True\n",
      "Name: 3, dtype: bool\n",
      "number of differents samples: 2 / total of samples: 100\n",
      "specify data type for 3:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature 3\n",
      "displaying first 10 values of feature time (n_feature: 5/7)\n",
      "0    2018-01-01 00:00:00\n",
      "1    2018-01-01 01:00:00\n",
      "2    2018-01-01 02:00:00\n",
      "3    2018-01-01 03:00:00\n",
      "4    2018-01-01 04:00:00\n",
      "5    2018-01-01 05:00:00\n",
      "6    2018-01-01 06:00:00\n",
      "7    2018-01-01 07:00:00\n",
      "8    2018-01-01 08:00:00\n",
      "9    2018-01-01 09:00:00\n",
      "Name: time, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for time:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature time\n",
      "displaying first 10 values of feature pH (n_feature: 6/7)\n",
      "0    0.023107\n",
      "1         NaN\n",
      "2    0.407279\n",
      "3    0.536301\n",
      "4    0.749443\n",
      "5    0.203130\n",
      "6    0.524939\n",
      "7    0.518098\n",
      "8    0.760008\n",
      "9    0.998239\n",
      "Name: pH, dtype: float64\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pH:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature pH\n",
      "displaying first 10 values of feature pkey (n_feature: 7/7)\n",
      "0    kkmjozalfyirgsire ui\n",
      "1    xkdawggpnuulcewuoyzz\n",
      "2    khuulhwgwnjggrfoefce\n",
      "3    xxysdmwwmjsmyhaswfdb\n",
      "4    ldejfuij mnbnf wwmms\n",
      "5    pvhkafscqfwzofgqziko\n",
      "6    rgggl wzpfkfbftmtjoo\n",
      "7    stjrqcvljprtvralmnil\n",
      "8    kms wptwzta nzdbkncc\n",
      "9    kzrybjgjxm rde xqmra\n",
      "Name: pkey, dtype: object\n",
      "number of differents samples: 100 / total of samples: 100\n",
      "specify data type for pkey:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "7\n",
      "Ignoring feature pkey\n"
     ]
    }
   ],
   "source": [
    "multi_data_format_file = get_from_user_multi_view_dataset_fromat_file(multi_view_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d2102a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True,\n",
       "   'data_amputation_method': 'MODE_IMPUTATION'},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False,\n",
       "   'data_amputation_method': 'MODE_IMPUTATION'},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True,\n",
       "   'data_amputation_method': 'KNN_IMPUTATION'},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False,\n",
       "   'data_amputation_method': 'KNN_IMPUTATION'}},\n",
       " 'contatct': {},\n",
       " 'file2': {}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af6f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e57d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved at multi_format_file\n"
     ]
    }
   ],
   "source": [
    "save_format_file_ref(multi_data_format_file, 'multi_format_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76cb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data format file\n",
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "with open(json_file_name, \"w\") as format_file:\n",
    "    json.dump(data_format_file, format_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]], path: str):\n",
    "    # save `format_file_ref` into a JSON file\n",
    "    with open(path, \"w\") as format_file:\n",
    "        json.dump(format_file_ref, format_file)\n",
    "    print(f\"Model successfully saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_format_file_ref(path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    # retrieve data format file\n",
    "    with open(path, \"r\") as format_file:\n",
    "        format_file_ref = json.load(format_file)\n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36e3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "\n",
    "format_file = load_format_file_ref(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82331c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31f18285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1)data_type\\n2) Values taken\\n3) Cancel Operation\\n', 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_msg_action_selection(DataTypeProperties.CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85f24f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_format': 'DATETIME',\n",
       " 'values': [\"<class 'pandas._libs.tslibs.timestamps.Timestamp'>\",\n",
       "  \"<class 'pandas._libs.tslibs.timedeltas.Timedelta'>\",\n",
       "  \"<class 'pandas._libs.tslibs.period.Period'>\",\n",
       "  \"<class 'datetime.datetime'>\",\n",
       "  \"<class 'numpy.datetime64'>\"]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for_data_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603615e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fca66943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from dateutil.parser._parser import ParserError\n",
    "\n",
    "\n",
    "def create_msg_action_selection(data_type_propreties: Enum) -> Tuple[str, int, Dict[int, Callable]]:\n",
    "    # create edit selection message for user given data type\n",
    "    # number of possible action depend of data type properties\n",
    "    msg = \"\"\n",
    "    action_counter = 1\n",
    "    actions = {}\n",
    "    \n",
    "    # data type change command\n",
    "    msg += \"%d) data_type\\n\" % action_counter\n",
    "    actions[str(action_counter)] = ask_for_data_type\n",
    "    action_counter += 1\n",
    "    \n",
    "    if data_type_propreties.lower_bound:\n",
    "        # lower bound edit command\n",
    "        msg += \"%d) lower bound\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_lower_bound\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.upper_bound:\n",
    "        # upper bound edit command\n",
    "        msg += \"%d)upper bound\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_upper_bound\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.set_of_values:\n",
    "        # value taken edit command\n",
    "        msg += \"%d) Values taken\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_categorical_values\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.date_format:\n",
    "        # date formatter edit command\n",
    "        msg += \"%d) Date format\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_date_format\n",
    "        action_counter += 1\n",
    "        \n",
    "    msg += \"%d) Cancel Operation\\n\" % action_counter\n",
    "    actions[str(action_counter)] = cancel_operation\n",
    "    return msg, action_counter, actions\n",
    "\n",
    "def select_action(\n",
    "                  action: str,\n",
    "                  possible_actions: Dict[str, Callable],\n",
    "                  ) -> Tuple[Dict[str, Any], bool]:\n",
    "    \n",
    "    # variable initialization\n",
    "    is_cancelled = False\n",
    "    new_field = None\n",
    "    _action_counter = 1  # integer for dynamic cli management\n",
    "    print('action', action, str(len(possible_actions.keys())),possible_actions)\n",
    "    if action == str(len(possible_actions.keys())):\n",
    "        is_cancelled = True\n",
    "        \n",
    "    else:\n",
    "        # define action among the pool of possible actions\n",
    "        _action_func = possible_actions[action]\n",
    "        new_field = _action_func()\n",
    "    \n",
    "    return new_field, is_cancelled\n",
    "\n",
    "\n",
    "def isfloat(value:str) ->bool:\n",
    "    \"\"\"checks if string represents a float or int\"\"\"\n",
    "    is_float = True\n",
    "    try:\n",
    "        float(value)\n",
    "    except ValueError as e:\n",
    "        is_float = False\n",
    "    return is_float\n",
    "\n",
    "\n",
    "\n",
    "def is_datetime(date: str) -> bool:\n",
    "    \"\"\"checks if date is a date\"\"\"\n",
    "    is_date_parsable = True\n",
    "    try:\n",
    "        dateutil.parser.parse(date)\n",
    "    except (ParserError, ValueError) as err:\n",
    "        is_date_parsable = False\n",
    "        \n",
    "    return is_date_parsable\n",
    "\n",
    "def cancel_operation():\n",
    "    print(\"operation cancelled\")\n",
    "        \n",
    "def ask_for_lower_bound() -> Dict[str, float]:\n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        lower_bound = input('enter lower bound')\n",
    "        if isfloat(lower_bound) or is_datetime(lower_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "        else:\n",
    "            print('Value not a Number! please retry')\n",
    "    return {'lower_bound': float(lower_bound)}\n",
    "\n",
    "def ask_for_upper_bound() -> Dict[str, float]:\n",
    "    \n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        upper_bound = input('enter upper bound')\n",
    "        if isfloat(upper_bound) or is_datetime(upper_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "        else:\n",
    "            print('Value not a Number! please retry')\n",
    "    return {'upper_bound': float(upper_bound)}\n",
    "\n",
    "\n",
    "def _ask_for_data_type(data_type: Enum) -> Enum:\n",
    "    \"\"\"asks user for datatype contains in `data_type`\n",
    "    If user selects `cancel`, it will return None\n",
    "    \"\"\"\n",
    "    \n",
    "    _available_data_type = [t for t in data_type]  # get all keys contain in data_type\n",
    "    n_avail_data_type = len(_available_data_type)\n",
    "    msg, _n_answer = get_data_type_selection_msg(data_type, ign_msg=\"cancel operation\")\n",
    "    data_type_selection = get_user_input(msg, _n_answer)\n",
    "    \n",
    "    if str(_n_answer) != data_type_selection:\n",
    "        return _available_data_type[int(data_type_selection) - 1]\n",
    "    \n",
    "    \n",
    "def ask_for_data_type() -> Dict[str, Any]:\n",
    "\n",
    "    updates = None\n",
    "    \n",
    "    new_data_format = _ask_for_data_type(DataType)\n",
    "    \n",
    "    if new_data_format is not None: \n",
    "        # case where 'cancel operation' hasnot been selected\n",
    "                \n",
    "        updates = {'data_format': new_data_format.name}\n",
    "        if  isinstance(next(iter(new_data_format.value)), Enum):\n",
    "            # if subtypes are available\n",
    "            new_data_type= _ask_for_data_type(new_data_format.value)\n",
    "            new_values = list(map(lambda x: str(x), new_data_type.value))\n",
    "            updates.update({'data_type': new_data_type, 'values': new_values})\n",
    "        else:\n",
    "            new_values = list(map(lambda x: str(x), new_data_format.value))\n",
    "            updates.update({'values': new_values})\n",
    "    return updates\n",
    "\n",
    "\n",
    "def ask_for_categorical_values() -> Dict[str, Any]:\n",
    "    possible_values = input('enter possible values (separated by \",\")')\n",
    "    possible_values = possible_values.split(\",\")  # separate values passed by user into a list\n",
    "    return {'categorical_values': possible_values}\n",
    "\n",
    "\n",
    "def ask_for_date_format() -> Dict[str, Any]:\n",
    "    # TODO : ask for date format (UTC, ....)\n",
    "    msg = 'please enter date format:\\n1)timetsamp\\n2)ISO date format (YYYY-MM-DD)\\n3)custom date format\\n'\n",
    "    user_selection = input(msg)\n",
    "    # default date format\n",
    "    msg = {'1': 'timestamp',\n",
    "          '2': '(American default date format) mm/dd/yy',\n",
    "          '3': '(Europeen default date format) dd/mm/yy',\n",
    "           '4': 'ISO date format (YYYY-MM-DD)',\n",
    "          '5': 'custom date format',\n",
    "          '6': 'select timezone'}\n",
    "    pass\n",
    "\n",
    "def edit_feature_format_file_ref(feature_content: Dict[str, Any],\n",
    "                                  feature_name: str,\n",
    "                                  available_categorical_data_type: List[Enum],\n",
    "                                  messages: Dict[str, str],\n",
    "                                  ignore_keystroke: int) -> Dict[str, Any]:\n",
    "    \"\"\"Edits a specific feature that belongs to a specific view within a format file\"\"\"\n",
    "    \n",
    "\n",
    "    _is_feature_unparsed = True  \n",
    "    _is_cancelled = False  # whether parsing of current column has been cancelled or not\n",
    "    _is_first_edit = True\n",
    "    _avail_data_type_properties = [dtype for dtype in DataTypeProperties]\n",
    "    \n",
    "    # iterate over number of feature contained in view, and ask for each feature if changes are needed\n",
    "    while _is_feature_unparsed:\n",
    "        if _is_cancelled or not _is_first_edit:\n",
    "            _f_answer = True\n",
    "        else:\n",
    "            _f_answer = get_user_input(f\"Edit variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "            # ask if user wants to edit feature variables\n",
    "            _f_answer = parse_yes_no_msg(_f_answer)\n",
    "            _is_operation_cancelled = False  # for cancelling feature edition\n",
    "            _is_first_edit = False\n",
    "        if _f_answer:\n",
    "            # case where user wants to edit the current feature\n",
    "            \n",
    "            _msg = messages['edit']\n",
    "            \n",
    "            data_format = feature_content.get('data_format')\n",
    "            for data_type_properties in _avail_data_type_properties:\n",
    "                if data_format == data_type_properties.name:\n",
    "                    # get data property from data_format\n",
    "                    select_msg, n_actions, possible_actions = create_msg_action_selection(data_type_properties)\n",
    "                    _msg += select_msg\n",
    "                    _edit_selection = get_user_input(_msg, n_actions)\n",
    "\n",
    "                    _edited_field, _is_cancelled = select_action(\n",
    "                                                                  _edit_selection,\n",
    "                                                                  possible_actions\n",
    "                                                                  #available_categorical_data_type,\n",
    "                                                                  #messages['data_type_select']\n",
    "                                                                )\n",
    "\n",
    "            if not _is_cancelled:\n",
    "                # if user has not cancelled field edition\n",
    "                if _edited_field is not None:\n",
    "                    feature_content.update(_edited_field)\n",
    "             \n",
    "                _c_answer = get_user_input(f\"Continue Editing variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "                _is_feature_unparsed = parse_yes_no_msg(_c_answer)\n",
    "            else:\n",
    "                _is_feature_unparsed = False\n",
    "                \n",
    "        else:\n",
    "            _is_feature_unparsed = False\n",
    "            \n",
    "    return feature_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ed0d815",
   "metadata": {},
   "outputs": [],
   "source": [
    " CategoricalDataType.BOOLEAN.name\n",
    "def edit_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \n",
    "    # CLI for editing `format_file_ref`, a file containing information about each variable\n",
    "    # in a tabular dataset\n",
    "    print(f'Now editing format file ref')\n",
    "    \n",
    "    ## variables initialization\n",
    "    available_categorical_data_types = [t for t in CategoricalDataType]\n",
    "    _file_names = list(format_file_ref.keys())\n",
    "    _n_tot_files = len(_file_names)\n",
    "    \n",
    "    ## messages definition\n",
    "    _data_type_selection_msg, ign_key = get_data_type_selection_msg(available_categorical_data_types)\n",
    "    \n",
    "    _messages = {\n",
    "        'yes_or_no': get_yes_no_msg(),\n",
    "        'edit': 'Which field should be modified?\\n',\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # iterate over name of files (ie views)\n",
    "    for i_file in range(_n_tot_files):\n",
    "        # ask for each file if user wants to edt it\n",
    "        _answer = get_user_input(f\"Edit file: {_file_names[i_file]}?\\n\" + _messages['yes_or_no'], 2)\n",
    "        _answer = parse_yes_no_msg(_answer)\n",
    "        \n",
    "        if _answer:\n",
    "            # case where user wants to modify current view scheme\n",
    "            _file_content = format_file_ref[_file_names[i_file]]  # get file (ie view) content\n",
    "            \n",
    "            ## variables initialization for parsing current view\n",
    "            _features_names = list(_file_content.keys())\n",
    "            _n_tot_feature = len(_features_names)\n",
    "            \n",
    "            # iterate over features found in view\n",
    "            for i_feature in range(_n_tot_feature):\n",
    "                feature_name = _features_names[i_feature]\n",
    "                feature_content = _file_content[feature_name]\n",
    "                feature_content = edit_feature_format_file_ref(feature_content,\n",
    "                                                               feature_name,\n",
    "                                                               available_categorical_data_types,\n",
    "                                                               _messages,\n",
    "                                                               ign_key)\n",
    "            format_file_ref[_file_names[i_file]].update({feature_name: feature_content})\n",
    "            \n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ef36608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': [\"<class 'bool'>\"],\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': ['1', ' 2', ' 4']},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False,\n",
       "   'lower_bound': 0.0},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_format_file_ref(format_file)\n",
    "\n",
    "# TODO: create possiblity for user to update data type when editing format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f519aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now editing format file ref\n",
      "Edit file: file1?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: contatct?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Edit variable: discrete?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1) data_type\n",
      "2) lower bound\n",
      "3)upper bound\n",
      "4) Cancel Operation\n",
      "1\n",
      "action 1 4 {'1': <function ask_for_data_type at 0x7ff11b38e3a0>, '2': <function ask_for_lower_bound at 0x7ff11b456ca0>, '3': <function ask_for_upper_bound at 0x7ff11b4569d0>, '4': <function cancel_operation at 0x7ff11b456160>}\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) cancel operation\n",
      "5\n",
      "1) DISCRETE \n",
      "2) CHARACTER \n",
      "3) cancel operation\n",
      "2\n",
      "Continue Editing variable: discrete?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: city?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1) data_type\n",
      "2) Values taken\n",
      "3) Cancel Operation\n",
      "2\n",
      "action 2 3 {'1': <function ask_for_data_type at 0x7ff11b38e3a0>, '2': <function ask_for_categorical_values at 0x7ff11b38e8b0>, '3': <function cancel_operation at 0x7ff11b456160>}\n",
      "enter possible values (separated by \",\")a, b, c\n",
      "Continue Editing variable: city?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: pkey?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: file2?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'CUSTOM',\n",
       "   'data_type': <CustomDataType.CHARACTER: [<class 'str'>, <class 'object'>]>,\n",
       "   'values': [\"<class 'str'>\", \"<class 'object'>\"],\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False,\n",
       "   'categorical_values': ['a', ' b', ' c']},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref = load_format_file_ref('multi_format_file')\n",
    "edit_format_file_ref(multi_format_file_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ba372",
   "metadata": {},
   "source": [
    "## tabular data sanity check using file format ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "\n",
    "\n",
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Tuple[Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    # check for each variable name existing in one view, that it doesnot exist in another\n",
    "    # view. if it is, rename both variables\n",
    "    # for this purpose, parse every combination once\n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "\n",
    "def join_muti_view_dataset(multi_view_dataset: pd.DataFrame,\n",
    "                           primary_key: str=None) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    _views_name = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "    \n",
    "    joined_dataframe = multi_view_dataset[_views_name[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_name)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_name[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_name[x]))\n",
    "        \n",
    "        #df['file1'].join(df['file2'].set_index('pkey'), on='pkey', rsuffix='.file2')\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"\"\"\"\n",
    "    _views_names = list(format_file_ref.keys())\n",
    "    primary_key = None\n",
    "    _c_view = None\n",
    "    for view_name in _views_names:\n",
    "        file_content = format_file_ref[view_name]\n",
    "        _features_names = list(file_content.keys())\n",
    "        for feature_name in _features_names:\n",
    "            feature_content  = file_content[feature_name]\n",
    "            _d_format = feature_content.get('data_format')\n",
    "            \n",
    "            if _d_format == DataType.KEY.name:\n",
    "                if _c_view is None:\n",
    "                    primary_key = feature_name\n",
    "                    _c_view = view_name\n",
    "                    print(f'found primary key {primary_key}')\n",
    "                else:\n",
    "                    print(f'error: found 2 primary keys is same view {view_name}')\n",
    "        _c_view = None\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "\n",
    "def select_data_from_format_file_ref(datasets: Dict[str, Dict[str, Any]],\n",
    "                                     format_file: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"returns an updated dataset containing only the features detailed in format_file\"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    updated_dataset = {}\n",
    "    _views_format_file = list(format_file.keys())\n",
    "    \n",
    "    for view in _views_format_file:\n",
    "        if view in datasets.keys():\n",
    "            # only extract features from format_file\n",
    "            _format_file_features = list(format_file[view].keys())\n",
    "            _current_dataset_feature = datasets[view].columns.tolist()\n",
    "            try:\n",
    "                updated_dataset[view] = datasets[view][_format_file_features]\n",
    "            except KeyError as ke:\n",
    "                # catch error if a column is specified in data format file\n",
    "                # but not found in dataset\n",
    "                _missing_feature = []\n",
    "                for feature in _format_file_features:\n",
    "                    if feature not in _current_dataset_feature:\n",
    "                        _missing_feature.append(feature)\n",
    "                print('Error: th following features', *_missing_feature, f'are not found in view: {view}')\n",
    "        else:\n",
    "            # trigger error\n",
    "            print(f'error!: missing view {view} in dataset')\n",
    "            \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68eedfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "df_to_check[n_feature_name]dataset_to_check = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd657d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary key None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>CDRSB.bl</th>\n",
       "      <th>ADAS11.bl</th>\n",
       "      <th>MMSE.bl</th>\n",
       "      <th>RAVLT.immediate.bl</th>\n",
       "      <th>RAVLT.learning.bl</th>\n",
       "      <th>FAQ.bl</th>\n",
       "      <th>TAU.MEDIAN.bl</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.739439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>132.571916</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>64.933800</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.787719</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.987722</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.049924</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>50.314425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.690457</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.217830</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-61.573234</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>61.896022</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.065806</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.083170</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.985248</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.289059</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.229102</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.650504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103.238647</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.089863</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.780563</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name  CDRSB.bl  ADAS11.bl  MMSE.bl  RAVLT.immediate.bl  \\\n",
       "0                  1.0        8.0     27.0           23.739439   \n",
       "1                  0.0        0.0     30.0           64.933800   \n",
       "2                  0.0        8.0     24.0           36.987722   \n",
       "3                  0.0        3.0     29.0           50.314425   \n",
       "4                  0.0        0.0     30.0           57.217830   \n",
       "..                 ...        ...      ...                 ...   \n",
       "995                1.0        2.0     29.0           61.896022   \n",
       "996                0.0        1.0     29.0           62.083170   \n",
       "997                3.0       14.0     24.0           22.289059   \n",
       "998                0.0       13.0     26.0           31.650504   \n",
       "999                0.0       15.0     28.0           29.089863   \n",
       "\n",
       "feature_name  RAVLT.learning.bl  FAQ.bl  TAU.MEDIAN.bl   AGE  \n",
       "0                           4.0     3.0     132.571916  75.0  \n",
       "1                           9.0     0.0      33.787719  67.0  \n",
       "2                           3.0     0.0     110.049924  63.0  \n",
       "3                           5.0     3.0     138.690457  75.0  \n",
       "4                           9.0     0.0     -61.573234  65.0  \n",
       "..                          ...     ...            ...   ...  \n",
       "995                         8.0     0.0      87.065806  76.0  \n",
       "996                         8.0     1.0     121.985248  77.0  \n",
       "997                         2.0     7.0     157.229102  74.0  \n",
       "998                         2.0     4.0     103.238647  64.0  \n",
       "999                         3.0     4.0      54.780563  65.0  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(format_file.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(dataset_to_check, format_file)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_to_check = join_muti_view_dataset(multi_df_to_check)\n",
    "    \n",
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15a12eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': [\"<class 'bool'>\"], 'is_missing_values': True, 'categorical_values': ['1', ' 2', ' 4']}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': True}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False, 'lower_bound': 0.0}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# sanity check on columns using format_file\n",
    "\n",
    "\n",
    "\n",
    "_views_format_file = list(format_file.keys())\n",
    "    \n",
    "for view in _views_format_file:\n",
    "    _features_format_file = format_file[view]\n",
    "    \n",
    "    for feature in _features_format_file:\n",
    "        feature_content = df_to_check[feature]\n",
    "        feature_format =  _features_format_file[feature]\n",
    "        # 1. check for datatype consistency\n",
    "        \n",
    "        data_types = find_data_type(feature_format['data_format'], feature_format['data_type'])\n",
    "        \n",
    "        print(dtype[-1].value, feature_content.dtype, feature_format)\n",
    "        if any(t == feature_content.dtype for t in data_types[-1].value ):\n",
    "            print('ok')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6f61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e87a346",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_format_file_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7316/3000991797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_format_file_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_format_file_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multi_format_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmulti_dataset_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tabular_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'test7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_format_file_ref' is not defined"
     ]
    }
   ],
   "source": [
    "multi_format_file_ref = load_format_file_ref('multi_format_file')\n",
    "multi_dataset_to_check = load_tabular_datasets(r'test7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34813b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'CATEGORICAL',\n",
       " 'data_type': 'BOOLEAN',\n",
       " 'values': 'bool',\n",
       " 'is_missing_values': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file1']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "566d2128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found primary key pkey\n",
      "found primary key pkey\n",
      "found primary key pkey\n",
      "primary key pkey\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.98667</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.78856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name discrete       city                  pkey   e file1.1      2  \\\n",
       "0                64.0      Lille  qpqorfhylu gmfjy bdj  16   False  False   \n",
       "1                26.0      Lille  kkmjozalfyirgsire ui  96    True   True   \n",
       "2                61.0      Paris  ezfasuuycdda foisjte   8    True   True   \n",
       "3                29.0      Paris  faxiqkt xggzmwzoidbg   6    True  False   \n",
       "4                99.0      Lille  znwhlj rwzdutnagwasy  79    True   True   \n",
       "..                ...        ...                   ...  ..     ...    ...   \n",
       "95                9.0      Paris  zeqhcikzdodus jn qjf  62    True   True   \n",
       "96               98.0  Marseille  iicthcvfmkajbvr gzir  49   False   True   \n",
       "97               21.0      Lille  ztjakcsk bhjoksdz lm  14   False  False   \n",
       "98               42.0  Marseille   sabunaa opt vpulnxj  10    True  False   \n",
       "99                3.0      Paris  qmbexyexvgromrm admu  89    True   True   \n",
       "\n",
       "feature_name           file1.time  pressure e.1 gender blood type file2.1  \\\n",
       "0             2018-01-03 04:00:00   0.98667  98  WOMAN          A   False   \n",
       "1             2018-01-02 04:00:00  0.996889  35    MAN         AB    True   \n",
       "2             2018-01-01 09:00:00  0.777026  65    MAN          A   False   \n",
       "3             2018-01-04 20:00:00  0.877527  81    MAN         AB    True   \n",
       "4             2018-01-04 09:00:00  0.447389  88  WOMAN          O    True   \n",
       "..                            ...       ...  ..    ...        ...     ...   \n",
       "95            2018-01-02 13:00:00  0.953184  53    MAN         AB   False   \n",
       "96            2018-01-02 21:00:00  0.442283  35    MAN        NaN    True   \n",
       "97            2018-01-02 06:00:00  0.988543  67    MAN          B   False   \n",
       "98            2018-01-02 01:00:00  0.059791  48    MAN          B    True   \n",
       "99            2018-01-03 22:00:00  0.939352  13    MAN          B   False   \n",
       "\n",
       "feature_name           file2.time        pH  \n",
       "0             2018-01-02 06:00:00       NaN  \n",
       "1             2018-01-01 00:00:00  0.023107  \n",
       "2             2018-01-02 10:00:00  0.587685  \n",
       "3             2018-01-03 12:00:00  0.894073  \n",
       "4             2018-01-01 10:00:00  0.026831  \n",
       "..                            ...       ...  \n",
       "95            2018-01-02 05:00:00   0.78856  \n",
       "96            2018-01-05 02:00:00  0.402979  \n",
       "97            2018-01-01 12:00:00       NaN  \n",
       "98            2018-01-02 09:00:00  0.651801  \n",
       "99            2018-01-03 02:00:00  0.751969  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(multi_format_file_ref.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(multi_format_file_ref)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(multi_dataset_to_check, multi_format_file_ref)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names,\n",
    "                                                             primary_key)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_to_check = join_muti_view_dataset(multi_df_to_check, primary_key)\n",
    "    \n",
    "df_to_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3541d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': ['e',\n",
       "  'file1.1',\n",
       "  '2',\n",
       "  'file1.time',\n",
       "  'pressure',\n",
       "  'e.1',\n",
       "  'gender',\n",
       "  'blood type',\n",
       "  'pkey'],\n",
       " 'contatct': ['discrete', 'city', 'pkey'],\n",
       " 'file2': ['file2.1', 'file2.time', 'pH', 'pkey']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e97829a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_key_variable_compliance(column: pd.Series,\n",
    "                                  \n",
    "                                  col_name:str=None,\n",
    "                                  warning=None) -> bool:\n",
    "    \"\"\"performs data sanity check over variable of type `KEY`\n",
    "    warning should be Critical warnings\n",
    "    \"\"\"\n",
    "    # variables initialisation\n",
    "    is_test_passed = True \n",
    "    \n",
    "    # 1. check unicity of values in column\n",
    "    \n",
    "    n_unique_samples = unique(column, number=True)\n",
    "    n_samples = column.shape[0]\n",
    "    \n",
    "    if n_unique_samples != n_samples:\n",
    "        is_test_passed = False\n",
    "        print(f'error: keys not unique ! b of samples= {n_samples} and unique values {n_unique_samples}')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "    # 2. check if missing database contained in key (key should not contain any missing data)\n",
    "    if check_missing_data(column):\n",
    "        is_test_passed = False\n",
    "        print('error: missing data found in key')\n",
    "    else:\n",
    "        \n",
    "        print('test 2 passed')\n",
    "\n",
    "                \n",
    "    return is_test_passed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ac12cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_to_check['file1.time'] > '2018-01-04 09:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81f47c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_to_check['file1.time'].apply(is_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce557ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "test 2 passed\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "test 5: passed\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['2'], multi_format_file_ref['file1']['2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "168b85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_format_file_ref['file1']['2'].update({'categorical_values': [True,False]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0b8e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'CATEGORICAL',\n",
       " 'data_type': 'BOOLEAN',\n",
       " 'values': 'bool',\n",
       " 'is_missing_values': True,\n",
       " 'categorical_values': [True, False]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file1']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14d311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variable_compliance(column: pd.Series,\n",
    "                               format_file_ref: Dict[str, Any],\n",
    "                               col_name:str=None,\n",
    "                               warning=None) -> Tuple[bool, bool]:\n",
    "    \"\"\"performs a data sanity check on variable `col_name` given instruction in \n",
    "    data_file_ref\n",
    "    \"\"\"\n",
    "    is_test_passed = True\n",
    "    \n",
    "    \n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    # remove nan (missing values) from \n",
    "    column_without_nan = column.dropna()\n",
    "    \n",
    "    \n",
    "    if data_format_name is None:\n",
    "        print(f'critical wraning: data fromat {data_format_name} not understood')\n",
    "    # 1. check data sub type\n",
    "    try:\n",
    "        data_type = find_data_type(data_format_name, data_type_name)\n",
    "    except ValueError as err:\n",
    "        data_type = None\n",
    "        print('Critical warning: data format and data type mismatch')\n",
    "    does_column_have_correct_data_type = any(t for t in data_type.value)\n",
    "    if not does_column_have_correct_data_type:\n",
    "        print(f'error: data type {column.dtype} doesnot have the data type specified in format reference file')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "\n",
    "    # 2. check if missing values are allowed\n",
    "    is_missing_data = check_missing_data(column)\n",
    "    is_missing_values_authorized = format_file_ref.get('is_missing_values', 'test_skipped')\n",
    "    if is_missing_values_authorized == 'test_skipped':\n",
    "        print('missing_value test skipped')\n",
    "    elif not is_missing_values_authorized and is_missing_data:\n",
    "\n",
    "        print('Error found missing data but missing data are not authorized')\n",
    "    else:\n",
    "        print('test 2 passed')\n",
    "    \n",
    "    \n",
    "    # 3. check lower bound\n",
    "    print(format_file_ref)\n",
    "    lower_bound = format_file_ref.get('lower_bound')\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        \n",
    "        # should work for both numerical and datetime data sets\n",
    "        \n",
    "        is_lower_bound_correct = np.all(column_without_nan >= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_lower_bound_correct:\n",
    "            print('Warning: found some data below lower bound')\n",
    "        else:\n",
    "            print('test 3 passed')\n",
    "    else:\n",
    "        print('test 3 skipped ')\n",
    "    # 4. check upper bound\n",
    "    upper_bound = format_file_ref.get('upper_bound')\n",
    "    if upper_bound is not None:\n",
    "         # should work for both numerical and datetime data sets\n",
    "        is_upper_bound_correct = np.all(column_without_nan <= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_upper_bound_correct:\n",
    "            print('Warning: found some data  above upper bound')\n",
    "        else:\n",
    "            print('test 4 passed')\n",
    "            \n",
    "    else:\n",
    "        print('test 4 skipped')\n",
    "    # 5. check if possible_values are contained in variable\n",
    "    categorical_values = format_file_ref.get('categorical_values')    \n",
    "    if categorical_values is None:\n",
    "        print('categorical value check test skipped')\n",
    "    else:\n",
    "        unique_values = unique(column)\n",
    "        _is_error_found = False\n",
    "        for val in unique_values:\n",
    "            if val not in categorical_values and not np.isnan(val):\n",
    "                print(f'critical warning: {val} not in possible values')\n",
    "                _is_error_found = True\n",
    "        if not _is_error_found:\n",
    "            print('test 5: passed')\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "def check_datetime_variable_compliance(column: pd.Series):\n",
    "    \"\"\"additional data sanity checks for datetime variable\"\"\"\n",
    "    # test 1. check if datetime is parsable\n",
    "    \n",
    "    # remove nan\n",
    "    column_without_nan = column.dropna()\n",
    "    are_datetime_parsables =  np.all(column.apply(is_datetime))\n",
    "    if not are_datetime_parsables:\n",
    "        print('Warning: at least one variable is not a datetime')\n",
    "        \n",
    "    else:\n",
    "        print('datetime parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c7bc10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': True, 'categorical_values': [True, False]}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "test 5: passed\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "contatct\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "file2\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "Error found missing data but missing data are not authorized\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False, 'lower_bound': -10}\n",
      "test 3 passed\n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n"
     ]
    }
   ],
   "source": [
    "# Data sanity check\n",
    "\n",
    "new_feature_name = { v: list(pre_parsed_dataset_to_check[v].columns) for v in views_names}\n",
    "new_feature_name\n",
    "\n",
    "for view in views_names:\n",
    "    print(view)\n",
    "    \n",
    "    feature_names = list(multi_format_file_ref[view].keys())\n",
    "    for n_feature_name, feature_name in zip(new_feature_name[view], feature_names):\n",
    "        check_variable_compliance(df_to_check[n_feature_name], multi_format_file_ref[view][feature_name])\n",
    "        data_format = multi_format_file_ref[view][feature_name].get('data_format')\n",
    "        if data_format == DataType.DATETIME.name:\n",
    "            # addtional check for DATETIME data format\n",
    "            check_datetime_variable_compliance(df_to_check[n_feature_name])\n",
    "            \n",
    "        if data_format == DataType.KEY.name:\n",
    "            check_key_variable_compliance(df_to_check[n_feature_name])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0946850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'QUANTITATIVE',\n",
       " 'data_type': 'CONTINUOUS',\n",
       " 'values': 'float64',\n",
       " 'is_missing_values': False,\n",
       " 'lower_bound': -10}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file2']['pH'].update({'lower_bound': -10})\n",
    "multi_format_file_ref['file2']['pH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "936c0a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'DATETIME',\n",
       " 'data_type': 'DATETIME',\n",
       " 'values': 'object',\n",
       " 'is_missing_values': False}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref[view][feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_data_type("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c760347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "Error found missing data but missing data are not authorized\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False, 'lower_bound': -10}\n",
      "test 3 passed\n",
      "test 4 skipped\n",
      "categorical value check test skipped\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['pH'], multi_format_file_ref['file2']['pH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b06bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "95    False\n",
       "96    False\n",
       "97    False\n",
       "98    False\n",
       "99    False\n",
       "Name: pH, Length: 100, dtype: bool"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check['pH'] > np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c94fb830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': [True, False]},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ee4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "test 2 passed\n",
      "categorical value check test skipped\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['pkey'], multi_format_file_ref['file2']['pkey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ff5d5",
   "metadata": {},
   "source": [
    "## data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca423d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "887f090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) YES\\n2) NO\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_yes_or_no_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2460bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85714c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d161776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "ll\n",
      "please add new feature name:\n",
      "ll\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "4\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "kk\n",
      "please add new feature name:\n",
      "vkeof\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "is_views_finished = False\n",
    "\n",
    "\n",
    "views_format_file = {}\n",
    "\n",
    "while not is_views_finished:\n",
    "    is_features_finished = False\n",
    "    resp = input('do you want to add a new view (file)?\\n' + msg_yes_or_no_question)\n",
    "    resp = yes_or_no_question_key.get(resp)\n",
    "    if not resp:\n",
    "        is_views_finished = True\n",
    "        print('process done')\n",
    "        continue\n",
    "    new_view = input('please add new view name:\\n')\n",
    "    while not is_features_finished:\n",
    "        feature_format_file = {}\n",
    "        new_feature = input('please add new feature name:\\n')\n",
    "        feature_format_file[new_feature] = {}categorical_values\n",
    "        is_column_parsed = False\n",
    "        try:\n",
    "            while not is_column_parsed:\n",
    "                data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "                if data_format_id.isdigit() and int(data_format_id) <= n_available_data_type+1:\n",
    "                    # check if value passed by user is correct (if it is integer,\n",
    "                    # and whithin range [1, n_available_data_type])\n",
    "                    is_column_parsed = True\n",
    "                \n",
    "                else:\n",
    "                    print(f'error ! {data_format_id} value not understood')\n",
    "                    \n",
    "        except KeyboardInterrupt as e:\n",
    "            print('stopping now' + str(e))\n",
    "        resp = input('do you want to add a new variable (feature) ?' + msg_yes_or_no_question)\n",
    "        resp = yes_or_no_question_key.get(resp)\n",
    "        if not resp:\n",
    "            is_features_finished = True\n",
    "            print('process done')\n",
    "            continue\n",
    "    views_format_file[new_view] = feature_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee0e0cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'datetime64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9387/2903691535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'datetime64'"
     ]
    }
   ],
   "source": [
    "type(np.datetime64(\"2018-01-01\"))\n",
    "import datetime\n",
    "type(datetime.datetime(2018, 1, 1))\n",
    "\n",
    "pd.datetime64[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24be197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = type(pd.to_datetime('13000101', format='%Y%m%d', errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52c37736",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Series(pd.date_range(\"1/1/2011\", freq=\"H\", periods=3)).dtype\n",
    "\n",
    "t =type(t).type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b13acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(t == t1 for t1 in [pd.Timestamp, pd.Timedelta, pd.Period, datetime.datetime,np.datetime64] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb165f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9881289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDRSB.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'ADAS11.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'MMSE.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.immediate.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.learning.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.forgetting.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'FAQ.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'WholeBrain.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Ventricles.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Hippocampus.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'MidTemp.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Entorhinal.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'ABETA.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'PTAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'TAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'AGE': {'data_type': <QuantitativeDataType.DISCRETE: [<class 'int'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb7ad297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.dtype[float64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(dataset[feature].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dbaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_add_numeric_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_arith_method',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_binop',\n",
       " '_cacher',\n",
       " '_can_hold_na',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_from_arguments',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_convert',\n",
       " '_convert_dtypes',\n",
       " '_data',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_duplicated',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_from_mgr',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_index_resolvers',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_get_values_tuple',\n",
       " '_get_with',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_index',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_init_dict',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_cached',\n",
       " '_is_copy',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_item_cache',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_map_values',\n",
       " '_maybe_update_cacher',\n",
       " '_memory_usage',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_name',\n",
       " '_needs_reindex_multi',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_indexer',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_replace_single',\n",
       " '_repr_data_resource_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_labels',\n",
       " '_set_name',\n",
       " '_set_value',\n",
       " '_set_values',\n",
       " '_set_with',\n",
       " '_set_with_engine',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'array',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'autocorr',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'clip',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'factorize',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'hasnans',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'interpolate',\n",
       " 'is_monotonic',\n",
       " 'is_monotonic_decreasing',\n",
       " 'is_monotonic_increasing',\n",
       " 'is_unique',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'item',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memory_usage',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'ravel',\n",
       " 'rdiv',\n",
       " 'rdivmod',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'repeat',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'searchsorted',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_frame',\n",
       " 'to_hdf',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_list',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_sql',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unique',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae52ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
