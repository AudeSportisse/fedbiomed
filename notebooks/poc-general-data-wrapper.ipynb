{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee99bdb4",
   "metadata": {},
   "source": [
    "Related to user story: [SP11-Item04: General Data Wrapper PoC](https://gitlab.inria.fr/fedbiomed/fedbiomed/-/issues/164)\n",
    "\n",
    "## Tabular dataset\n",
    "\n",
    "Workflow of data pre processing:\n",
    "\n",
    "1. Columns name should be shared with the researcher\n",
    "2. Data format file to be filled by clinicians.\n",
    "3. Specify if missing data are allowed for a given columns (Exception). The file will be used for data verification during FL pre-processing,\n",
    "4. Outlier verification for quantitative data, continuous and discrete, and for dates (Critical warning),\n",
    "5. Missing data imputation by local mean (or optional NN), or majority voting for discrete labels. Give warnings when missing data are found (for verification a posteriori).\n",
    "6. Give critical warning when too many missing are found (>50%),\n",
    "7. Verify that number of available data is greater then minimum required (Error)\n",
    "\n",
    "Critical warnings have different levels of disclosure to the researcher (1) only the warning, 2) type of warning, 3) type of warning and column affected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b334c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-2.4.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ae4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. load  a single view dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional, Callable\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17059269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class ExcelSignatures(Enum):\n",
    "    XLSX = (b'\\x50\\x4B\\x05\\x06', 2, -22, 4)\n",
    "    LSX1 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 512, 8)\n",
    "    LSX2 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 1536, 8)\n",
    "    LSX3 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 2048, 8)\n",
    "    \n",
    "    def __init__(self, sig, whence, offset, size):\n",
    "        self._sig = sig\n",
    "        self._whence = whence\n",
    "        self._offset = offset\n",
    "        self._size = size\n",
    "\n",
    "    @property \n",
    "    def signature(self) -> bytes:\n",
    "        return self._sig\n",
    "    \n",
    "    @property\n",
    "    def whence(self) -> int:\n",
    "        return self._whence\n",
    "    \n",
    "    @property\n",
    "    def offset(self) -> int:\n",
    "        return self._offset\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self._size\n",
    "\n",
    "\n",
    "\n",
    "def load_tabular_datasets(path:str) -> Dict[str, pd.DataFrame]:\n",
    "    tabular_datasets = {}\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        print('directory found')\n",
    "        _is_folder = True\n",
    "        \n",
    "        _tabular_data_files = os.listdir(path)\n",
    "    else:\n",
    "        print('file found')\n",
    "        _is_folder = False\n",
    "        _tabular_data_files = (path,)\n",
    "        \n",
    "    for tabular_data_file in _tabular_data_files:\n",
    "        if _is_folder:\n",
    "            tabular_data_file = os.path.join(path, tabular_data_file)\n",
    "        \n",
    "        _is_excel = excel_sniffer(tabular_data_file)\n",
    "        _csv_delimiter, _csv_header = csv_sniffer(tabular_data_file)\n",
    "        _view_name = os.path.basename(tabular_data_file)\n",
    "        if _is_excel:\n",
    "            tabular_datasets[_view_name] = load_excel_file(tabular_data_file)\n",
    "        elif _csv_delimiter is not None:\n",
    "            tabular_datasets[_view_name] = load_csv_file(tabular_data_file,\n",
    "                                                               _csv_delimiter, \n",
    "                                                               _csv_header)\n",
    "        else:\n",
    "            print(f'warning: cannot parse {tabular_data_file}: not a tabular data file')\n",
    "        \n",
    "    return tabular_datasets\n",
    "\n",
    "def load_csv_file(path:str, delimiter:str, header:int) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataframe = pd.read_csv(path, delimiter=delimiter, header=header)\n",
    "    except csv.Error as err:\n",
    "        print('err', err, 'in file', path)\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "#https://stackoverflow.com/questions/23515791/how-to-check-the-uploaded-file-is-csv-or-xls-in-python/23515973\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_excel_file(path:str, sheet_name: Union[str, int]=0) -> pd.DataFrame:\n",
    "    \"\"\"May rely on openpyxl package\"\"\"\n",
    "    #with open(path, 'r') as excl:\n",
    "    #    _c = csv.DictReader(excl, dialect=csv.excel_tab)\n",
    "    #    _delimiter = _c.dialect.delimiter\n",
    "    \n",
    "    dataframe = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def excel_sniffer(path: str) -> bool:\n",
    "    \n",
    "    for excel_sig in ExcelSignatures:\n",
    "        with open(path, 'rb') as f:\n",
    "            f.seek(excel_sig.offset, excel_sig.whence)\n",
    "            bytes = f.read(excel_sig.size)\n",
    "\n",
    "            if bytes == excel_sig.signature:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "\n",
    "def csv_sniffer(path:str) :\n",
    "        \n",
    "    with open(path, 'r') as csvfile:\n",
    "        try:\n",
    "            # do some operation on file using sniffer to make sure considered file\n",
    "            # is a CSV file\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline())\n",
    "            delimiter = dialect.delimiter\n",
    "            dialect.lineterminator\n",
    "            has_header = csv.Sniffer().has_header(csvfile.readline())\n",
    "            if has_header:\n",
    "                header = 0\n",
    "            else:\n",
    "                header = None\n",
    "        except (csv.Error, UnicodeDecodeError) as err:\n",
    "            delimiter, header = None, None\n",
    "            print('err', err, 'in file', path)\n",
    "    return delimiter, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "750c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (3.0.9)\r\n",
      "Requirement already satisfied: et-xmlfile in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from openpyxl) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n",
      "err 'utf-8' codec can't decode byte 0x8c in position 15: invalid start byte in file ../../Exceltest.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Exceltest.xlsx':    ID   Age Eligibility\n",
       " 0    1   45           Y\n",
       " 1    2   45           Y\n",
       " 2    3   33           N\n",
       " 3    4   54           Y\n",
       " 4    5   45           Y\n",
       " 5    6   54         NaN\n",
       " 6    7   34           N\n",
       " 7    8   54         NaN\n",
       " 8    9   45         NaN\n",
       " 9   10   44           Y}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tabular_datasets('../../Exceltest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "single_view_dataset = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed673dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1':      a   e   i   o      0      1      2      3                 time  pressure  \\\n",
       " 0   48  98  65   5  False   True  False  False  2018-01-01 00:00:00  0.088082   \n",
       " 1   87  83  13  70   True  False   True  False  2018-01-01 01:00:00  0.774788   \n",
       " 2   46  73  81  96  False  False  False   True  2018-01-01 02:00:00  0.514092   \n",
       " 3   84  45  81  39  False   True   True   True  2018-01-01 03:00:00  0.832881   \n",
       " 4   94  84   0  15  False   True  False  False  2018-01-01 04:00:00  0.696152   \n",
       " ..  ..  ..  ..  ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95  14  66  25  64   True   True   True  False  2018-01-04 23:00:00  0.295578   \n",
       " 96  91  81  48  53  False  False   True   True  2018-01-05 00:00:00  0.474322   \n",
       " 97  15  82  12  51   True   True   True   True  2018-01-05 01:00:00  0.927511   \n",
       " 98  51  18   4  52  False  False   True   True  2018-01-05 02:00:00  0.494798   \n",
       " 99  51  70  63  77  False  False   True  False  2018-01-05 03:00:00  0.316395   \n",
       " \n",
       "         sp02  a.1  e.1  i.1  o.1 gender blood type                  pkey  \n",
       " 0   0.360430   90   63   60    8    MAN          A  zmixzrgvxrjqxoe sluk  \n",
       " 1   0.072426   15   20   45   10    MAN          O  vrzahnpfluspdcbfnaqt  \n",
       " 2   0.538551   30    2    6   22  WOMAN          A  pnrepvmrxqabdlvisclv  \n",
       " 3   0.761962   42   70    7   79  WOMAN         AB  gwj luzejwdxzsiljxzd  \n",
       " 4   0.389385   65   90   12   90    MAN          B  jjdvcnofivbqhirxzdyo  \n",
       " ..       ...  ...  ...  ...  ...    ...        ...                   ...  \n",
       " 95  0.027930   13   41   93   19  WOMAN          A  hrvepmqjn llgbzplshv  \n",
       " 96  0.545006   91   41   99   71  WOMAN          B  wroevwyuamxibzshlxxh  \n",
       " 97  0.059205   38    7   73   47    MAN          B  ywadcykylymkdtzfctpg  \n",
       " 98  0.125428   38   11   97    3    MAN          O  ruchbfa zwgenxslegrl  \n",
       " 99  0.772293   49   74   36   42  WOMAN         AB  zeapltpxuvuibfybxcll  \n",
       " \n",
       " [100 rows x 18 columns],\n",
       " 'contatct':     discrete       city                  pkey\n",
       " 0       64.0      Lille  qpqorfhylu gmfjy bdj\n",
       " 1       26.0      Lille  kkmjozalfyirgsire ui\n",
       " 2       61.0      Paris  ezfasuuycdda foisjte\n",
       " 3       29.0      Paris  faxiqkt xggzmwzoidbg\n",
       " 4       99.0      Lille  znwhlj rwzdutnagwasy\n",
       " ..       ...        ...                   ...\n",
       " 95       9.0      Paris  zeqhcikzdodus jn qjf\n",
       " 96      98.0  Marseille  iicthcvfmkajbvr gzir\n",
       " 97      21.0      Lille  ztjakcsk bhjoksdz lm\n",
       " 98      42.0  Marseille   sabunaa opt vpulnxj\n",
       " 99       3.0      Paris  qmbexyexvgromrm admu\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'file2':         0      1      2      3                 time        pH  \\\n",
       " 0   False   True  False   True  2018-01-01 00:00:00  0.023107   \n",
       " 1    True  False  False  False  2018-01-01 01:00:00       NaN   \n",
       " 2   False   True  False   True  2018-01-01 02:00:00  0.407279   \n",
       " 3    True   True   True   True  2018-01-01 03:00:00  0.536301   \n",
       " 4   False   True   True   True  2018-01-01 04:00:00  0.749443   \n",
       " ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95   True   True   True  False  2018-01-04 23:00:00       NaN   \n",
       " 96   True  False  False  False  2018-01-05 00:00:00  0.388389   \n",
       " 97  False   True   True  False  2018-01-05 01:00:00  0.889067   \n",
       " 98   True   True  False  False  2018-01-05 02:00:00  0.402979   \n",
       " 99   True  False   True  False  2018-01-05 03:00:00  0.667349   \n",
       " \n",
       "                     pkey  \n",
       " 0   kkmjozalfyirgsire ui  \n",
       " 1   xkdawggpnuulcewuoyzz  \n",
       " 2   khuulhwgwnjggrfoefce  \n",
       " 3   xxysdmwwmjsmyhaswfdb  \n",
       " 4   ldejfuij mnbnf wwmms  \n",
       " ..                   ...  \n",
       " 95  wrmdecb s pohtmrcdj   \n",
       " 96  whmwrpvqmerdpwwzxasf  \n",
       " 97  pnrepvmrxqabdlvisclv  \n",
       " 98  iicthcvfmkajbvr gzir  \n",
       " 99  kiejdmbuih awhuifwwd  \n",
       " \n",
       " [100 rows x 7 columns]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_view_dataframe =  load_tabular_datasets('test7')\n",
    "multi_view_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d533e99",
   "metadata": {},
   "source": [
    "Data format file to be filled by clinicians (step 2 int he workflow):\n",
    "\n",
    "Data format file will be a dictionary specifying the type: \n",
    "* for single view datasets:\n",
    "```{<feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}```\n",
    " * for multiview datatset\n",
    "```{{<view_name>: <feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}}```\n",
    "\n",
    "where\n",
    "* `<view_name>` is the name of the view\n",
    "* `<feature_name>` is the name of the feature\n",
    "* `<data_type>` can be categorical or continuous or missing_data or datetime\n",
    "* `<value_taken>` is the type of the value (eg int, char, float, signed, unsigned ...)\n",
    "* `<value_range>` represent either a list of bounds, an upper or a lower bound, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134b8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create data format file\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import enum\n",
    "import datetime\n",
    "\n",
    "# the use of Enum classes will prevent incorrect combination of values\n",
    "class QuantitativeDataType(Enum):\n",
    "    CONTINUOUS = [float, np.float64]\n",
    "    DISCRETE = [int, np.int64]\n",
    "\n",
    "class CategoricalDataType(Enum):\n",
    "    BOOLEAN = [bool]\n",
    "    NUMERICAL = [float, int, np.float64, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "class KeyDataType(Enum):\n",
    "    NUMERICAL = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "\n",
    "\n",
    "class CustomDataType(Enum):\n",
    "    \"\"\"for demo purpose: here a custom datatype\"\"\"\n",
    "    DISCRETE = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "    \n",
    "class DataType(Enum):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # what about \n",
    "    KEY = KeyDataType\n",
    "    QUANTITATIVE = QuantitativeDataType\n",
    "    CATEGORICAL = CategoricalDataType\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "    CUSTOM = CustomDataType  # custom data type (should be defined by user)\n",
    "    UNKNOWN = 'UNKNOWN'\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in DataType.__members__.items())\n",
    "\n",
    "class MissingValueAllowedDefault(Enum):\n",
    "    KEY = False\n",
    "    QUANTITATIVE = True\n",
    "    CATEGORICAL = True\n",
    "    DATETIME = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in MissingValueAllowedDefault.__members__.items())\n",
    "    \n",
    "    \n",
    "class DataTypeProperties(Enum):\n",
    "    \"\"\"Data Type possible modification (whithin CLI editing)\"\"\"\n",
    "    CATEGORICAL = (False, False, True, False, True)\n",
    "    QUANTITATIVE = (True, True, False, False, True)\n",
    "    DATETIME = (True, True, False, True, False)\n",
    "    UNKNOWN = (False, False, False, False, True)\n",
    "    CUSTOM = (True, True, True, False, True)\n",
    "    KEY = (True, True, False, True, False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 lower_bound: bool,\n",
    "                 upper_bound: bool,\n",
    "                 set_of_values: bool,\n",
    "                 date_format:bool,\n",
    "                 allow_missing_values: bool):\n",
    "        self._lower_bound = lower_bound\n",
    "        self._upper_bound = upper_bound\n",
    "        self._set_of_values = set_of_values\n",
    "        self._date_format = date_format\n",
    "        self._allow_missing_values = allow_missing_values\n",
    "    \n",
    "    @property\n",
    "    def lower_bound(self):\n",
    "        return self._lower_bound\n",
    "    \n",
    "    @property\n",
    "    def upper_bound(self):\n",
    "        return self._upper_bound\n",
    "    \n",
    "    @property\n",
    "    def set_of_values(self):\n",
    "        return self._set_of_values\n",
    "    \n",
    "    @property\n",
    "    def date_format(self):\n",
    "        return self._date_format\n",
    "    \n",
    "    @property\n",
    "    def allow_missing_values(self):\n",
    "        return self._allow_missing_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d35b2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.impute_missing_values_mode(data: Union[pandas.core.series.Series, pandas.core.frame.DataFrame])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImputationMethods.MODE_IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7e67bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(next(iter(DataType.DATETIME.value)), Enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type(\n",
    "                  #avail_data_types: enum.EnumMeta,\n",
    "                  d_format: Enum,\n",
    "                  d_type: type) ->  Tuple[Enum, List[Union[type, str]]]:\n",
    "    # varibales initialisation\n",
    "    present_d_types = []\n",
    "    sub_d_type_format = d_format\n",
    "    \n",
    "    \n",
    "    for avail_data_type in DataType:\n",
    "        if d_format is avail_data_type:\n",
    "            sub_dtypes = avail_data_type.value\n",
    "            #if not isinstance(sub_dtypes, str) and hasattr(sub_dtypes, '__getitem__') and isinstance(sub_dtypes[0], Enum):\n",
    "            if not isinstance(sub_dtypes, str):\n",
    "                # check if dtype has subtypes\n",
    "                #(eg if datatype is QUANTITATIVE, subtype will be CONTINOUS or DISCRETE)\n",
    "                if isinstance(next(iter(sub_dtypes)), Enum):\n",
    "                    \n",
    "                    for sub_dtype in sub_dtypes:\n",
    "                        if any(d_type == t for t in tuple(sub_dtype.value)):\n",
    "                            present_d_types.append(d_type)\n",
    "                            sub_d_type_format = sub_dtype\n",
    "                            print(sub_dtype, d_type)\n",
    "                else:\n",
    "                    # case where datatype doesnot have subtypes, eg DATETIME\n",
    "                    if any(d_type == t for t in sub_dtypes):\n",
    "                        present_d_types.append(d_type)\n",
    "                    sub_d_type_format = d_format\n",
    "            else:\n",
    "                # case where d_format is a string of character\n",
    "                sub_d_type_format = d_format\n",
    "    print(sub_d_type_format, '|', present_d_types)\n",
    "    return  sub_d_type_format, present_d_types\n",
    "\n",
    "\n",
    "def find_data_type(data_format_name: str, data_type_name: str=None) -> Enum:\n",
    "    \"\"\"Retrieves from a given data_format and data_type,\n",
    "    the corresponding Enum class describing data\"\"\"\n",
    "    \n",
    "    ## varible initialisation\n",
    "    data_type = None\n",
    "    _is_data_format_unrecognized = True\n",
    "    _is_data_type_unrecognized = True\n",
    "    \n",
    "    _available_data_types = [t for t in DataType]\n",
    "    \n",
    "    for a_data_type in _available_data_types:\n",
    "        if data_format_name == a_data_type.name:\n",
    "            _is_data_format_unrecognized = False\n",
    "            data_type = a_data_type\n",
    "            \n",
    "            for sub_type in a_data_type.value:\n",
    "                \n",
    "                if data_type_name is not None and  isinstance(sub_type, Enum):\n",
    "                    # check if sub data type exist (it shouldnot if variable is UNKNOWN)\n",
    "                    if data_type_name == sub_type.name:\n",
    "                        \n",
    "                        _is_data_type_unrecognized = False\n",
    "                        data_type = sub_type\n",
    "                \n",
    "                    \n",
    "                else:\n",
    "                    _is_data_type_unrecognized = False\n",
    "    # check for data formt file consistancy error\n",
    "    if any((_is_data_format_unrecognized, _is_data_type_unrecognized)):\n",
    "        if _is_data_format_unrecognized:\n",
    "            raise ValueError(f'error: {data_format_name} not recognized as a valid data type')\n",
    "        else:\n",
    "            raise ValueError(f'error {data_type_name} not recognized as a valid data type')\n",
    "            \n",
    "    return data_type\n",
    "\n",
    "def check_data_type_consistancy(sub_data_type: Enum, d_type: type):\n",
    "    \"\"\" checks if `sub_data_type` folds within \"\"\"\n",
    "    is_consistant = False\n",
    "    for data_type in sub_data_type:\n",
    "        is_type_in_data_type = any(d_type == t for t in data_type.value)\n",
    "        if is_type_in_data_type:\n",
    "            is_consistant = True\n",
    "            continue\n",
    "            \n",
    "    return is_consistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee00bd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data_type_consistancy(CategoricalDataType, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691fe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "df = pd.DataFrame({'w': [1, 2, 3, 4,  'jj', None]})\n",
    "print(check_missing_data(df['w']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f354b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936ac8f0",
   "metadata": {},
   "source": [
    "CLI details:\n",
    "\n",
    "1. open  csv file\n",
    "2. for each columns in file ask type of variable or if variable should be excluded\n",
    "3. automatically detect the type given values in columns \n",
    "4. ask for each columns if missing data are allowed\n",
    "\n",
    "\n",
    "eg :\n",
    "\n",
    "assume a column is of type discrete with integers\n",
    "\n",
    "\n",
    "1. user select it is quantitative\n",
    "2. then system will label it as quantitative-discrete\n",
    "\n",
    "\n",
    "**Question** : do we want an auto selection parameter choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d328b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_msg() -> str:\n",
    "    msg_yes_or_no_question = '1) YES\\n2) NO\\n'   \n",
    "    return msg_yes_or_no_question\n",
    "\n",
    "def parse_yes_no_msg(resp: str) -> bool:\n",
    "    \"\"\"implements logic to parse yes or no msg\"\"\"\n",
    "    yes_or_no_question_key = {'1': True,\n",
    "                    '2': False}\n",
    "    return yes_or_no_question_key.get(resp)\n",
    "\n",
    "def get_data_type_selection_msg(available_data_type:List[Enum],\n",
    "                               ign_msg: str = 'ignore this column') ->Tuple[str, int]:\n",
    "    \n",
    "    \n",
    "    n_available_data_type = len(available_data_type)\n",
    "    msg = ''\n",
    "\n",
    "    \n",
    "    for i, dtype in enumerate(available_data_type):\n",
    "        msg += '%d) %s \\n' %  (i+1, dtype.name)\n",
    "    \n",
    "    ignoring_key = i+2  # add ingoring entry\n",
    "    msg += f'%d) %s\\n' % (ignoring_key, ign_msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return msg, ignoring_key\n",
    "\n",
    "def unique(iterable: Iterator, number: bool = False) -> int:\n",
    "    \"\"\"returns number of unique values\"\"\"\n",
    "    set_of_values = set(iterable)\n",
    "    if number:\n",
    "        return len(set_of_values)\n",
    "    else:\n",
    "        return set_of_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65541a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# imputation methods\n",
    "\n",
    "def impute_missing_values_mean(data):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            for col in data.columns:\n",
    "                if (data[col].isnull().sum()>0):\n",
    "                    if any(data[col].dtype in x2 for x2 in  [x.value for x in QuantitativeDataType]):\n",
    "                        data[col].fillna(value=data[col].mean(),inplace=True)\n",
    "        else:\n",
    "            data = data.fillna(data.mean())\n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in loading data file')\n",
    "    \n",
    " #Categorical Data\n",
    "def impute_missing_values_mode(data: Union[pd.Series, pd.DataFrame]):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            for col in data.columns:\n",
    "                if (data[col].isnull().sum()>0):\n",
    "                    categorical_data_type = [x.value for x in CategoricalDataType]\n",
    "                    data_type_condition = any(data[col].dtype in x2 for x2 in categorical_data_type)\n",
    "                    if data_type_condition:\n",
    "                        print(col)\n",
    "                        data[col].fillna(value=data[col].mode()[0],inplace=True)\n",
    "                        \n",
    "        else:\n",
    "            data = data.fillna(data.value_counts().index[0])\n",
    "            \n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - mode')\n",
    "        \n",
    "    # Impute missing values with KNN            \n",
    "\n",
    "def impute_missing_values_knn(data,k=2):\n",
    "    try:\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            missing_cols = data.columns[data.isnull().any()]\n",
    "            if len(missing_cols)>0:\n",
    "                    imputer =KNNImputer(n_neighbors=k)\n",
    "                    data = pd.DataFrame(imputer.fit_transform(data),columns=data.columns) \n",
    "        else:\n",
    "            imputer =KNNImputer(n_neighbors=k)\n",
    "            data =pd.DataFrame( (imputer.fit_transform(np.array(data).reshape(1,-1))).reshape(-1,1),columns=[data.name])                    \n",
    "\n",
    "        return data  \n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - knn')\n",
    "\n",
    "#Impute missing values with Interpolate\n",
    "def impute_missing_values_interpolate(data):\n",
    "    try:\n",
    "        data_filled = data.interpolate()\n",
    "            \n",
    "        return data_filled\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('Error encountered in imputing missing values - interpolate')\n",
    "\n",
    "def ask_for_data_imputation_parameters(parameters_to_ask_for: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"asks for user parameters for data imputation\"\"\"\n",
    "    params_retrieved_from_user = {}\n",
    "    for param_name in parameters_to_ask_for:\n",
    "        param = input(f\"please specify {param_name} value:\\n\")\n",
    "        params_retrieved_from_user[param_name] = param\n",
    "        \n",
    "    return params_retrieved_from_user\n",
    "\n",
    "class ImputationMethods(Enum):\n",
    "    MEAN_IMPUTATION = (partial(impute_missing_values_mean), QuantitativeDataType, None)\n",
    "    MODE_IMPUTATION = (partial(impute_missing_values_mode), CategoricalDataType, None)\n",
    "    KNN_IMPUTATION = (partial(impute_missing_values_knn), CategoricalDataType, ['k'])\n",
    "    INTERPOLATION_IMPUTATION = (partial(impute_missing_values_interpolate), QuantitativeDataType, None)\n",
    "    \n",
    "    def __init__(self, method: Callable,\n",
    "                 data_type: Enum,\n",
    "                 parameters_to_ask_user: List[str]):\n",
    "        self._method = method\n",
    "        self._data_type = data_type\n",
    "        self._parameters_to_ask_user = parameters_to_ask_user\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        \"\"\"method avoiding to specify `value` when using an enum class\"\"\"\n",
    "        self.value(*args)\n",
    "\n",
    "    \n",
    "    def method(self, *args):\n",
    "        val = self._method(*args)\n",
    "        return val\n",
    "    \n",
    "    @property\n",
    "    def data_type(self):\n",
    "        return self._data_type\n",
    "    \n",
    "    @property\n",
    "    def parameters_to_ask_user(self):\n",
    "        return self._parameters_to_ask_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a158a595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    3\n",
       "2    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImputationMethods.MEAN_IMPUTATION.method(pd.Series([1, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4342ca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImputationMethods.KNN_IMPUTATION.parameters_to_ask_user is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "800e755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Please select the following method for filling missing values (if some are found)\\n1) MODE_IMPUTATION\\n2) KNN_IMPUTATION\\n3) No method\\n',\n",
       " {'1': <ImputationMethods.MODE_IMPUTATION: (functools.partial(<function impute_missing_values_mode at 0x7fba57063af0>), <enum 'CategoricalDataType'>, None)>,\n",
       "  '2': <ImputationMethods.KNN_IMPUTATION: (functools.partial(<function impute_missing_values_knn at 0x7fba57063b80>), <enum 'CategoricalDataType'>, ['k'])>,\n",
       "  '3': None})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLI for clinicians for setting up data format file\n",
    "\n",
    "get_data_imputation_methods_msg(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b6238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_imputation_methods_msg(d_type: type = None) -> Tuple[str, Dict[str, Enum]]:\n",
    "    msg = 'Please select the following method for filling missing values (if some are found)\\n'\n",
    "    \n",
    "    #available_methods = [method for ethod in ImputationMethods]\n",
    "    select_action = {}\n",
    "    i = 1\n",
    "    \n",
    "    for  imput_method in ImputationMethods:\n",
    "        if d_type is not None:\n",
    "            \n",
    "            is_d_type_in_sub_type = check_data_type_consistancy(imput_method.data_type, \n",
    "                                                               d_type)\n",
    "            if not is_d_type_in_sub_type:\n",
    "                continue # data type doesnot match method amputation requirments\n",
    "        msg += '%d) %s\\n' % (i, imput_method.name)\n",
    "        select_action[str(i)] = imput_method\n",
    "        i += 1\n",
    "\n",
    "    # ignore key\n",
    "    msg += '%d) No method\\n' % i \n",
    "    select_action[str(i)] = None\n",
    "    return msg, select_action\n",
    "    \n",
    "def no_methods(*kwargs):\n",
    "    return None\n",
    "\n",
    "    \n",
    "    \n",
    "def get_from_user_dataframe_format_file(dataset: pd.DataFrame) -> Dict[str, Any]:\n",
    "    ##\n",
    "    # variable initialisation\n",
    "    data_format_file = {}\n",
    "    \n",
    "    dataset_columns = dataset.columns\n",
    "    dataset_columns_length = len(dataset_columns)\n",
    "    \n",
    "    \n",
    "    available_data_type = [d_type for d_type in DataType]  # get all available data types\n",
    "    \n",
    "    for n_feature, feature in enumerate(dataset_columns):\n",
    "        print(f'displaying first 10 values of feature {feature} (n_feature: {n_feature+1}/{dataset_columns_length})') \n",
    "        #_file_name = os.path.basename(tabular_data_file)\n",
    "        #data_format_files[_file_name] = data_format_file\n",
    "        #print(tabulate(dataset[feature].head(10).values()))\n",
    "        pprint.pprint(dataset[feature].head(10))  # print first 10 lines of feature value\n",
    "        print(f'number of differents samples: {unique(dataset[feature], number=True)} / total of samples: {dataset[feature].shape[0]}')\n",
    "        \n",
    "        msg_data_type_selection, ignoring_id = get_data_type_selection_msg(available_data_type)\n",
    "        msg_data_type_selection = f'specify data type for {feature}:\\n' + msg_data_type_selection\n",
    "        \n",
    "        # ask user about data type\n",
    "        data_format_id = get_user_input(msg_data_type_selection,\n",
    "                                       \n",
    "                                       n_answers=ignoring_id)\n",
    "        \n",
    "        if int(data_format_id) > ignoring_id - 1:\n",
    "            # case where user decide to ingore column: go to next iteration (next feature)\n",
    "            print(f\"Ignoring feature {feature}\")\n",
    "            continue\n",
    "        else:\n",
    "            # case where user selected a data type: add data type and info to the format file\n",
    "            data_format = available_data_type[int(data_format_id)-1]\n",
    "            d_type = dataset[feature].dtype  \n",
    "            # TODO: rename data_type into d_type for consistancy sake\n",
    "            n_data_type, types = get_data_type(data_format, d_type)\n",
    "            \n",
    "        # KEY and DATETIME type \n",
    "        if data_format is DataType.KEY or data_format is DataType.DATETIME:  \n",
    "            # for these data type, missing values are disabled by default\n",
    "            is_missing_values_allowed = False\n",
    "        else: \n",
    "            # ask user if missing values are allowed for this specific variable\n",
    "            ## message definition\n",
    "            msg_yes_or_no_question = get_yes_no_msg()\n",
    "            msg_data_imputation_methods, data_imputation_methods = get_data_imputation_methods_msg(d_type=d_type)\n",
    "            n_data_imputation_method = len(data_imputation_methods)\n",
    "            msg_yes_or_no_question = f'Allow {feature} to have missing values:\\n' + msg_yes_or_no_question\n",
    "            \n",
    "            missing_values_user_selection = get_user_input(msg_yes_or_no_question,\n",
    "                                                        n_answers=2)\n",
    "            is_missing_values_allowed = parse_yes_no_msg(missing_values_user_selection)\n",
    "            \n",
    "            amputation_method = None\n",
    "            amputation_method_parameters = None\n",
    "            if is_missing_values_allowed:\n",
    "                # let user select amputation method if missing data are allowed\n",
    "                amputation_method_user_selection = get_user_input(msg_data_imputation_methods,\n",
    "                                                                n_answers=n_data_imputation_method)\n",
    "                \n",
    "                amputation_method_selected = data_imputation_methods.get(amputation_method_user_selection)\n",
    "                \n",
    "                if amputation_method_selected is not None:\n",
    "                    amputation_method = amputation_method_selected.name\n",
    "                if amputation_method_selected.parameters_to_ask_user is not None:\n",
    "                    print(f'Selected: {amputation_method}\\n')\n",
    "                    amputation_method_parameters = ask_for_data_imputation_parameters(amputation_method_selected.parameters_to_ask_user)\n",
    "                    print('amput param', amputation_method_parameters)\n",
    "                    \n",
    "        data_format_file[feature] = {'data_format': data_format.name,\n",
    "                                     'data_type': n_data_type.name,\n",
    "                                     'values': str(d_type),\n",
    "                                     'is_missing_values': is_missing_values_allowed,\n",
    "                                     'data_amputation_method': amputation_method,\n",
    "                                     'data_amputation_parameters': amputation_method_parameters\n",
    "                                    }\n",
    "        \n",
    "    return data_format_file\n",
    "            \n",
    "def get_user_input(msg:str,  n_answers:int) -> str:\n",
    "    \"\"\"\"\"\"\n",
    "    is_column_parsed = False\n",
    "    while not is_column_parsed:\n",
    "        #data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "        resp = input(msg)\n",
    "        if resp.isdigit() and int(resp) <= n_answers and int(resp)>0:\n",
    "            # check if value passed by user is correct (if it is integer,\n",
    "            # and whithin range [1, n_available_data_type])\n",
    "            is_column_parsed = True\n",
    "\n",
    "        else:\n",
    "            print(f'error ! {resp} value not understood')\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "185958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLI to use when dataset is available\n",
    "\n",
    "\n",
    "def get_from_user_multi_view_dataset_fromat_file(datasets: Dict[str, pd.DataFrame])-> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "    data_format_files = {}\n",
    "    \n",
    "    for tabular_data_file in datasets.keys():\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(f\"+++++++ Now parsing view: {tabular_data_file} +++++++\")\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        data_format_file = get_from_user_dataframe_format_file(datasets[tabular_data_file])\n",
    "        if data_format_file:\n",
    "            # (above condition avoids adding empty views)\n",
    "            _file_name = os.path.basename(tabular_data_file)\n",
    "            data_format_files[_file_name] = data_format_file\n",
    "        \n",
    "    return data_format_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8acbe629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_view_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6985/552618136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_view_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'single_view_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data_format_file = get_from_user_multi_view_dataset_fromat_file(single_view_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec5f1",
   "metadata": {},
   "source": [
    "data_fromat_ref (read only)\n",
    "\n",
    "CLI editer data_format_file\n",
    "\n",
    "review : \n",
    "- specify lower / upper bound NUMERICAL\n",
    "- Specify categorical (BOOLEAN, CHARACTER, NUMERICAL)\n",
    "\n",
    "- save different categorical values \n",
    " a posteriori ex SEX -> male or female, NOT FEMALE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b874ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: file1 +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature a (n_feature: 1/18)\n",
      "0    48\n",
      "1    87\n",
      "2    46\n",
      "3    84\n",
      "4    94\n",
      "5    18\n",
      "6    15\n",
      "7    30\n",
      "8    54\n",
      "9    46\n",
      "Name: a, dtype: int64\n",
      "number of differents samples: 57 / total of samples: 100\n",
      "specify data type for a:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "2\n",
      "QuantitativeDataType.DISCRETE int64\n",
      "QuantitativeDataType.DISCRETE | [dtype('int64')]\n",
      "Allow a to have missing values:\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Please select the following method for filling missing values (if some are found)\n",
      "1) MEAN_IMPUTATION\n",
      "2) MODE_IMPUTATION\n",
      "3) KNN_IMPUTATION\n",
      "4) INTERPOLATION_IMPUTATION\n",
      "5) No method\n",
      "3\n",
      "method ImputationMethods.KNN_IMPUTATION\n",
      "please specify k value:\n",
      "6\n",
      "amput param {'k': '6'}\n",
      "{'a': {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True, 'data_amputation_method': None, 'data_amputation_parameters': {'k': '6'}}}\n",
      "displaying first 10 values of feature e (n_feature: 2/18)\n",
      "0    98\n",
      "1    83\n",
      "2    73\n",
      "3    45\n",
      "4    84\n",
      "5     5\n",
      "6    44\n",
      "7    55\n",
      "8    37\n",
      "9     8\n",
      "Name: e, dtype: int64\n",
      "number of differents samples: 65 / total of samples: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6985/3532817470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_data_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_view_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6985/1879657107.py\u001b[0m in \u001b[0;36mget_from_user_multi_view_dataset_fromat_file\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"+++++++ Now parsing view: {tabular_data_file} +++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_dataframe_format_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtabular_data_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# (above condition avoids adding empty views)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6985/3533323687.py\u001b[0m in \u001b[0;36mget_from_user_dataframe_format_file\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# ask user about data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         data_format_id = get_user_input(msg_data_type_selection,\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                        n_answers=ignoring_id)\n",
      "\u001b[0;32m/tmp/ipykernel_6985/3533323687.py\u001b[0m in \u001b[0;36mget_user_input\u001b[0;34m(msg, n_answers)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_column_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#data_format_id = input(f'specify data type for {feature}:\\n' + msg )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_answers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# check if value passed by user is correct (if it is integer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "multi_data_format_file = get_from_user_multi_view_dataset_fromat_file(multi_view_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7777bb9",
   "metadata": {},
   "source": [
    "multi_data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e57d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved at multi_format_file\n"
     ]
    }
   ],
   "source": [
    "save_format_file_ref(multi_data_format_file, 'multi_format_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76cb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data format file\n",
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "with open(json_file_name, \"w\") as format_file:\n",
    "    json.dump(data_format_file, format_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]], path: str):\n",
    "    # save `format_file_ref` into a JSON file\n",
    "    with open(path, \"w\") as format_file:\n",
    "        json.dump(format_file_ref, format_file)\n",
    "    print(f\"Model successfully saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f6451c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10454/3457597443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_format_file_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# retrieve data format file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mformat_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mformat_file_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_file_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def load_format_file_ref(path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    # retrieve data format file\n",
    "    with open(path, \"r\") as format_file:\n",
    "        format_file_ref = json.load(format_file)\n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36e3dda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_format_file_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10454/3178173026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mformat_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_format_file_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_format_file_ref' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "\n",
    "format_file = load_format_file_ref(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82331c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31f18285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1)data_type\\n2) Values taken\\n3) Cancel Operation\\n', 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_msg_action_selection(DataTypeProperties.CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85f24f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_format': 'DATETIME',\n",
       " 'values': [\"<class 'pandas._libs.tslibs.timestamps.Timestamp'>\",\n",
       "  \"<class 'pandas._libs.tslibs.timedeltas.Timedelta'>\",\n",
       "  \"<class 'pandas._libs.tslibs.period.Period'>\",\n",
       "  \"<class 'datetime.datetime'>\",\n",
       "  \"<class 'numpy.datetime64'>\"]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for_data_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603615e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fca66943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from dateutil.parser._parser import ParserError\n",
    "\n",
    "\n",
    "def create_msg_action_selection(data_type_propreties: Enum) -> Tuple[str, int, Dict[int, Callable]]:\n",
    "    # create edit selection message for user given data type\n",
    "    # number of possible action depend of data type properties\n",
    "    msg = \"\"\n",
    "    action_counter = 1\n",
    "    actions = {}\n",
    "    \n",
    "    # data type change command\n",
    "    msg += \"%d) data_type\\n\" % action_counter\n",
    "    actions[str(action_counter)] = ask_for_data_type\n",
    "    action_counter += 1\n",
    "    \n",
    "    if data_type_propreties.lower_bound:\n",
    "        # lower bound edit command\n",
    "        msg += \"%d) lower bound\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_lower_bound\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.upper_bound:\n",
    "        # upper bound edit command\n",
    "        msg += \"%d)upper bound\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_upper_bound\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.set_of_values:\n",
    "        # value taken edit command\n",
    "        msg += \"%d) Values taken\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_categorical_values\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.date_format:\n",
    "        # date formatter edit command\n",
    "        msg += \"%d) Date format\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_date_format\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.allow_missing_values:\n",
    "        # change data method for data imputation\n",
    "        msg += \"%d) Data Value amputation method\\n\" % action_counter\n",
    "        actions[str(action_counter)] = ask_for_data_amputation_method\n",
    "        action_counter += 1\n",
    "        \n",
    "    msg += \"%d) Cancel Operation\\n\" % action_counter\n",
    "    actions[str(action_counter)] = cancel_operation\n",
    "    return msg, action_counter, actions\n",
    "\n",
    "def select_action(\n",
    "                  action: str,\n",
    "                  possible_actions: Dict[str, Callable],\n",
    "                  ) -> Tuple[Dict[str, Any], bool]:\n",
    "    \n",
    "    # variable initialization\n",
    "    is_cancelled = False\n",
    "    new_field = None\n",
    "    _action_counter = 1  # integer for dynamic cli management\n",
    "    print('action', action, str(len(possible_actions.keys())),possible_actions)\n",
    "    if action == str(len(possible_actions.keys())):\n",
    "        is_cancelled = True\n",
    "        \n",
    "    else:\n",
    "        # define action among the pool of possible actions\n",
    "        _action_func = possible_actions[action]\n",
    "        new_field = _action_func()\n",
    "    \n",
    "    return new_field, is_cancelled\n",
    "\n",
    "\n",
    "def isfloat(value:str) ->bool:\n",
    "    \"\"\"checks if string represents a float or int\"\"\"\n",
    "    is_float = True\n",
    "    try:\n",
    "        float(value)\n",
    "    except ValueError as e:\n",
    "        is_float = False\n",
    "    return is_float\n",
    "\n",
    "\n",
    "\n",
    "def is_datetime(date: str) -> bool:\n",
    "    \"\"\"checks if date is a date\"\"\"\n",
    "    is_date_parsable = True\n",
    "    try:\n",
    "        dateutil.parser.parse(date)\n",
    "    except (ParserError, ValueError) as err:\n",
    "        is_date_parsable = False\n",
    "        \n",
    "    return is_date_parsable\n",
    "\n",
    "def cancel_operation():\n",
    "    print(\"operation cancelled\")\n",
    "        \n",
    "def ask_for_lower_bound() -> Dict[str, float]:\n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        lower_bound = input('enter lower bound')\n",
    "        if isfloat(lower_bound) or is_datetime(lower_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "        else:\n",
    "            print('Value not a Number! please retry')\n",
    "    return {'lower_bound': float(lower_bound)}\n",
    "\n",
    "def ask_for_upper_bound() -> Dict[str, float]:\n",
    "    \n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        upper_bound = input('enter upper bound')\n",
    "        if isfloat(upper_bound) or is_datetime(upper_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "        else:\n",
    "            print('Value not a Number! please retry')\n",
    "    return {'upper_bound': float(upper_bound)}\n",
    "\n",
    "\n",
    "def _ask_for_data_type(data_type: Enum) -> Enum:\n",
    "    \"\"\"asks user for datatype contains in `data_type`\n",
    "    If user selects `cancel`, it will return None\n",
    "    \"\"\"\n",
    "    \n",
    "    _available_data_type = [t for t in data_type]  # get all keys contain in data_type\n",
    "    n_avail_data_type = len(_available_data_type)\n",
    "    msg, _n_answer = get_data_type_selection_msg(data_type, ign_msg=\"cancel operation\")\n",
    "    data_type_selection = get_user_input(msg, _n_answer)\n",
    "    \n",
    "    if str(_n_answer) != data_type_selection:\n",
    "        return _available_data_type[int(data_type_selection) - 1]\n",
    "    \n",
    "    \n",
    "def ask_for_data_type() -> Dict[str, Any]:\n",
    "\n",
    "    updates = None\n",
    "    \n",
    "    new_data_format = _ask_for_data_type(DataType)\n",
    "    \n",
    "    if new_data_format is not None: \n",
    "        # case where 'cancel operation' hasnot been selected\n",
    "                \n",
    "        updates = {'data_format': new_data_format.name}\n",
    "        if  isinstance(next(iter(new_data_format.value)), Enum):\n",
    "            # if subtypes are available\n",
    "            new_data_type= _ask_for_data_type(new_data_format.value)\n",
    "            new_values = list(map(lambda x: str(x), new_data_type.value))\n",
    "            updates.update({'data_type': new_data_type, 'values': new_values})\n",
    "        else:\n",
    "            new_values = list(map(lambda x: str(x), new_data_format.value))\n",
    "            updates.update({'values': new_values})\n",
    "    return updates\n",
    "\n",
    "\n",
    "def ask_for_data_amputation_method(d_type: type=None) -> Dict[str, str]:\n",
    "    \n",
    "    # variable initialisation\n",
    "    _amputation_method = None\n",
    "    _amputation_method_parameters = None\n",
    "    \n",
    "    # get user message + dictionary mapping user responses to data amputation methods\n",
    "    _msg_data_imputation_methods, _data_imputation_methods = get_data_imputation_methods_msg(d_type)\n",
    "    \n",
    "    # ask for user selection\n",
    "    _amputation_method_user_selection = get_user_input(_msg_data_imputation_methods,\n",
    "                                                      n_answers=len(_data_imputation_methods))\n",
    "    \n",
    "    # select user data amputation method given user command\n",
    "    _amputation_method_selected = _data_imputation_methods.get(_amputation_method_user_selection)\n",
    "\n",
    "    if _amputation_method_selected is not None:\n",
    "        _amputation_method = _amputation_method_selected.name\n",
    "        \n",
    "        if _amputation_method_selected.parameters_to_ask_user is not None:\n",
    "            print(f'Method amputation selected: {_amputation_method}\\n')\n",
    "            _amputation_method_parameters = ask_for_data_imputation_parameters(_amputation_method_selected.parameters_to_ask_user)\n",
    "            print('amput param', _amputation_method_parameters)\n",
    "    updates = {'data_amputation_method': _amputation_method,\n",
    "               'data_amputation_parameters': _amputation_method_parameters}\n",
    "    return updates\n",
    "\n",
    "def ask_for_categorical_values() -> Dict[str, Any]:\n",
    "    possible_values = input('enter possible values (separated by \",\")')\n",
    "    possible_values = possible_values.split(\",\")  # separate values passed by user into a list\n",
    "    return {'categorical_values': possible_values}\n",
    "\n",
    "\n",
    "def ask_for_date_format() -> Dict[str, Any]:\n",
    "    # TODO : ask for date format (UTC, ....)\n",
    "    msg = 'please enter date format:\\n1)timetsamp\\n2)ISO date format (YYYY-MM-DD)\\n3)custom date format\\n'\n",
    "    user_selection = input(msg)\n",
    "    # default date format\n",
    "    msg = {'1': 'timestamp',\n",
    "          '2': '(American default date format) mm/dd/yy',\n",
    "          '3': '(Europeen default date format) dd/mm/yy',\n",
    "           '4': 'ISO date format (YYYY-MM-DD)',\n",
    "          '5': 'custom date format',\n",
    "          '6': 'select timezone'}\n",
    "    pass\n",
    "\n",
    "def edit_feature_format_file_ref(feature_content: Dict[str, Any],\n",
    "                                  feature_name: str,\n",
    "                                  available_categorical_data_type: List[Enum],\n",
    "                                  messages: Dict[str, str],\n",
    "                                  ignore_keystroke: int) -> Dict[str, Any]:\n",
    "    \"\"\"Edits a specific feature that belongs to a specific view within a format file\"\"\"\n",
    "    \n",
    "\n",
    "    _is_feature_unparsed = True  \n",
    "    _is_cancelled = False  # whether parsing of current column has been cancelled or not\n",
    "    _is_first_edit = True\n",
    "    _avail_data_type_properties = [dtype for dtype in DataTypeProperties]\n",
    "    \n",
    "    # iterate over number of feature contained in view, and ask for each feature if changes are needed\n",
    "    while _is_feature_unparsed:\n",
    "        if _is_cancelled or not _is_first_edit:\n",
    "            _f_answer = True\n",
    "        else:\n",
    "            _f_answer = get_user_input(f\"Edit variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "            # ask if user wants to edit feature variables\n",
    "            _f_answer = parse_yes_no_msg(_f_answer)\n",
    "            _is_operation_cancelled = False  # for cancelling feature edition\n",
    "            _is_first_edit = False\n",
    "        if _f_answer:\n",
    "            # case where user wants to edit the current feature\n",
    "            \n",
    "            _msg = messages['edit']\n",
    "            \n",
    "            data_format = feature_content.get('data_format')\n",
    "            for data_type_properties in _avail_data_type_properties:\n",
    "                if data_format == data_type_properties.name:\n",
    "                    # get data property from data_format\n",
    "                    select_msg, n_actions, possible_actions = create_msg_action_selection(data_type_properties)\n",
    "                    _msg += select_msg\n",
    "                    _edit_selection = get_user_input(_msg, n_actions)\n",
    "\n",
    "                    _edited_field, _is_cancelled = select_action(\n",
    "                                                                  _edit_selection,\n",
    "                                                                  possible_actions\n",
    "                                                                  #available_categorical_data_type,\n",
    "                                                                  #messages['data_type_select']\n",
    "                                                                )\n",
    "\n",
    "            if not _is_cancelled:\n",
    "                # if user has not cancelled field edition\n",
    "                if _edited_field is not None:\n",
    "                    feature_content.update(_edited_field)\n",
    "             \n",
    "                _c_answer = get_user_input(f\"Continue Editing variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "                _is_feature_unparsed = parse_yes_no_msg(_c_answer)\n",
    "            else:\n",
    "                _is_feature_unparsed = False\n",
    "                \n",
    "        else:\n",
    "            _is_feature_unparsed = False\n",
    "            \n",
    "    return feature_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed0d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \n",
    "    # CLI for editing `format_file_ref`, a file containing information about each variable\n",
    "    # in a tabular dataset\n",
    "    print(f'Now editing format file ref')\n",
    "    \n",
    "    ## variables initialization\n",
    "    available_categorical_data_types = [t for t in CategoricalDataType]\n",
    "    _file_names = list(format_file_ref.keys())\n",
    "    _n_tot_files = len(_file_names)\n",
    "    \n",
    "    ## messages definition\n",
    "    _data_type_selection_msg, ign_key = get_data_type_selection_msg(available_categorical_data_types)\n",
    "    \n",
    "    _messages = {\n",
    "        'yes_or_no': get_yes_no_msg(),\n",
    "        'edit': 'Which field should be modified?\\n',\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # iterate over name of files (ie views)\n",
    "    for i_file in range(_n_tot_files):\n",
    "        # ask for each file if user wants to edt it\n",
    "        _answer = get_user_input(f\"Edit file: {_file_names[i_file]}?\\n\" + _messages['yes_or_no'], 2)\n",
    "        _answer = parse_yes_no_msg(_answer)\n",
    "        \n",
    "        if _answer:\n",
    "            # case where user wants to modify current view scheme\n",
    "            _file_content = format_file_ref[_file_names[i_file]]  # get file (ie view) content\n",
    "            \n",
    "            ## variables initialization for parsing current view\n",
    "            _features_names = list(_file_content.keys())\n",
    "            _n_tot_feature = len(_features_names)\n",
    "            \n",
    "            # iterate over features found in view\n",
    "            for i_feature in range(_n_tot_feature):\n",
    "                feature_name = _features_names[i_feature]\n",
    "                feature_content = _file_content[feature_name]\n",
    "                feature_content = edit_feature_format_file_ref(feature_content,\n",
    "                                                               feature_name,\n",
    "                                                               available_categorical_data_types,\n",
    "                                                               _messages,\n",
    "                                                               ign_key)\n",
    "            format_file_ref[_file_names[i_file]].update({feature_name: feature_content})\n",
    "            \n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ef36608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': [\"<class 'bool'>\"],\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': ['1', ' 2', ' 4']},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False,\n",
       "   'lower_bound': 0.0},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_format_file_ref(format_file)\n",
    "\n",
    "# TODO: create possiblity for user to update data type when editing format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f519aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now editing format file ref\n",
      "Edit file: file1?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Edit variable: e?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1) data_type\n",
      "2) lower bound\n",
      "3)upper bound\n",
      "4) Data Value amputation method\n",
      "5) Cancel Operation\n",
      "4\n",
      "action 4 5 {'1': <function ask_for_data_type at 0x7fba5611e550>, '2': <function ask_for_lower_bound at 0x7fba55aaeca0>, '3': <function ask_for_upper_bound at 0x7fba55aae5e0>, '4': <function ask_for_data_amputation_method at 0x7fba55aae670>, '5': <function cancel_operation at 0x7fba55aae9d0>}\n",
      "Please select the following method for filling missing values (if some are found)\n",
      "1) MEAN_IMPUTATION\n",
      "2) MODE_IMPUTATION\n",
      "3) KNN_IMPUTATION\n",
      "4) INTERPOLATION_IMPUTATION\n",
      "5) No method\n",
      "5\n",
      "Continue Editing variable: e?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1) data_type\n",
      "2) lower bound\n",
      "3)upper bound\n",
      "4) Data Value amputation method\n",
      "5) Cancel Operation\n",
      "4\n",
      "action 4 5 {'1': <function ask_for_data_type at 0x7fba5611e550>, '2': <function ask_for_lower_bound at 0x7fba55aaeca0>, '3': <function ask_for_upper_bound at 0x7fba55aae5e0>, '4': <function ask_for_data_amputation_method at 0x7fba55aae670>, '5': <function cancel_operation at 0x7fba55aae9d0>}\n",
      "Please select the following method for filling missing values (if some are found)\n",
      "1) MEAN_IMPUTATION\n",
      "2) MODE_IMPUTATION\n",
      "3) KNN_IMPUTATION\n",
      "4) INTERPOLATION_IMPUTATION\n",
      "5) No method\n",
      "2\n",
      "Continue Editing variable: e?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: 1?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: 2?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: time?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: pressure?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: e.1?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: gender?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: blood type?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: pkey?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: contatct?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: file2?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True,\n",
       "   'data_amputation_method': 'MODE_IMPUTATION',\n",
       "   'data_amputation_parameters': None},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref = load_format_file_ref('multi_format_file')\n",
    "edit_format_file_ref(multi_format_file_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ba372",
   "metadata": {},
   "source": [
    "## tabular data sanity check using file format ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2a9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "\n",
    "\n",
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Tuple[Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    # check for each variable name existing in one view, that it doesnot exist in another\n",
    "    # view. if it is, rename both variables\n",
    "    # for this purpose, parse every combination once\n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "\n",
    "def join_muti_view_dataset(multi_view_dataset: pd.DataFrame,\n",
    "                           primary_key: str=None,\n",
    "                          as_multi_index: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    \n",
    "    _views_names = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "    \n",
    "    joined_dataframe = multi_view_dataset[_views_names[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_names)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_names[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_names[x]))\n",
    "    \n",
    "    if as_multi_index:\n",
    "        # convert plain dataframe into multi index dataframe\n",
    "        # primary key will have its own view\n",
    "        _header_labels = ['views', 'feature_name']\n",
    "        _primary_key_label = 'primary_key'\n",
    "        \n",
    "        _multi_index = multi_view_dataset.columns\n",
    "        _key_values = joined_dataframe[primary_key].values  # storing primary key\n",
    "\n",
    "        _all_features_names = []\n",
    "        _new_views_names = []\n",
    "        for view_name in _views_names:\n",
    "            # get all columns name for each view, and remove primary key\n",
    "            _features_names = list(multi_view_dataset[view_name].columns)\n",
    "            _features_names.remove(primary_key)\n",
    "            _all_features_names.extend(_features_names)\n",
    "\n",
    "            for feature_name in _features_names:\n",
    "                _new_views_names.append(view_name)\n",
    "                # appending as much as there are feature within each view\n",
    "            #features_name[name].remove(primary_key)\n",
    "\n",
    "        _header = pd.MultiIndex.from_arrays([ _new_views_names, _all_features_names],\n",
    "                                            names=_header_labels)\n",
    "        joined_dataframe  = pd.DataFrame(joined_dataframe[_all_features_names].values, columns=_header)\n",
    "        joined_dataframe[_primary_key_label, primary_key] = _key_values\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"\"\"\"\n",
    "    _views_names = list(format_file_ref.keys())\n",
    "    primary_key = None\n",
    "    _c_view = None\n",
    "    for view_name in _views_names:\n",
    "        file_content = format_file_ref[view_name]\n",
    "        _features_names = list(file_content.keys())\n",
    "        for feature_name in _features_names:\n",
    "            feature_content  = file_content[feature_name]\n",
    "            _d_format = feature_content.get('data_format')\n",
    "            \n",
    "            if _d_format == DataType.KEY.name:\n",
    "                if _c_view is None:\n",
    "                    primary_key = feature_name\n",
    "                    _c_view = view_name\n",
    "                    print(f'found primary key {primary_key}')\n",
    "                else:\n",
    "                    print(f'error: found 2 primary keys is same view {view_name}')\n",
    "        _c_view = None\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "\n",
    "def select_data_from_format_file_ref(datasets: Dict[str, Dict[str, Any]],\n",
    "                                     format_file: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"returns an updated dataset containing only the features detailed in format_file\"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    updated_dataset = {}\n",
    "    _views_format_file = list(format_file.keys())\n",
    "    \n",
    "    for view in _views_format_file:\n",
    "        if view in datasets.keys():\n",
    "            # only extract features from format_file\n",
    "            _format_file_features = list(format_file[view].keys())\n",
    "            _current_dataset_feature = datasets[view].columns.tolist()\n",
    "            try:\n",
    "                updated_dataset[view] = datasets[view][_format_file_features]\n",
    "            except KeyError as ke:\n",
    "                # catch error if a column is specified in data format file\n",
    "                # but not found in dataset\n",
    "                _missing_feature = []\n",
    "                for feature in _format_file_features:\n",
    "                    if feature not in _current_dataset_feature:\n",
    "                        _missing_feature.append(feature)\n",
    "                print('Error: th following features', *_missing_feature, f'are not found in view: {view}')\n",
    "        else:\n",
    "            # trigger error\n",
    "            print(f'error!: missing view {view} in dataset')\n",
    "            \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdc0e51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9584/340986651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68eedfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "df_to_check[n_feature_name]dataset_to_check = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fd657d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6392/468163090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract views names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mviews_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'format_file' is not defined"
     ]
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(format_file.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(dataset_to_check, format_file)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_to_check = join_muti_view_dataset(multi_df_to_check)\n",
    "    \n",
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15a12eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': [\"<class 'bool'>\"], 'is_missing_values': True, 'categorical_values': ['1', ' 2', ' 4']}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': True}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False, 'lower_bound': 0.0}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# sanity check on columns using format_file\n",
    "\n",
    "\n",
    "\n",
    "_views_format_file = list(format_file.keys())\n",
    "    \n",
    "for view in _views_format_file:\n",
    "    _features_format_file = format_file[view]\n",
    "    \n",
    "    for feature in _features_format_file:\n",
    "        feature_content = df_to_check[feature]\n",
    "        feature_format =  _features_format_file[feature]\n",
    "        # 1. check for datatype consistency\n",
    "        \n",
    "        data_types = find_data_type(feature_format['data_format'], feature_format['data_type'])\n",
    "        \n",
    "        print(dtype[-1].value, feature_content.dtype, feature_format)\n",
    "        if any(t == feature_content.dtype for t in data_types[-1].value ):\n",
    "            print('ok')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e87a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    }
   ],
   "source": [
    "multi_format_file_ref = load_format_file_ref('multi_format_file')\n",
    "multi_dataset_to_check = load_tabular_datasets(r'test7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6f61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34813b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'CATEGORICAL',\n",
       " 'data_type': 'BOOLEAN',\n",
       " 'values': 'bool',\n",
       " 'is_missing_values': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file1']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "566d2128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found primary key pkey\n",
      "found primary key pkey\n",
      "found primary key pkey\n",
      "primary key pkey\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>pkey</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.088082</td>\n",
       "      <td>63</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>zmixzrgvxrjqxoe sluk</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>0.774788</td>\n",
       "      <td>20</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>vrzahnpfluspdcbfnaqt</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xkdawggpnuulcewuoyzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.514092</td>\n",
       "      <td>2</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.407279</td>\n",
       "      <td>khuulhwgwnjggrfoefce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>70</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>gwj luzejwdxzsiljxzd</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.536301</td>\n",
       "      <td>xxysdmwwmjsmyhaswfdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>90</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>jjdvcnofivbqhirxzdyo</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.749443</td>\n",
       "      <td>ldejfuij mnbnf wwmms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>0.295578</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>hrvepmqjn llgbzplshv</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrmdecb s pohtmrcdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>81</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.474322</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>B</td>\n",
       "      <td>wroevwyuamxibzshlxxh</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.388389</td>\n",
       "      <td>whmwrpvqmerdpwwzxasf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.927511</td>\n",
       "      <td>7</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>ywadcykylymkdtzfctpg</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.889067</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.494798</td>\n",
       "      <td>11</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>ruchbfa zwgenxslegrl</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>74</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>zeapltpxuvuibfybxcll</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.667349</td>\n",
       "      <td>kiejdmbuih awhuifwwd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name   e file1.1      2           file1.time  pressure e.1 gender  \\\n",
       "0             98    True  False  2018-01-01 00:00:00  0.088082  63    MAN   \n",
       "1             83   False   True  2018-01-01 01:00:00  0.774788  20    MAN   \n",
       "2             73   False  False  2018-01-01 02:00:00  0.514092   2  WOMAN   \n",
       "3             45    True   True  2018-01-01 03:00:00  0.832881  70  WOMAN   \n",
       "4             84    True  False  2018-01-01 04:00:00  0.696152  90    MAN   \n",
       "..            ..     ...    ...                  ...       ...  ..    ...   \n",
       "95            66    True   True  2018-01-04 23:00:00  0.295578  41  WOMAN   \n",
       "96            81   False   True  2018-01-05 00:00:00  0.474322  41  WOMAN   \n",
       "97            82    True   True  2018-01-05 01:00:00  0.927511   7    MAN   \n",
       "98            18   False   True  2018-01-05 02:00:00  0.494798  11    MAN   \n",
       "99            70   False   True  2018-01-05 03:00:00  0.316395  74  WOMAN   \n",
       "\n",
       "feature_name blood type                  pkey discrete       city  \\\n",
       "0                     A  zmixzrgvxrjqxoe sluk     64.0      Lille   \n",
       "1                     O  vrzahnpfluspdcbfnaqt     26.0      Lille   \n",
       "2                     A  pnrepvmrxqabdlvisclv     61.0      Paris   \n",
       "3                    AB  gwj luzejwdxzsiljxzd     29.0      Paris   \n",
       "4                     B  jjdvcnofivbqhirxzdyo     99.0      Lille   \n",
       "..                  ...                   ...      ...        ...   \n",
       "95                    A  hrvepmqjn llgbzplshv      9.0      Paris   \n",
       "96                    B  wroevwyuamxibzshlxxh     98.0  Marseille   \n",
       "97                    B  ywadcykylymkdtzfctpg     21.0      Lille   \n",
       "98                    O  ruchbfa zwgenxslegrl     42.0  Marseille   \n",
       "99                   AB  zeapltpxuvuibfybxcll      3.0      Paris   \n",
       "\n",
       "feature_name                  pkey file2.1           file2.time        pH  \\\n",
       "0             qpqorfhylu gmfjy bdj    True  2018-01-01 00:00:00  0.023107   \n",
       "1             kkmjozalfyirgsire ui   False  2018-01-01 01:00:00       NaN   \n",
       "2             ezfasuuycdda foisjte    True  2018-01-01 02:00:00  0.407279   \n",
       "3             faxiqkt xggzmwzoidbg    True  2018-01-01 03:00:00  0.536301   \n",
       "4             znwhlj rwzdutnagwasy    True  2018-01-01 04:00:00  0.749443   \n",
       "..                             ...     ...                  ...       ...   \n",
       "95            zeqhcikzdodus jn qjf    True  2018-01-04 23:00:00       NaN   \n",
       "96            iicthcvfmkajbvr gzir   False  2018-01-05 00:00:00  0.388389   \n",
       "97            ztjakcsk bhjoksdz lm    True  2018-01-05 01:00:00  0.889067   \n",
       "98             sabunaa opt vpulnxj    True  2018-01-05 02:00:00  0.402979   \n",
       "99            qmbexyexvgromrm admu   False  2018-01-05 03:00:00  0.667349   \n",
       "\n",
       "feature_name                  pkey  \n",
       "0             kkmjozalfyirgsire ui  \n",
       "1             xkdawggpnuulcewuoyzz  \n",
       "2             khuulhwgwnjggrfoefce  \n",
       "3             xxysdmwwmjsmyhaswfdb  \n",
       "4             ldejfuij mnbnf wwmms  \n",
       "..                             ...  \n",
       "95            wrmdecb s pohtmrcdj   \n",
       "96            whmwrpvqmerdpwwzxasf  \n",
       "97            pnrepvmrxqabdlvisclv  \n",
       "98            iicthcvfmkajbvr gzir  \n",
       "99            kiejdmbuih awhuifwwd  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(multi_format_file_ref.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(multi_format_file_ref)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(multi_dataset_to_check, multi_format_file_ref)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names,\n",
    "                                                             primary_key)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "multi_df_joined = join_muti_view_dataset(multi_df_to_check, primary_key)\n",
    "    \n",
    "df_to_check = multi_df_to_check.droplevel(0, axis=1)  # remove views from dataset\n",
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "52b6cc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>views</th>\n",
       "      <th colspan=\"2\" halign=\"left\">contatct</th>\n",
       "      <th colspan=\"8\" halign=\"left\">file1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">file2</th>\n",
       "      <th>primary_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.98667</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.78856</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "views        contatct            file1                                      \\\n",
       "feature_name discrete       city     e file1.1      2           file1.time   \n",
       "0                64.0      Lille    16   False  False  2018-01-03 04:00:00   \n",
       "1                26.0      Lille    96    True   True  2018-01-02 04:00:00   \n",
       "2                61.0      Paris     8    True   True  2018-01-01 09:00:00   \n",
       "3                29.0      Paris     6    True  False  2018-01-04 20:00:00   \n",
       "4                99.0      Lille    79    True   True  2018-01-04 09:00:00   \n",
       "..                ...        ...   ...     ...    ...                  ...   \n",
       "95                9.0      Paris    62    True   True  2018-01-02 13:00:00   \n",
       "96               98.0  Marseille    49   False   True  2018-01-02 21:00:00   \n",
       "97               21.0      Lille    14   False  False  2018-01-02 06:00:00   \n",
       "98               42.0  Marseille    10    True  False  2018-01-02 01:00:00   \n",
       "99                3.0      Paris    89    True   True  2018-01-03 22:00:00   \n",
       "\n",
       "views                                          file2                       \\\n",
       "feature_name  pressure e.1 gender blood type file2.1           file2.time   \n",
       "0              0.98667  98  WOMAN          A   False  2018-01-02 06:00:00   \n",
       "1             0.996889  35    MAN         AB    True  2018-01-01 00:00:00   \n",
       "2             0.777026  65    MAN          A   False  2018-01-02 10:00:00   \n",
       "3             0.877527  81    MAN         AB    True  2018-01-03 12:00:00   \n",
       "4             0.447389  88  WOMAN          O    True  2018-01-01 10:00:00   \n",
       "..                 ...  ..    ...        ...     ...                  ...   \n",
       "95            0.953184  53    MAN         AB   False  2018-01-02 05:00:00   \n",
       "96            0.442283  35    MAN        NaN    True  2018-01-05 02:00:00   \n",
       "97            0.988543  67    MAN          B   False  2018-01-01 12:00:00   \n",
       "98            0.059791  48    MAN          B    True  2018-01-02 09:00:00   \n",
       "99            0.939352  13    MAN          B   False  2018-01-03 02:00:00   \n",
       "\n",
       "views                            primary_key  \n",
       "feature_name        pH                  pkey  \n",
       "0                  NaN  qpqorfhylu gmfjy bdj  \n",
       "1             0.023107  kkmjozalfyirgsire ui  \n",
       "2             0.587685  ezfasuuycdda foisjte  \n",
       "3             0.894073  faxiqkt xggzmwzoidbg  \n",
       "4             0.026831  znwhlj rwzdutnagwasy  \n",
       "..                 ...                   ...  \n",
       "95             0.78856  zeqhcikzdodus jn qjf  \n",
       "96            0.402979  iicthcvfmkajbvr gzir  \n",
       "97                 NaN  ztjakcsk bhjoksdz lm  \n",
       "98            0.651801   sabunaa opt vpulnxj  \n",
       "99            0.751969  qmbexyexvgromrm admu  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07de11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e97829a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_key_variable_compliance(column: pd.Series,\n",
    "                                  \n",
    "                                  col_name:str=None,\n",
    "                                  warning=None) -> bool:\n",
    "    \"\"\"performs data sanity check over variable of type `KEY`\n",
    "    warning should be Critical warnings\n",
    "    \"\"\"\n",
    "    # variables initialisation\n",
    "    is_test_passed = True \n",
    "    \n",
    "    # 1. check unicity of values in column\n",
    "    \n",
    "    n_unique_samples = unique(column, number=True)\n",
    "    n_samples = column.shape[0]\n",
    "    \n",
    "    if n_unique_samples != n_samples:\n",
    "        is_test_passed = False\n",
    "        print(f'error: keys not unique ! b of samples= {n_samples} and unique values {n_unique_samples}')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "    # 2. check if missing database contained in key (key should not contain any missing data)\n",
    "    if check_missing_data(column):\n",
    "        is_test_passed = False\n",
    "        print('error: missing data found in key')\n",
    "    else:\n",
    "        \n",
    "        print('test 2 passed')\n",
    "\n",
    "                \n",
    "    return is_test_passed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ac12cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_to_check['file1.time'] > '2018-01-04 09:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81f47c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_to_check['file1.time'].apply(is_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce557ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "test 2 passed\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "test 5: passed\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['2'], multi_format_file_ref['file1']['2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "168b85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_format_file_ref['file1']['2'].update({'categorical_values': [True,False]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0b8e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'CATEGORICAL',\n",
       " 'data_type': 'BOOLEAN',\n",
       " 'values': 'bool',\n",
       " 'is_missing_values': True,\n",
       " 'categorical_values': [True, False]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file1']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variable_compliance(column: pd.Series,\n",
    "                               format_file_ref: Dict[str, Any],\n",
    "                               col_name:str=None,\n",
    "                               warning=None) -> Tuple[bool, bool]:\n",
    "    \"\"\"performs a data sanity check on variable `col_name` given instruction in \n",
    "    data_file_ref\n",
    "    \"\"\"\n",
    "    is_test_passed = True\n",
    "    \n",
    "    \n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    # remove nan (missing values) from \n",
    "    column_without_nan = column.dropna()\n",
    "    \n",
    "    \n",
    "    if data_format_name is None:\n",
    "        print(f'critical wraning: data fromat {data_format_name} not understood')\n",
    "    # 1. check data sub type\n",
    "    try:\n",
    "        data_type = find_data_type(data_format_name, data_type_name)\n",
    "    except ValueError as err:\n",
    "        data_type = None\n",
    "        print('Critical warning: data format and data type mismatch')\n",
    "    does_column_have_correct_data_type = any(t for t in data_type.value)\n",
    "    if not does_column_have_correct_data_type:\n",
    "        print(f'error: data type {column.dtype} doesnot have the data type specified in format reference file')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "\n",
    "    # 2. check if missing values are allowed\n",
    "    is_missing_data = check_missing_data(column)\n",
    "    is_missing_values_authorized = format_file_ref.get('is_missing_values', 'test_skipped')\n",
    "    if is_missing_values_authorized == 'test_skipped':\n",
    "        print('missing_value test skipped')\n",
    "    elif not is_missing_values_authorized and is_missing_data:\n",
    "\n",
    "        print('Error found missing data but missing data are not authorized')\n",
    "    else:\n",
    "        print('test 2 passed')\n",
    "    \n",
    "    \n",
    "    # 3. check lower bound\n",
    "    print(format_file_ref)\n",
    "    lower_bound = format_file_ref.get('lower_bound')\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        \n",
    "        # should work for both numerical and datetime data sets\n",
    "        \n",
    "        is_lower_bound_correct = np.all(column_without_nan >= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_lower_bound_correct:\n",
    "            print('Warning: found some data below lower bound')\n",
    "        else:\n",
    "            print('test 3 passed')\n",
    "    else:\n",
    "        print('test 3 skipped ')\n",
    "    # 4. check upper bound\n",
    "    upper_bound = format_file_ref.get('upper_bound')\n",
    "    if upper_bound is not None:\n",
    "         # should work for both numerical and datetime data sets\n",
    "        is_upper_bound_correct = np.all(column_without_nan <= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_upper_bound_correct:\n",
    "            print('Warning: found some data  above upper bound')\n",
    "        else:\n",
    "            print('test 4 passed')\n",
    "            \n",
    "    else:\n",
    "        print('test 4 skipped')\n",
    "    # 5. check if possible_values are contained in variable\n",
    "    categorical_values = format_file_ref.get('categorical_values')    \n",
    "    if categorical_values is None:\n",
    "        print('categorical value check test skipped')\n",
    "    else:\n",
    "        unique_values = unique(column)\n",
    "        _is_error_found = False\n",
    "        for val in unique_values:\n",
    "            if val not in categorical_values and not np.isnan(val):\n",
    "                print(f'critical warning: {val} not in possible values')\n",
    "                _is_error_found = True\n",
    "        if not _is_error_found:\n",
    "            print('test 5: passed')\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "def check_datetime_variable_compliance(column: pd.Series):\n",
    "    \"\"\"additional data sanity checks for datetime variable\"\"\"\n",
    "    # test 1. check if datetime is parsable\n",
    "    \n",
    "    # remove nan\n",
    "    column_without_nan = column.dropna()\n",
    "    are_datetime_parsables =  np.all(column.apply(is_datetime))\n",
    "    if not are_datetime_parsables:\n",
    "        print('Warning: at least one variable is not a datetime')\n",
    "        \n",
    "    else:\n",
    "        print('datetime parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c7bc10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': True, 'categorical_values': [True, False]}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "test 5: passed\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "contatct\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "file2\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "Error found missing data but missing data are not authorized\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False, 'lower_bound': -10}\n",
      "test 3 passed\n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n"
     ]
    }
   ],
   "source": [
    "# Data sanity check\n",
    "\n",
    "new_feature_name = { v: list(pre_parsed_dataset_to_check[v].columns) for v in views_names}\n",
    "new_feature_name\n",
    "\n",
    "for view in views_names:\n",
    "    print(view)\n",
    "    \n",
    "    feature_names = list(multi_format_file_ref[view].keys())\n",
    "    for n_feature_name, feature_name in zip(new_feature_name[view], feature_names):\n",
    "        check_variable_compliance(df_to_check[n_feature_name], multi_format_file_ref[view][feature_name])\n",
    "        data_format = multi_format_file_ref[view][feature_name].get('data_format')\n",
    "        if data_format == DataType.DATETIME.name:\n",
    "            # addtional check for DATETIME data format\n",
    "            check_datetime_variable_compliance(df_to_check[n_feature_name])\n",
    "            \n",
    "        if data_format == DataType.KEY.name:\n",
    "            check_key_variable_compliance(df_to_check[n_feature_name])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0946850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'QUANTITATIVE',\n",
       " 'data_type': 'CONTINUOUS',\n",
       " 'values': 'float64',\n",
       " 'is_missing_values': False,\n",
       " 'lower_bound': -10}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref['file2']['pH'].update({'lower_bound': -10})\n",
    "multi_format_file_ref['file2']['pH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "936c0a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'DATETIME',\n",
       " 'data_type': 'DATETIME',\n",
       " 'values': 'object',\n",
       " 'is_missing_values': False}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref[view][feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_data_type("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c760347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "Error found missing data but missing data are not authorized\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False, 'lower_bound': -10}\n",
      "test 3 passed\n",
      "test 4 skipped\n",
      "categorical value check test skipped\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['pH'], multi_format_file_ref['file2']['pH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b06bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "95    False\n",
       "96    False\n",
       "97    False\n",
       "98    False\n",
       "99    False\n",
       "Name: pH, Length: 100, dtype: bool"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check['pH'] > np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c94fb830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': [True, False]},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ee4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 passed\n",
      "test 2 passed\n",
      "categorical value check test skipped\n"
     ]
    }
   ],
   "source": [
    "check_variable_compliance(df_to_check['pkey'], multi_format_file_ref['file2']['pkey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ff5d5",
   "metadata": {},
   "source": [
    "## data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca423d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "887f090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) YES\\n2) NO\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_yes_or_no_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2460bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85714c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d161776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "ll\n",
      "please add new feature name:\n",
      "ll\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "4\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "kk\n",
      "please add new feature name:\n",
      "vkeof\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "is_views_finished = False\n",
    "\n",
    "\n",
    "views_format_file = {}\n",
    "\n",
    "while not is_views_finished:\n",
    "    is_features_finished = False\n",
    "    resp = input('do you want to add a new view (file)?\\n' + msg_yes_or_no_question)\n",
    "    resp = yes_or_no_question_key.get(resp)\n",
    "    if not resp:\n",
    "        is_views_finished = True\n",
    "        print('process done')\n",
    "        continue\n",
    "    new_view = input('please add new view name:\\n')\n",
    "    while not is_features_finished:\n",
    "        feature_format_file = {}\n",
    "        new_feature = input('please add new feature name:\\n')\n",
    "        feature_format_file[new_feature] = {}categorical_values\n",
    "        is_column_parsed = False\n",
    "        try:\n",
    "            while not is_column_parsed:\n",
    "                data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "                if data_format_id.isdigit() and int(data_format_id) <= n_available_data_type+1:\n",
    "                    # check if value passed by user is correct (if it is integer,\n",
    "                    # and whithin range [1, n_available_data_type])\n",
    "                    is_column_parsed = True\n",
    "                \n",
    "                else:\n",
    "                    print(f'error ! {data_format_id} value not understood')\n",
    "                    \n",
    "        except KeyboardInterrupt as e:\n",
    "            print('stopping now' + str(e))\n",
    "        resp = input('do you want to add a new variable (feature) ?' + msg_yes_or_no_question)\n",
    "        resp = yes_or_no_question_key.get(resp)\n",
    "        if not resp:\n",
    "            is_features_finished = True\n",
    "            print('process done')\n",
    "            continue\n",
    "    views_format_file[new_view] = feature_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee0e0cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'datetime64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9387/2903691535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'datetime64'"
     ]
    }
   ],
   "source": [
    "type(np.datetime64(\"2018-01-01\"))\n",
    "import datetime\n",
    "type(datetime.datetime(2018, 1, 1))\n",
    "\n",
    "pd.datetime64[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24be197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = type(pd.to_datetime('13000101', format='%Y%m%d', errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52c37736",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Series(pd.date_range(\"1/1/2011\", freq=\"H\", periods=3)).dtype\n",
    "\n",
    "t =type(t).type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b13acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(t == t1 for t1 in [pd.Timestamp, pd.Timedelta, pd.Period, datetime.datetime,np.datetime64] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb165f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9881289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDRSB.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'ADAS11.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'MMSE.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.immediate.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.learning.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.forgetting.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'FAQ.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'WholeBrain.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Ventricles.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Hippocampus.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'MidTemp.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Entorhinal.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'ABETA.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'PTAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'TAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'AGE': {'data_type': <QuantitativeDataType.DISCRETE: [<class 'int'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb7ad297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.dtype[float64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(dataset[feature].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dbaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_add_numeric_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_arith_method',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_binop',\n",
       " '_cacher',\n",
       " '_can_hold_na',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_from_arguments',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_convert',\n",
       " '_convert_dtypes',\n",
       " '_data',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_duplicated',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_from_mgr',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_index_resolvers',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_get_values_tuple',\n",
       " '_get_with',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_index',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_init_dict',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_cached',\n",
       " '_is_copy',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_item_cache',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_map_values',\n",
       " '_maybe_update_cacher',\n",
       " '_memory_usage',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_name',\n",
       " '_needs_reindex_multi',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_indexer',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_replace_single',\n",
       " '_repr_data_resource_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_labels',\n",
       " '_set_name',\n",
       " '_set_value',\n",
       " '_set_values',\n",
       " '_set_with',\n",
       " '_set_with_engine',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'array',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'autocorr',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'clip',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'factorize',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'hasnans',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'interpolate',\n",
       " 'is_monotonic',\n",
       " 'is_monotonic_decreasing',\n",
       " 'is_monotonic_increasing',\n",
       " 'is_unique',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'item',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memory_usage',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'ravel',\n",
       " 'rdiv',\n",
       " 'rdivmod',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'repeat',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'searchsorted',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_frame',\n",
       " 'to_hdf',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_list',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_sql',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unique',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae52ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
