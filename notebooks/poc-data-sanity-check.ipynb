{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc33777f",
   "metadata": {},
   "source": [
    "# data sanity check\n",
    "\n",
    "\n",
    "assumption when parsing:\n",
    "\n",
    "\n",
    "- name of private key is the same for each dataet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1667e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional, Callable\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "import json\n",
    "\n",
    "import utils\n",
    "from data_type import DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645ef4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Tuple[Dict[str, pd.DataFrame],\n",
    "                                                                              Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    # check for each variable name existing in one view, that it doesnot exist in another\n",
    "    # view. if it is, rename both variables\n",
    "    # for this purpose, parse every combination once\n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets, _features_names\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe_from_dictionary(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    # WARNING: DOESNOT CONTAIN FACILITY FOR KEEPING PRIMARY KEY\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "def create_multi_view_dataframe_from_dataframe(dataframe: pd.DataFrame,\n",
    "                                               views_features: Dict[str, List[str]],\n",
    "                                               primary_key: str = None):\n",
    "    # convert plain dataframe into multi index dataframe\n",
    "    # primary key will have its own view\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    _primary_key_label = 'primary_key'\n",
    "    _n_features = 0\n",
    "    \n",
    "    _multi_index = dataframe.columns\n",
    "    if primary_key is not None:\n",
    "        _key_values = dataframe[primary_key].values  # storing primary key values\n",
    "\n",
    "    _all_features_names = []\n",
    "    _new_views_names = []\n",
    "    for view_name in views_features.keys():\n",
    "        # get all columns name for each view, and remove primary keymulti_view_dataset[view_name] = pd.concat[]\n",
    "        _features_names = list(views_features[view_name])\n",
    "        \n",
    "        if primary_key is not None:\n",
    "            _features_names.remove(primary_key)\n",
    "        \n",
    "        for feature_name in _features_names:\n",
    "            #if feature_name not in _all_features_names:\n",
    "            _new_views_names.append(view_name)\n",
    "            # appending as much as there are feature within each view\n",
    "        _n_features += len(_features_names)\n",
    "        _all_features_names.extend(_features_names)\n",
    "        \n",
    "        #_all_features_names = list(set(_all_features_names))  # remove duplicates\n",
    "    \n",
    "    print('length', _all_features_names, _new_views_names)\n",
    "    _header = pd.MultiIndex.from_arrays([ _new_views_names, _all_features_names],\n",
    "                                        names=_header_labels)\n",
    "    \n",
    "    print('BUG',_all_features_names, dataframe[_all_features_names].values.shape, dataframe.shape)\n",
    "    print(_header)\n",
    "    multi_view_dataframe = pd.DataFrame(dataframe[_all_features_names].values, columns=_header)\n",
    "    \n",
    "    if primary_key is not None:\n",
    "        \n",
    "        multi_view_dataframe[_primary_key_label, primary_key] = _key_values  # creating a specific value for\n",
    "    # private key\n",
    "    return multi_view_dataframe\n",
    "\n",
    "\n",
    "def join_multi_view_dataset(multi_view_dataset: Union[pd.DataFrame, Dict[str, pd.DataFrame]],\n",
    "                           #multi_view_dataframe: pd.DataFrame=None,\n",
    "                           #multi_view_dictionary_dataset: Dict[str, pd.DataFrame] = None, \n",
    "                           primary_key: str=None,\n",
    "                          as_multi_index: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    \n",
    "    if isinstance(multi_view_dataset, pd.DataFrame):\n",
    "        _views_names = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "\n",
    "        \n",
    "    elif isinstance(multi_view_dataset, dict):\n",
    "        _views_names = sorted(list(multi_view_dataset.keys()))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('method can only accept as input multi view pandas dataframe or dictionary of pandas dataframes')\n",
    "        \n",
    "    joined_dataframe = multi_view_dataset[_views_names[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_names)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_names[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_names[x]))\n",
    "    \n",
    "    if as_multi_index:\n",
    "        # convert plain dataframe into multi index dataframe\n",
    "        # primary key will have its own view\n",
    "        _header_labels = ['views', 'feature_name']\n",
    "        _primary_key_label = 'primary_key'\n",
    "        \n",
    "        _multi_index = multi_view_dataset.columns\n",
    "        \n",
    "        _key_values = joined_dataframe[primary_key].values  # storing primary key\n",
    "\n",
    "        _all_features_names = []\n",
    "        _new_views_names = []\n",
    "        for view_name in _views_names:\n",
    "            # get all columns name for each view, and remove primary key\n",
    "            _features_names = list(multi_view_dataset[view_name].columns)\n",
    "            if primary_key is not None:\n",
    "                _features_names.remove(primary_key)\n",
    "            _all_features_names.extend(_features_names)\n",
    "\n",
    "            for feature_name in _features_names:\n",
    "                _new_views_names.append(view_name)\n",
    "                # appending as much as there are feature within each view\n",
    "            #features_name[name].remove(primary_key)\n",
    "\n",
    "        _header = pd.MultiIndex.from_arrays([ _new_views_names, _all_features_names],\n",
    "                                            names=_header_labels)\n",
    "        print(_header)\n",
    "        joined_dataframe  = pd.DataFrame(joined_dataframe[_all_features_names].values, columns=_header)\n",
    "        joined_dataframe[_primary_key_label, primary_key] = _key_values\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"\"\"\"\n",
    "    _views_names = list(format_file_ref.keys())\n",
    "    primary_key = None\n",
    "    _c_view = None\n",
    "    for view_name in _views_names:\n",
    "        file_content = format_file_ref[view_name]\n",
    "        _features_names = list(file_content.keys())\n",
    "        for feature_name in _features_names:\n",
    "            feature_content  = file_content[feature_name]\n",
    "            _d_format = feature_content.get('data_format')\n",
    "            \n",
    "            if _d_format == DataType.KEY.name:\n",
    "                if _c_view is None:\n",
    "                    primary_key = feature_name\n",
    "                    _c_view = view_name\n",
    "                    print(f'found primary key {primary_key}')\n",
    "                else:\n",
    "                    print(f'error: found 2 primary keys is same view {view_name}')\n",
    "        _c_view = None\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "\n",
    "def select_data_from_format_file_ref(datasets: Dict[str, Dict[str, Any]],\n",
    "                                     format_file: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"returns an updated dataset containing only the features detailed in format_file\"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    updated_dataset = {}\n",
    "    _views_format_file = list(format_file.keys())\n",
    "    \n",
    "    for view in _views_format_file:\n",
    "        if view in datasets.keys():\n",
    "            # only extract features from format_file\n",
    "            _format_file_features = list(format_file[view].keys())\n",
    "            _current_dataset_feature = datasets[view].columns.tolist()\n",
    "            try:\n",
    "                updated_dataset[view] = datasets[view][_format_file_features]\n",
    "            except KeyError as ke:\n",
    "                # catch error if a column is specified in data format file\n",
    "                # but not found in dataset\n",
    "                _missing_feature = []\n",
    "                for feature in _format_file_features:\n",
    "                    if feature not in _current_dataset_feature:\n",
    "                        _missing_feature.append(feature)\n",
    "                print('Error: th following features', *_missing_feature, f'are not found in view: {view}')\n",
    "        else:\n",
    "            # trigger error\n",
    "            print(f'error!: missing view {view} in dataset')\n",
    "            \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CustomWarning():\n",
    "    \n",
    "        def __init__(self, disclosure: int,level: str):\n",
    "            #super().__init__('l')\n",
    "            self.disclosure = disclosure\n",
    "            self.level = level\n",
    "        \n",
    "        def display(self,message, columns = ' ' ):  \n",
    "            \n",
    "            \n",
    "            logger = logging.getLogger('mylogger')\n",
    "            #logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            #handler = logging.FileHandler('mylog.log')\n",
    "            #formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            #handler.setFormatter(formatter)\n",
    "            #logger.addHandler(handler)\n",
    "            \n",
    "            if self.disclosure == 1:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    logger.warning(message)\n",
    "            elif self.disclosure == 2:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    message = 'Critical Warning.' + message\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    message = 'Regular Warning.' + message\n",
    "                    logger.warning(message)\n",
    "            elif self.disclosure == 3:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    message = 'Critical Warning. ' + message + 'Columns affected :' + columns\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    message = 'Regular Warning. ' + message  + 'Columns affected :' + columns\n",
    "                    logger.warning(message) \n",
    "            print (message) \n",
    "            return message\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f981756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_multi_view_dataset(dataframe: pd.DataFrame,\n",
    "                                         views_features_mapping: Dict[str, List[str]],\n",
    "                                         primary_key: str=None) -> Dict[str, pd.DataFrame]:\n",
    "    _primary_key_label = 'primary_key'\n",
    "    \n",
    "    multi_view_dataset = {}\n",
    "    \n",
    "    if primary_key is not None:\n",
    "        _key_values = dataframe[primary_key].values  # storing primary key values\n",
    "\n",
    "    _all_features_names = []\n",
    "    _new_views_names = []\n",
    "    for view_name in views_features_mapping.keys():\n",
    "        # get all columns name for each view, and remove primary key\n",
    "        _features_names = list(views_features_mapping[view_name])\n",
    "        \n",
    "        if primary_key is not None:\n",
    "            _features_names.remove(primary_key)\n",
    "        _tmp_dataframe = dataframe[_features_names[0]].values\n",
    "        _tmp_dataframe = _tmp_dataframe.reshape(-1, 1)  # need to reshape,\n",
    "        #(otherwise concatenation wont work)\n",
    "        for feature in _features_names[1:]:\n",
    "            # iterate over the remaining items in _feature_name\n",
    "            # need to do it that way because indexing dataframe is somehow broken\n",
    "            \n",
    "            _new_feature = dataframe[feature].to_numpy()\n",
    "            _new_feature = _new_feature.reshape(-1, 1)\n",
    "            _tmp_dataframe = np.concatenate([_tmp_dataframe, _new_feature], axis=1)\n",
    "            \n",
    "        multi_view_dataset[view_name] =pd.DataFrame( _tmp_dataframe, columns=_features_names)\n",
    "    \n",
    "    if primary_key is not None:\n",
    "        multi_view_dataset[primary_key] = dataframe[primary_key]\n",
    "    return multi_view_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a533dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'views_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6195/198182079.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_feature_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_parsed_dataset_to_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mviews_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_feature_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcreate_multi_view_dataframe_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_feature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimary_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprimary_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'views_names' is not defined"
     ]
    }
   ],
   "source": [
    "new_feature_name = { v: list(pre_parsed_dataset_to_check[v].columns) for v in views_names}\n",
    "\n",
    "print(new_feature_name)\n",
    "\n",
    "create_multi_view_dataframe_from_dataframe(df_joined,new_feature_name, primary_key=primary_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fce9e7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 4],\n",
       "       [3, 5]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[[1],[2],[3]], [[4],[4],[5]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7006c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>2</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>pkey</th>\n",
       "      <th>pkey</th>\n",
       "      <th>discrete</th>\n",
       "      <th>pH</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>e</th>\n",
       "      <th>gender</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>blood type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Lille</td>\n",
       "      <td>zmixzrgvxrjqxoe sluk</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>0.088082</td>\n",
       "      <td>63</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>98</td>\n",
       "      <td>MAN</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Lille</td>\n",
       "      <td>vrzahnpfluspdcbfnaqt</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>xkdawggpnuulcewuoyzz</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774788</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>83</td>\n",
       "      <td>MAN</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>Paris</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>khuulhwgwnjggrfoefce</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.407279</td>\n",
       "      <td>0.514092</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>73</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>Paris</td>\n",
       "      <td>gwj luzejwdxzsiljxzd</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>xxysdmwwmjsmyhaswfdb</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.536301</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>Lille</td>\n",
       "      <td>jjdvcnofivbqhirxzdyo</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>ldejfuij mnbnf wwmms</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.749443</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>84</td>\n",
       "      <td>MAN</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>True</td>\n",
       "      <td>Paris</td>\n",
       "      <td>hrvepmqjn llgbzplshv</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>wrmdecb s pohtmrcdj</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295578</td>\n",
       "      <td>41</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>66</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>True</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>wroevwyuamxibzshlxxh</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>whmwrpvqmerdpwwzxasf</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.388389</td>\n",
       "      <td>0.474322</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>81</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>True</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ywadcykylymkdtzfctpg</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.889067</td>\n",
       "      <td>0.927511</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>82</td>\n",
       "      <td>MAN</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>True</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>ruchbfa zwgenxslegrl</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>0.494798</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>MAN</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>True</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeapltpxuvuibfybxcll</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>kiejdmbuih awhuifwwd</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.667349</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name      2       city                  pkey                  pkey  \\\n",
       "0             False      Lille  zmixzrgvxrjqxoe sluk  qpqorfhylu gmfjy bdj   \n",
       "1              True      Lille  vrzahnpfluspdcbfnaqt  kkmjozalfyirgsire ui   \n",
       "2             False      Paris  pnrepvmrxqabdlvisclv  ezfasuuycdda foisjte   \n",
       "3              True      Paris  gwj luzejwdxzsiljxzd  faxiqkt xggzmwzoidbg   \n",
       "4             False      Lille  jjdvcnofivbqhirxzdyo  znwhlj rwzdutnagwasy   \n",
       "..              ...        ...                   ...                   ...   \n",
       "95             True      Paris  hrvepmqjn llgbzplshv  zeqhcikzdodus jn qjf   \n",
       "96             True  Marseille  wroevwyuamxibzshlxxh  iicthcvfmkajbvr gzir   \n",
       "97             True      Lille  ywadcykylymkdtzfctpg  ztjakcsk bhjoksdz lm   \n",
       "98             True  Marseille  ruchbfa zwgenxslegrl   sabunaa opt vpulnxj   \n",
       "99             True      Paris  zeapltpxuvuibfybxcll  qmbexyexvgromrm admu   \n",
       "\n",
       "feature_name                  pkey discrete        pH  pressure e.1 file1.1  \\\n",
       "0             kkmjozalfyirgsire ui     64.0  0.023107  0.088082  63    True   \n",
       "1             xkdawggpnuulcewuoyzz     26.0       NaN  0.774788  20   False   \n",
       "2             khuulhwgwnjggrfoefce     61.0  0.407279  0.514092   2   False   \n",
       "3             xxysdmwwmjsmyhaswfdb     29.0  0.536301  0.832881  70    True   \n",
       "4             ldejfuij mnbnf wwmms     99.0  0.749443  0.696152  90    True   \n",
       "..                             ...      ...       ...       ...  ..     ...   \n",
       "95            wrmdecb s pohtmrcdj       9.0       NaN  0.295578  41    True   \n",
       "96            whmwrpvqmerdpwwzxasf     98.0  0.388389  0.474322  41   False   \n",
       "97            pnrepvmrxqabdlvisclv     21.0  0.889067  0.927511   7    True   \n",
       "98            iicthcvfmkajbvr gzir     42.0  0.402979  0.494798  11   False   \n",
       "99            kiejdmbuih awhuifwwd      3.0  0.667349  0.316395  74   False   \n",
       "\n",
       "feature_name file2.1           file1.time   e gender           file2.time  \\\n",
       "0               True  2018-01-01 00:00:00  98    MAN  2018-01-01 00:00:00   \n",
       "1              False  2018-01-01 01:00:00  83    MAN  2018-01-01 01:00:00   \n",
       "2               True  2018-01-01 02:00:00  73  WOMAN  2018-01-01 02:00:00   \n",
       "3               True  2018-01-01 03:00:00  45  WOMAN  2018-01-01 03:00:00   \n",
       "4               True  2018-01-01 04:00:00  84    MAN  2018-01-01 04:00:00   \n",
       "..               ...                  ...  ..    ...                  ...   \n",
       "95              True  2018-01-04 23:00:00  66  WOMAN  2018-01-04 23:00:00   \n",
       "96             False  2018-01-05 00:00:00  81  WOMAN  2018-01-05 00:00:00   \n",
       "97              True  2018-01-05 01:00:00  82    MAN  2018-01-05 01:00:00   \n",
       "98              True  2018-01-05 02:00:00  18    MAN  2018-01-05 02:00:00   \n",
       "99             False  2018-01-05 03:00:00  70  WOMAN  2018-01-05 03:00:00   \n",
       "\n",
       "feature_name blood type  \n",
       "0                     A  \n",
       "1                     O  \n",
       "2                     A  \n",
       "3                    AB  \n",
       "4                     B  \n",
       "..                  ...  \n",
       "95                    A  \n",
       "96                    B  \n",
       "97                    B  \n",
       "98                    O  \n",
       "99                   AB  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check[['2', 'city', 'pkey', 'discrete', 'pH', 'pressure', 'e.1', 'file1.1', 'file2.1', 'file1.time', 'e', 'gender', 'file2.time', 'blood type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "055dcd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>pkey</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.088082</td>\n",
       "      <td>63</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>zmixzrgvxrjqxoe sluk</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>0.774788</td>\n",
       "      <td>20</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>vrzahnpfluspdcbfnaqt</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xkdawggpnuulcewuoyzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.514092</td>\n",
       "      <td>2</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.407279</td>\n",
       "      <td>khuulhwgwnjggrfoefce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>70</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>gwj luzejwdxzsiljxzd</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.536301</td>\n",
       "      <td>xxysdmwwmjsmyhaswfdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>90</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>jjdvcnofivbqhirxzdyo</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.749443</td>\n",
       "      <td>ldejfuij mnbnf wwmms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>0.295578</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>hrvepmqjn llgbzplshv</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrmdecb s pohtmrcdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>81</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.474322</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>B</td>\n",
       "      <td>wroevwyuamxibzshlxxh</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.388389</td>\n",
       "      <td>whmwrpvqmerdpwwzxasf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.927511</td>\n",
       "      <td>7</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>ywadcykylymkdtzfctpg</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.889067</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.494798</td>\n",
       "      <td>11</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>ruchbfa zwgenxslegrl</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>74</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>zeapltpxuvuibfybxcll</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.667349</td>\n",
       "      <td>kiejdmbuih awhuifwwd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name   e file1.1      2           file1.time  pressure e.1 gender  \\\n",
       "0             98    True  False  2018-01-01 00:00:00  0.088082  63    MAN   \n",
       "1             83   False   True  2018-01-01 01:00:00  0.774788  20    MAN   \n",
       "2             73   False  False  2018-01-01 02:00:00  0.514092   2  WOMAN   \n",
       "3             45    True   True  2018-01-01 03:00:00  0.832881  70  WOMAN   \n",
       "4             84    True  False  2018-01-01 04:00:00  0.696152  90    MAN   \n",
       "..            ..     ...    ...                  ...       ...  ..    ...   \n",
       "95            66    True   True  2018-01-04 23:00:00  0.295578  41  WOMAN   \n",
       "96            81   False   True  2018-01-05 00:00:00  0.474322  41  WOMAN   \n",
       "97            82    True   True  2018-01-05 01:00:00  0.927511   7    MAN   \n",
       "98            18   False   True  2018-01-05 02:00:00  0.494798  11    MAN   \n",
       "99            70   False   True  2018-01-05 03:00:00  0.316395  74  WOMAN   \n",
       "\n",
       "feature_name blood type                  pkey discrete       city  \\\n",
       "0                     A  zmixzrgvxrjqxoe sluk     64.0      Lille   \n",
       "1                     O  vrzahnpfluspdcbfnaqt     26.0      Lille   \n",
       "2                     A  pnrepvmrxqabdlvisclv     61.0      Paris   \n",
       "3                    AB  gwj luzejwdxzsiljxzd     29.0      Paris   \n",
       "4                     B  jjdvcnofivbqhirxzdyo     99.0      Lille   \n",
       "..                  ...                   ...      ...        ...   \n",
       "95                    A  hrvepmqjn llgbzplshv      9.0      Paris   \n",
       "96                    B  wroevwyuamxibzshlxxh     98.0  Marseille   \n",
       "97                    B  ywadcykylymkdtzfctpg     21.0      Lille   \n",
       "98                    O  ruchbfa zwgenxslegrl     42.0  Marseille   \n",
       "99                   AB  zeapltpxuvuibfybxcll      3.0      Paris   \n",
       "\n",
       "feature_name                  pkey file2.1           file2.time        pH  \\\n",
       "0             qpqorfhylu gmfjy bdj    True  2018-01-01 00:00:00  0.023107   \n",
       "1             kkmjozalfyirgsire ui   False  2018-01-01 01:00:00       NaN   \n",
       "2             ezfasuuycdda foisjte    True  2018-01-01 02:00:00  0.407279   \n",
       "3             faxiqkt xggzmwzoidbg    True  2018-01-01 03:00:00  0.536301   \n",
       "4             znwhlj rwzdutnagwasy    True  2018-01-01 04:00:00  0.749443   \n",
       "..                             ...     ...                  ...       ...   \n",
       "95            zeqhcikzdodus jn qjf    True  2018-01-04 23:00:00       NaN   \n",
       "96            iicthcvfmkajbvr gzir   False  2018-01-05 00:00:00  0.388389   \n",
       "97            ztjakcsk bhjoksdz lm    True  2018-01-05 01:00:00  0.889067   \n",
       "98             sabunaa opt vpulnxj    True  2018-01-05 02:00:00  0.402979   \n",
       "99            qmbexyexvgromrm admu   False  2018-01-05 03:00:00  0.667349   \n",
       "\n",
       "feature_name                  pkey  \n",
       "0             kkmjozalfyirgsire ui  \n",
       "1             xkdawggpnuulcewuoyzz  \n",
       "2             khuulhwgwnjggrfoefce  \n",
       "3             xxysdmwwmjsmyhaswfdb  \n",
       "4             ldejfuij mnbnf wwmms  \n",
       "..                             ...  \n",
       "95            wrmdecb s pohtmrcdj   \n",
       "96            whmwrpvqmerdpwwzxasf  \n",
       "97            pnrepvmrxqabdlvisclv  \n",
       "98            iicthcvfmkajbvr gzir  \n",
       "99            kiejdmbuih awhuifwwd  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207638d",
   "metadata": {},
   "source": [
    "existing tests\n",
    "\n",
    "- test keys (should be done before joining) \n",
    " |-> unicity of value\n",
    "- test datetime\n",
    " |-> are they datetime parsable (for that i am using `dateutil` python package)\n",
    " \n",
    "- test variables including\n",
    " |-> test if data have missing values and missing values are not allowed\n",
    " | -> test correct categories / sub categories\n",
    " | -> test lower bound\n",
    " | -> test upper bound\n",
    " | -> check if defined values are contained in categorical variables\n",
    " \n",
    "- data transformation\n",
    "\n",
    "|-> interpolate missing values (if allowed) using specific method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d485c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regular Warning.hsskks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Warning.hsskks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regular Warning.hsskks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = CustomWarning(2, 'WARNING')\n",
    "inst.display('hsskks', 'kl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c02841a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1':      e  file1.1      2           file1.time  pressure  e.1 gender blood type  \\\n",
       " 0   98     True  False  2018-01-01 00:00:00  0.088082   63    MAN          A   \n",
       " 1   83    False   True  2018-01-01 01:00:00  0.774788   20    MAN          O   \n",
       " 2   73    False  False  2018-01-01 02:00:00  0.514092    2  WOMAN          A   \n",
       " 3   45     True   True  2018-01-01 03:00:00  0.832881   70  WOMAN         AB   \n",
       " 4   84     True  False  2018-01-01 04:00:00  0.696152   90    MAN          B   \n",
       " ..  ..      ...    ...                  ...       ...  ...    ...        ...   \n",
       " 95  66     True   True  2018-01-04 23:00:00  0.295578   41  WOMAN          A   \n",
       " 96  81    False   True  2018-01-05 00:00:00  0.474322   41  WOMAN          B   \n",
       " 97  82     True   True  2018-01-05 01:00:00  0.927511    7    MAN          B   \n",
       " 98  18    False   True  2018-01-05 02:00:00  0.494798   11    MAN          O   \n",
       " 99  70    False   True  2018-01-05 03:00:00  0.316395   74  WOMAN         AB   \n",
       " \n",
       "                     pkey  \n",
       " 0   zmixzrgvxrjqxoe sluk  \n",
       " 1   vrzahnpfluspdcbfnaqt  \n",
       " 2   pnrepvmrxqabdlvisclv  \n",
       " 3   gwj luzejwdxzsiljxzd  \n",
       " 4   jjdvcnofivbqhirxzdyo  \n",
       " ..                   ...  \n",
       " 95  hrvepmqjn llgbzplshv  \n",
       " 96  wroevwyuamxibzshlxxh  \n",
       " 97  ywadcykylymkdtzfctpg  \n",
       " 98  ruchbfa zwgenxslegrl  \n",
       " 99  zeapltpxuvuibfybxcll  \n",
       " \n",
       " [100 rows x 9 columns],\n",
       " 'contatct':     discrete       city                  pkey\n",
       " 0       64.0      Lille  qpqorfhylu gmfjy bdj\n",
       " 1       26.0      Lille  kkmjozalfyirgsire ui\n",
       " 2       61.0      Paris  ezfasuuycdda foisjte\n",
       " 3       29.0      Paris  faxiqkt xggzmwzoidbg\n",
       " 4       99.0      Lille  znwhlj rwzdutnagwasy\n",
       " ..       ...        ...                   ...\n",
       " 95       9.0      Paris  zeqhcikzdodus jn qjf\n",
       " 96      98.0  Marseille  iicthcvfmkajbvr gzir\n",
       " 97      21.0      Lille  ztjakcsk bhjoksdz lm\n",
       " 98      42.0  Marseille   sabunaa opt vpulnxj\n",
       " 99       3.0      Paris  qmbexyexvgromrm admu\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'file2':     file2.1           file2.time        pH                  pkey\n",
       " 0      True  2018-01-01 00:00:00  0.023107  kkmjozalfyirgsire ui\n",
       " 1     False  2018-01-01 01:00:00       NaN  xkdawggpnuulcewuoyzz\n",
       " 2      True  2018-01-01 02:00:00  0.407279  khuulhwgwnjggrfoefce\n",
       " 3      True  2018-01-01 03:00:00  0.536301  xxysdmwwmjsmyhaswfdb\n",
       " 4      True  2018-01-01 04:00:00  0.749443  ldejfuij mnbnf wwmms\n",
       " ..      ...                  ...       ...                   ...\n",
       " 95     True  2018-01-04 23:00:00       NaN  wrmdecb s pohtmrcdj \n",
       " 96    False  2018-01-05 00:00:00  0.388389  whmwrpvqmerdpwwzxasf\n",
       " 97     True  2018-01-05 01:00:00  0.889067  pnrepvmrxqabdlvisclv\n",
       " 98     True  2018-01-05 02:00:00  0.402979  iicthcvfmkajbvr gzir\n",
       " 99    False  2018-01-05 03:00:00  0.667349  kiejdmbuih awhuifwwd\n",
       " \n",
       " [100 rows x 4 columns]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_parsed_dataset_to_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78c5b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343be64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': ['e',\n",
       "  'file1.1',\n",
       "  '2',\n",
       "  'file1.time',\n",
       "  'pressure',\n",
       "  'e.1',\n",
       "  'gender',\n",
       "  'blood type',\n",
       "  'pkey'],\n",
       " 'contatct': ['discrete', 'city', 'pkey'],\n",
       " 'file2': ['file2.1', 'file2.time', 'pH', 'pkey']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3201f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>views</th>\n",
       "      <th colspan=\"2\" halign=\"left\">contatct</th>\n",
       "      <th colspan=\"8\" halign=\"left\">file1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">file2</th>\n",
       "      <th>primary_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.98667</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.78856</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "views        contatct            file1                                      \\\n",
       "feature_name discrete       city     e file1.1      2           file1.time   \n",
       "0                64.0      Lille    16   False  False  2018-01-03 04:00:00   \n",
       "1                26.0      Lille    96    True   True  2018-01-02 04:00:00   \n",
       "2                61.0      Paris     8    True   True  2018-01-01 09:00:00   \n",
       "3                29.0      Paris     6    True  False  2018-01-04 20:00:00   \n",
       "4                99.0      Lille    79    True   True  2018-01-04 09:00:00   \n",
       "..                ...        ...   ...     ...    ...                  ...   \n",
       "95                9.0      Paris    62    True   True  2018-01-02 13:00:00   \n",
       "96               98.0  Marseille    49   False   True  2018-01-02 21:00:00   \n",
       "97               21.0      Lille    14   False  False  2018-01-02 06:00:00   \n",
       "98               42.0  Marseille    10    True  False  2018-01-02 01:00:00   \n",
       "99                3.0      Paris    89    True   True  2018-01-03 22:00:00   \n",
       "\n",
       "views                                          file2                       \\\n",
       "feature_name  pressure e.1 gender blood type file2.1           file2.time   \n",
       "0              0.98667  98  WOMAN          A   False  2018-01-02 06:00:00   \n",
       "1             0.996889  35    MAN         AB    True  2018-01-01 00:00:00   \n",
       "2             0.777026  65    MAN          A   False  2018-01-02 10:00:00   \n",
       "3             0.877527  81    MAN         AB    True  2018-01-03 12:00:00   \n",
       "4             0.447389  88  WOMAN          O    True  2018-01-01 10:00:00   \n",
       "..                 ...  ..    ...        ...     ...                  ...   \n",
       "95            0.953184  53    MAN         AB   False  2018-01-02 05:00:00   \n",
       "96            0.442283  35    MAN        NaN    True  2018-01-05 02:00:00   \n",
       "97            0.988543  67    MAN          B   False  2018-01-01 12:00:00   \n",
       "98            0.059791  48    MAN          B    True  2018-01-02 09:00:00   \n",
       "99            0.939352  13    MAN          B   False  2018-01-03 02:00:00   \n",
       "\n",
       "views                            primary_key  \n",
       "feature_name        pH                  pkey  \n",
       "0                  NaN  qpqorfhylu gmfjy bdj  \n",
       "1             0.023107  kkmjozalfyirgsire ui  \n",
       "2             0.587685  ezfasuuycdda foisjte  \n",
       "3             0.894073  faxiqkt xggzmwzoidbg  \n",
       "4             0.026831  znwhlj rwzdutnagwasy  \n",
       "..                 ...                   ...  \n",
       "95             0.78856  zeqhcikzdodus jn qjf  \n",
       "96            0.402979  iicthcvfmkajbvr gzir  \n",
       "97                 NaN  ztjakcsk bhjoksdz lm  \n",
       "98            0.651801   sabunaa opt vpulnxj  \n",
       "99            0.751969  qmbexyexvgromrm admu  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47e0f6",
   "metadata": {},
   "source": [
    "# for simple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "60c31809",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9618/3990744942.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract views names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mviews_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'format_file' is not defined"
     ]
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(format_file.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(dataset_to_check, format_file)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe_from_dictionary(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_joined = join_muti_view_dataset(multi_df_to_check)\n",
    "    \n",
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ed7c3",
   "metadata": {},
   "source": [
    "# for multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fb620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    }
   ],
   "source": [
    "multi_format_file_ref = utils.load_format_file_ref('multi_format_file')\n",
    "multi_dataset_to_check = utils.load_tabular_datasets(r'test7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5025cdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ef4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found primary key pkey\n",
      "found primary key pkey\n",
      "found primary key pkey\n",
      "primary key pkey\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.986670</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.788560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    discrete       city                  pkey   e  file1.1      2  \\\n",
       "0       64.0      Lille  qpqorfhylu gmfjy bdj  16    False  False   \n",
       "1       26.0      Lille  kkmjozalfyirgsire ui  96     True   True   \n",
       "2       61.0      Paris  ezfasuuycdda foisjte   8     True   True   \n",
       "3       29.0      Paris  faxiqkt xggzmwzoidbg   6     True  False   \n",
       "4       99.0      Lille  znwhlj rwzdutnagwasy  79     True   True   \n",
       "..       ...        ...                   ...  ..      ...    ...   \n",
       "95       9.0      Paris  zeqhcikzdodus jn qjf  62     True   True   \n",
       "96      98.0  Marseille  iicthcvfmkajbvr gzir  49    False   True   \n",
       "97      21.0      Lille  ztjakcsk bhjoksdz lm  14    False  False   \n",
       "98      42.0  Marseille   sabunaa opt vpulnxj  10     True  False   \n",
       "99       3.0      Paris  qmbexyexvgromrm admu  89     True   True   \n",
       "\n",
       "             file1.time  pressure  e.1 gender blood type  file2.1  \\\n",
       "0   2018-01-03 04:00:00  0.986670   98  WOMAN          A    False   \n",
       "1   2018-01-02 04:00:00  0.996889   35    MAN         AB     True   \n",
       "2   2018-01-01 09:00:00  0.777026   65    MAN          A    False   \n",
       "3   2018-01-04 20:00:00  0.877527   81    MAN         AB     True   \n",
       "4   2018-01-04 09:00:00  0.447389   88  WOMAN          O     True   \n",
       "..                  ...       ...  ...    ...        ...      ...   \n",
       "95  2018-01-02 13:00:00  0.953184   53    MAN         AB    False   \n",
       "96  2018-01-02 21:00:00  0.442283   35    MAN        NaN     True   \n",
       "97  2018-01-02 06:00:00  0.988543   67    MAN          B    False   \n",
       "98  2018-01-02 01:00:00  0.059791   48    MAN          B     True   \n",
       "99  2018-01-03 22:00:00  0.939352   13    MAN          B    False   \n",
       "\n",
       "             file2.time        pH  \n",
       "0   2018-01-02 06:00:00       NaN  \n",
       "1   2018-01-01 00:00:00  0.023107  \n",
       "2   2018-01-02 10:00:00  0.587685  \n",
       "3   2018-01-03 12:00:00  0.894073  \n",
       "4   2018-01-01 10:00:00  0.026831  \n",
       "..                  ...       ...  \n",
       "95  2018-01-02 05:00:00  0.788560  \n",
       "96  2018-01-05 02:00:00  0.402979  \n",
       "97  2018-01-01 12:00:00       NaN  \n",
       "98  2018-01-02 09:00:00  0.651801  \n",
       "99  2018-01-03 02:00:00  0.751969  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(multi_format_file_ref.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(multi_format_file_ref)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(multi_dataset_to_check, multi_format_file_ref)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check, new_views_name = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names,\n",
    "                                                             primary_key)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "#multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)  # remove that\n",
    "#multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_joined = join_multi_view_dataset(pre_parsed_dataset_to_check, primary_key, False) # should accept DIct[pd.DataFrame]\n",
    "    \n",
    "#df_to_check = multi_df_joined.droplevel(0, axis=1)  # remove views from dataset\n",
    "#df_to_check\n",
    "\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dacace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'1': 'file1.1', 'time': 'file1.time'},\n",
       " 'file2': {'1': 'file2.1', 'time': 'file2.time'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_views_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f60960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1':      e file1.1  2           file1.time  pressure e.1 gender blood type\n",
       " 0   16       0  0  2018-01-03 04:00:00   0.98667  98  WOMAN          A\n",
       " 1   96       1  1  2018-01-02 04:00:00  0.996889  35    MAN         AB\n",
       " 2    8       1  1  2018-01-01 09:00:00  0.777026  65    MAN          A\n",
       " 3    6       1  0  2018-01-04 20:00:00  0.877527  81    MAN         AB\n",
       " 4   79       1  1  2018-01-04 09:00:00  0.447389  88  WOMAN          O\n",
       " ..  ..     ... ..                  ...       ...  ..    ...        ...\n",
       " 95  62       1  1  2018-01-02 13:00:00  0.953184  53    MAN         AB\n",
       " 96  49       0  1  2018-01-02 21:00:00  0.442283  35    MAN        NaN\n",
       " 97  14       0  0  2018-01-02 06:00:00  0.988543  67    MAN          B\n",
       " 98  10       1  0  2018-01-02 01:00:00  0.059791  48    MAN          B\n",
       " 99  89       1  1  2018-01-03 22:00:00  0.939352  13    MAN          B\n",
       " \n",
       " [100 rows x 8 columns],\n",
       " 'contatct':    discrete       city\n",
       " 0      64.0      Lille\n",
       " 1      26.0      Lille\n",
       " 2      61.0      Paris\n",
       " 3      29.0      Paris\n",
       " 4      99.0      Lille\n",
       " ..      ...        ...\n",
       " 95      9.0      Paris\n",
       " 96     98.0  Marseille\n",
       " 97     21.0      Lille\n",
       " 98     42.0  Marseille\n",
       " 99      3.0      Paris\n",
       " \n",
       " [100 rows x 2 columns],\n",
       " 'file2':    file2.1           file2.time        pH\n",
       " 0    False  2018-01-02 06:00:00       NaN\n",
       " 1     True  2018-01-01 00:00:00  0.023107\n",
       " 2    False  2018-01-02 10:00:00  0.587685\n",
       " 3     True  2018-01-03 12:00:00  0.894073\n",
       " 4     True  2018-01-01 10:00:00  0.026831\n",
       " ..     ...                  ...       ...\n",
       " 95   False  2018-01-02 05:00:00   0.78856\n",
       " 96    True  2018-01-05 02:00:00  0.402979\n",
       " 97   False  2018-01-01 12:00:00       NaN\n",
       " 98    True  2018-01-02 09:00:00  0.651801\n",
       " 99   False  2018-01-03 02:00:00  0.751969\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'pkey': 0     qpqorfhylu gmfjy bdj\n",
       " 1     kkmjozalfyirgsire ui\n",
       " 2     ezfasuuycdda foisjte\n",
       " 3     faxiqkt xggzmwzoidbg\n",
       " 4     znwhlj rwzdutnagwasy\n",
       "               ...         \n",
       " 95    zeqhcikzdodus jn qjf\n",
       " 96    iicthcvfmkajbvr gzir\n",
       " 97    ztjakcsk bhjoksdz lm\n",
       " 98     sabunaa opt vpulnxj\n",
       " 99    qmbexyexvgromrm admu\n",
       " Name: pkey, Length: 100, dtype: object}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert joined data frame into muti view (multi index dataframe)\n",
    "\n",
    "new_feature_name = { v: list(pre_parsed_dataset_to_check[v].columns) for v in views_names}\n",
    "new_feature_name\n",
    "\n",
    "\n",
    "create_dictionary_multi_view_dataset(df_joined, new_feature_name, primary_key=primary_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad97b1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': ['e',\n",
       "  'file1.1',\n",
       "  '2',\n",
       "  'file1.time',\n",
       "  'pressure',\n",
       "  'e.1',\n",
       "  'gender',\n",
       "  'blood type',\n",
       "  'pkey'],\n",
       " 'contatct': ['discrete', 'city', 'pkey'],\n",
       " 'file2': ['file2.1', 'file2.time', 'pH', 'pkey']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4447dc77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_feature_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8673/2264684971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_format_file_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mn_feature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_feature_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# new_feature_ame is the result of the join operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcheck_variable_compliance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_joined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_format_file_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_feature_name' is not defined"
     ]
    }
   ],
   "source": [
    "exception_collector = []  #collects Exception\n",
    "\n",
    "# Data sanity check\n",
    "\n",
    "warning_report = WarningReportLogger(disclosure=3)\n",
    "\n",
    "for view in views_names:\n",
    "    print(view)\n",
    "    \n",
    "    feature_names = list(multi_format_file_ref[view].keys())\n",
    "    for n_feature_name, feature_name in zip(new_feature_name[view], feature_names):\n",
    "        # new_feature_ame is the result of the join operation\n",
    "        check_variable_compliance(df_joined[n_feature_name], multi_format_file_ref[view][feature_name])\n",
    "        data_format = multi_format_file_ref[view][feature_name].get('data_format')\n",
    "        \n",
    "        \n",
    "        check_missing_entry_format_file_ref(multi_format_file_ref[view][feature_name],\n",
    "                                            warning_report, 'lol', feature_name)\n",
    "        \n",
    "        check_missing_values(multi_format_file_ref[view][feature_name], \n",
    "                             df_joined[n_feature_name],\n",
    "                            warning_report, exception_collector)\n",
    "        if data_format == DataType.DATETIME.name:\n",
    "            # addtional check for DATETIME data format\n",
    "            check_datetime_variable_compliance( df_joined[n_feature_name], warning_report)\n",
    "            \n",
    "        if data_format == DataType.KEY.name:\n",
    "            check_key_variable_compliance(df_joined[n_feature_name])\n",
    "            \n",
    "        print(warning_report.get_report())\n",
    "        \n",
    "if exception_collector:\n",
    "    # case where exception collector is not empty\n",
    "    raise DataSanityCheckException(exception_collector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67a11d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features names updated\n",
      "N_SAMPLES_BELOW_THRESHOLD\n",
      "file1 e\n",
      "file1 e\n",
      "INCORRECT_FORMAT_FILE\n",
      "file1 e\n",
      "DATA_TYPE_MISMATCH\n",
      "INCORRECT_DATA_TYPE\n",
      "file1 e\n",
      "MISSING_DATA_ALLOWED\n",
      "file1 e\n",
      "OUTLIER_DETECTION_LOWER_BOUND\n",
      "file1 e\n",
      "OUTLIER_DETECTION_UPPER_BOUND\n",
      "file1 e\n",
      "INCORRECT_VALUES_CATEGORICAL_DATA\n",
      "file1 1\n",
      "file1 1\n",
      "file1 1\n",
      "file1 1\n",
      "file1 1\n",
      "file1 1\n",
      "file1 1\n",
      "file1 2\n",
      "file1 2\n",
      "file1 2\n",
      "file1 2\n",
      "file1 2\n",
      "file1 2\n",
      "file1 2\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "file1 time\n",
      "INCORRECT_DATETIME_DATA\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 pressure\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 e.1\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 gender\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 blood type\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "file1 pkey\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct discrete\n",
      "contatct city\n",
      "contatct city\n",
      "contatct city\n",
      "contatct city\n",
      "contatct city\n",
      "contatct city\n",
      "contatct city\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "contatct pkey\n",
      "file2 1\n",
      "file2 1\n",
      "file2 1\n",
      "file2 1\n",
      "file2 1\n",
      "file2 1\n",
      "file2 1\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 time\n",
      "file2 pH\n",
      "file2 pH\n",
      "file2 pH\n",
      "file2 pH\n",
      "MissingDataException: Variable pH must not have missing data, but some were found\n",
      "file2 pH\n",
      "file2 pH\n",
      "file2 pH\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "file2 pkey\n",
      "number of warnings: 1\n",
      "Number of error: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'N_SAMPLES_BELOW_THRESHOLD': [{'feature': 'ALL',\n",
       "   'msg': 'Test passed',\n",
       "   'view': 'file1',\n",
       "   'success': True},\n",
       "  {'feature': 'ALL',\n",
       "   'msg': 'Test passed',\n",
       "   'view': 'contatct',\n",
       "   'success': True},\n",
       "  {'feature': 'ALL', 'msg': 'Test passed', 'view': 'file2', 'success': True}],\n",
       " 'INCORRECT_FORMAT_FILE': [{'feature': 'e',\n",
       "   'msg': 'Test passed',\n",
       "   'view': '',\n",
       "   'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '2', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pressure', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'e.1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'gender', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'blood type', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'discrete', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'city', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pH', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True}],\n",
       " 'DATA_TYPE_MISMATCH': [{'feature': 'e',\n",
       "   'msg': 'Test passed',\n",
       "   'view': '',\n",
       "   'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '2', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pressure', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'e.1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'gender', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'blood type', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'discrete', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'city', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pH', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True}],\n",
       " 'INCORRECT_DATA_TYPE': [{'feature': 'e',\n",
       "   'msg': 'Test passed',\n",
       "   'view': '',\n",
       "   'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '2', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pressure', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'e.1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'gender', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'blood type', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'discrete', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'city', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pH', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True}],\n",
       " 'MISSING_DATA_ALLOWED': [{'feature': 'e',\n",
       "   'msg': 'Test passed',\n",
       "   'view': '',\n",
       "   'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '2', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pressure', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'e.1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'gender', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'blood type',\n",
       "   'msg': 'Missing data found in variable blood type',\n",
       "   'view': '',\n",
       "   'success': False},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'discrete', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'city', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': '1', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True},\n",
       "  {'feature': 'pH',\n",
       "   'msg': 'MissingDataException: Variable pH must not have missing data, but some were found',\n",
       "   'view': '',\n",
       "   'success': False},\n",
       "  {'feature': 'pkey', 'msg': 'Test passed', 'view': '', 'success': True}],\n",
       " 'OUTLIER_DETECTION_LOWER_BOUND': [{'feature': 'e',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '2', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pressure', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'e.1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'gender', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'blood type',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'discrete', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'city', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pH', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None}],\n",
       " 'OUTLIER_DETECTION_UPPER_BOUND': [{'feature': 'e',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '2', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pressure', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'e.1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'gender', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'blood type',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'discrete', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'city', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pH', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None}],\n",
       " 'INCORRECT_VALUES_CATEGORICAL_DATA': [{'feature': 'e',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '2', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pressure', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'e.1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'gender', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'blood type',\n",
       "   'msg': 'Test skipped',\n",
       "   'view': '',\n",
       "   'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'discrete', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'city', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': '1', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'time', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pH', 'msg': 'Test skipped', 'view': '', 'success': None},\n",
       "  {'feature': 'pkey', 'msg': 'Test skipped', 'view': '', 'success': None}],\n",
       " 'INCORRECT_DATETIME_DATA': [{'feature': 'time',\n",
       "   'msg': 'Test passed',\n",
       "   'view': '',\n",
       "   'success': True},\n",
       "  {'feature': 'time', 'msg': 'Test passed', 'view': '', 'success': True}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warning_report = WarningReportLogger(disclosure=3)\n",
    "\n",
    "checker = PreProcessingChecker(multi_format_file_ref, df_joined,'jdl' , warning_report)\n",
    "\n",
    "checker.update_views_features_name(new_views_name)\n",
    "checker.check_all()\n",
    "\n",
    "checker.get_warning_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5baaeeba",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSanityCheckException",
     "evalue": "MissingDataException: Variable pH must not have missing data, but some were found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataSanityCheckException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8673/3919521331.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warning_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8673/1637128002.py\u001b[0m in \u001b[0;36mraise_exception\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_collector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# case where exception collector is not empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataSanityCheckException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_collector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mDataSanityCheckException\u001b[0m: MissingDataException: Variable pH must not have missing data, but some were found"
     ]
    }
   ],
   "source": [
    "checker._warning_logger.raise_exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e7177f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of warnings: 1\n",
      "Number of error: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>msg</th>\n",
       "      <th>view</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pressure</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>e.1</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gender</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blood type</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pkey</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>discrete</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>city</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pkey</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>time</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pH</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pkey</td>\n",
       "      <td>Test passed</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature          msg view  success\n",
       "0            e  Test passed          True\n",
       "1            1  Test passed          True\n",
       "2            2  Test passed          True\n",
       "3         time  Test passed          True\n",
       "4     pressure  Test passed          True\n",
       "5          e.1  Test passed          True\n",
       "6       gender  Test passed          True\n",
       "7   blood type  Test passed          True\n",
       "8         pkey  Test passed          True\n",
       "9     discrete  Test passed          True\n",
       "10        city  Test passed          True\n",
       "11        pkey  Test passed          True\n",
       "12           1  Test passed          True\n",
       "13        time  Test passed          True\n",
       "14          pH  Test passed          True\n",
       "15        pkey  Test passed          True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(checker.get_warning_logger()['INCORRECT_FORMAT_FILE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf6f6934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[__main__.MissingDataException('Variable pH must not have missing data, but some were found')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exception_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "776bdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_TYPE_MISMATCH\n",
      "INCORRECT_DATA_TYPE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_correct_variable_sub_type(multi_format_file_ref[view][feature_name],\n",
    "                                df_joined[n_feature_name],\n",
    "                                            warning_report,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b26e186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warning_report.get_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178708da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarningReportLogger:\n",
    "    def __init__(self, disclosure:int):\n",
    "        self._disclosure = disclosure\n",
    "        \n",
    "        \n",
    "        self._report = {}\n",
    "        self._current_entry = None\n",
    "        self._n_warnings = 1\n",
    "        self._n_exception = 1\n",
    "        self._n_feature = 1\n",
    "        self._saved_msg = None\n",
    "        self._exception_collector = []\n",
    "\n",
    "        \n",
    "    def write_new_entry(self, check: PreProcessingChecks):\n",
    "        self._current_entry = check.name\n",
    "        if check.name not in self._report:\n",
    "            class WarningReportLogger:\n",
    "    def __init__(self, disclosure:int):\n",
    "        self._disclosure = disclosure\n",
    "        \n",
    "        \n",
    "        self._report = {}\n",
    "        self._current_entry = None\n",
    "        self._n_warnings = 1\n",
    "        self._n_exception = 1\n",
    "        self._n_feature = 1\n",
    "        self._saved_msg = None\n",
    "        self._exception_collector = []\n",
    "\n",
    "        \n",
    "    def write_new_entry(self, check: PreProcessingChecks):\n",
    "        self._current_entry = check.name\n",
    "        if check.name not in self._report:\n",
    "            \n",
    "            #self._current_entry = checkand\n",
    "            \n",
    "            if self._disclosure < 2:\n",
    "                if isinstance(check, PreProcessingChecks):\n",
    "                    self._current_entry = 'Warning_' + str(self._n_warnings)\n",
    "                    self._n_warnings += 1\n",
    "                \n",
    "                elif issubclass(check.warning_type, Exception):\n",
    "                    self._current_entry = 'Error_' + str(self._n_exception)\n",
    "                    self._n_exception += 1\n",
    "                else:\n",
    "                    print(\"input not understood\")\n",
    "            \n",
    "            print(self._current_entry)\n",
    "            self._report[self._current_entry] = []\n",
    "        \n",
    "    def write_checking_result(self, success: bool=None, msg:str='', feature_name:str='', view_name: str=''):\n",
    "        \n",
    "        _new_entry = {}\n",
    "        \n",
    "        if success :\n",
    "            msg = \"Test passed\"\n",
    "        elif success is None:\n",
    "            msg = 'Test skipped'\n",
    "        if self._disclosure > 2:\n",
    "            _new_entry['feature'] = feature_name\n",
    "            _new_entry['msg'] = msg\n",
    "        else:\n",
    "            _new_entry['feature'] = 'feature_' + str(self._n_feature)\n",
    "            _new_entry['msg']= ''\n",
    "            self._n_feature += 1\n",
    "        _new_entry['view'] = view_name\n",
    "        _new_entry['success'] = success\n",
    "        self._report[self._current_entry].append(_new_entry)\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_report(self):\n",
    "        print(f'number of warnings: {self._n_warnings}\\nNumber of error: {self._n_exception}')\n",
    "        return self._report\n",
    "    \n",
    "    def clean_report(self):\n",
    "        self._report = {}\n",
    "    def add_exception(self, exception: Exception):\n",
    "        self._exception_collector.append(exception)\n",
    "        \n",
    "    def raise_exception(self):\n",
    "        if self._exception_collector:\n",
    "            # case where exception collector is not empty\n",
    "            raise DataSanityCheckException(self._exception_collector)\n",
    "            #self._current_entry = checkand\n",
    "            \n",
    "            if self._disclosure < 2:\n",
    "                if isinstance(check, PreProcessingChecks):\n",
    "                    self._current_entry = 'Warning_' + str(self._n_warnings)\n",
    "                    self._n_warnings += 1\n",
    "                \n",
    "                elif issubclass(check.warning_type, Exception):\n",
    "                    self._current_entry = 'Error_' + str(self._n_exception)\n",
    "                    self._n_exception += 1\n",
    "                else:\n",
    "                    print(\"input not understood\")\n",
    "            \n",
    "            print(self._current_entry)\n",
    "            self._report[self._current_entry] = []\n",
    "        \n",
    "    def write_checking_result(self, success: bool=None, msg:str='', feature_name:str='', view_name: str=''):\n",
    "        \n",
    "        _new_entry = {}\n",
    "        \n",
    "        if success :\n",
    "            msg = \"Test passed\"\n",
    "        elif success is None:\n",
    "            msg = 'Test skipped'\n",
    "        if self._disclosure > 2:\n",
    "            _new_entry['feature'] = feature_name\n",
    "            _new_entry['msg'] = msg\n",
    "        else:\n",
    "            _new_entry['feature'] = 'feature_' + str(self._n_feature)\n",
    "            _new_entry['msg']= ''\n",
    "            self._n_feature += 1\n",
    "        _new_entry['view'] = view_name\n",
    "        _new_entry['success'] = success\n",
    "        self._report[self._current_entry].append(_new_entry)\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_report(self):\n",
    "        print(f'number of warnings: {self._n_warnings}\\nNumber of error: {self._n_exception}')\n",
    "        return self._report\n",
    "    \n",
    "    def clean_report(self):\n",
    "        self._report = {}\n",
    "    def add_exception(self, exception: Exception):\n",
    "        self._exception_collector.append(exception)\n",
    "        \n",
    "    def raise_exception(self):\n",
    "        if self._exception_collector:\n",
    "            # case where exception collector is not empty\n",
    "            raise DataSanityCheckException(self._exception_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec379ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False is None:\n",
    "    print('l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "310002a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCORRECT_DATA_TYPE\n",
      "INCORRECT_DATETIME_DATA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'INCORRECT_DATA_TYPE': [{'feature': '',\n",
       "   'msg': 'Incorrect Data Type for variable %s: Excpected %s but found %s',\n",
       "   'view': '',\n",
       "   'success': False},\n",
       "  {'feature': '',\n",
       "   'msg': 'Incorrect Data Type for variable %s: Excpected %s but found %s',\n",
       "   'view': '',\n",
       "   'success': False}],\n",
       " 'INCORRECT_DATETIME_DATA': [{'feature': '',\n",
       "   'msg': 'Variable %s has been defined as a DATETIME variable, but samples are not parsable as date',\n",
       "   'view': '',\n",
       "   'success': False}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warning_report = WarningReportLogger(disclosure=3)\n",
    "\n",
    "warning_report.write_new_entry(PreProcessingChecks.INCORRECT_DATA_TYPE)\n",
    "warning_report.write_checking_result(False, PreProcessingChecks.INCORRECT_DATA_TYPE.error_message)\n",
    "warning_report.write_checking_result(False, PreProcessingChecks.INCORRECT_DATA_TYPE.error_message)\n",
    "return self\n",
    "warning_report.write_new_entry(PreProcessingChecks.INCORRECT_DATETIME_DATA)\n",
    "warning_report.write_checking_result(False, PreProcessingChecks.INCORRECT_DATETIME_DATA.error_message)\n",
    "warning_report.get_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d811bb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q e f'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(['q', 'e', 'f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0387d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MissingDataException(Exception):\n",
    "    def __init__(self,message:str=\"\"):\n",
    "        self._message = message\n",
    "        super().__init__(message)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MissingDataException: ' + self._message\n",
    "\n",
    "class MinimumSamplesViolatedException(Exception):\n",
    "    def __init__(self, message: str=\"\"):\n",
    "        self._message = message\n",
    "        super().__init__(message)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MinimumSamplesViolatedException: ' + self._message\n",
    "        \n",
    "class MissingFeatureException(Exception):\n",
    "    def __init__(self, message: str = \"\"):\n",
    "        self._message = message\n",
    "        super().__init__(message)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MissingFeatureException: ' + self._message\n",
    "    \n",
    "class MissingViewException(Exception):\n",
    "    def __init__(self, message: str=\"\"):\n",
    "        self._message = message\n",
    "        super().__init__(message)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MissingViewException' + self._message\n",
    "\n",
    "class DataSanityCheckException(Exception):\n",
    "    def __init__(self, exceptions: List[Exception]):\n",
    "        message  = \"\\n\".join([str(exception) for exception in exceptions])\n",
    "        super().__init__(message)\n",
    "    \n",
    "class WarningType(Enum):\n",
    "    REGULAR_WARNING = 1\n",
    "    CRITICAL_WARNING = 2\n",
    "\n",
    "class PreProcessingChecks(Enum):\n",
    "    INCORRECT_FORMAT_FILE = (\"Format File %s is incorrect: cannot parse variable %s\", WarningType.CRITICAL_WARNING,\n",
    "                            )\n",
    "    KEY_UNICITY_VIOLATED = (\"Key Variable %s violated unicity of data\", WarningType.CRITICAL_WARNING, \n",
    "                           )\n",
    "    MISSING_DATA_NOT_ALLOWED = (\"Variable %s must not have missing data, but some were found\",\n",
    "                                MissingDataException, \n",
    "                               )\n",
    "    MISSING_DATA_ALLOWED = (\"Missing data found in variable %s\", WarningType.REGULAR_WARNING \n",
    "                           )\n",
    "    INCORRECT_STRUCTURE_DATA_TYPE = (\"Data Type %s has an incorrect structure: %s\", WarningType.CRITICAL_WARNING)\n",
    "    DATA_TYPE_MISMATCH = (\"Data Type  %s mismatch: %s is not a subtype of %s\",\n",
    "                           WarningType.REGULAR_WARNING)\n",
    "    \n",
    "    INCORRECT_DATA_TYPE = ('Variable named %s should be a %s variable, but it contains %s type',\n",
    "                          WarningType.REGULAR_WARNING)\n",
    "    INCORRECT_DATETIME_DATA = (\"Variable %s has been defined as a DATETIME variable, but samples are not parsable as date\",\n",
    "                               WarningType.CRITICAL_WARNING)\n",
    "    \n",
    "    OUTLIER_DETECTION_LOWER_BOUND = (\"Detected outliers for Variable %s: samples violate lower bound %s\",\n",
    "                                   WarningType.CRITICAL_WARNING)\n",
    "    \n",
    "    OUTLIER_DETECTION_UPPER_BOUND = (\"Detected outliers for Varaiable %s: samples violate upper bound %s\",\n",
    "                                   WarningType.CRITICAL_WARNING)\n",
    "    \n",
    "    INCORRECT_VALUES_CATEGORICAL_DATA = (\"Found at least one sample with incorrect label in Categorical Vraiable %s. Expected data are %s, but found %s\",\n",
    "                                        WarningType.CRITICAL_WARNING)\n",
    "    \n",
    "    N_MISSING_DATA_ABOVE_THRESHOLD = (\"Found too many missing samples in variable %s, threshold is set at %s\",\n",
    "                                     WarningType.CRITICAL_WARNING)\n",
    "    \n",
    "    N_SAMPLES_BELOW_THRESHOLD = (\"Number of samples contained in dataset %s is below threshold (expected at least %s samples, found %s samples)\",\n",
    "                                MinimumSamplesViolatedException)\n",
    "    MISSING_FEATURE = (\"Feature %s has not been found in dataset, but is needed for experiment\",\n",
    "                       MissingFeatureException)\n",
    "    \n",
    "    #MISSING_VIEW = (\"View %s not found in dataset, but needed for experiment\")\n",
    "    \n",
    "    def __init__(self, message: str,  warning_type: Union[WarningType, Exception]):\n",
    "        self._message = message\n",
    "        #self._additional_message = additional_message\n",
    "        self._warning_type = warning_type\n",
    "        #self._is_exception = is_exception\n",
    "        \n",
    "    @property\n",
    "    def error_message(self):\n",
    "        return self._message\n",
    "\n",
    "    @property\n",
    "    def warning_type(self):\n",
    "        return self._warning_type\n",
    "    \n",
    "    @property\n",
    "    def additional_message(self):\n",
    "        return self._additional_message\n",
    "    \n",
    "    def __call__(self, *kwargs) -> Union[str, Exception]:\n",
    "        \n",
    "        msg = self.error_message % kwargs\n",
    "        if isinstance(self.warning_type, WarningType):\n",
    "            \n",
    "            return msg\n",
    "        elif issubclass(self.warning_type, Exception):\n",
    "            \n",
    "            return self.warning_type(message=msg)\n",
    "\n",
    "    \n",
    "def raise_warning(warning: PreProcessingChecks, *kwargs) -> str:\n",
    "    if isinstance(warning.warning_type, WarningType):\n",
    "        #warning.warning_type.value(warning_disclosure)\n",
    "        \n",
    "        return warning.error_message % kwargs\n",
    "    elif issubclass(warning.warning_type, Exception):\n",
    "        \n",
    "        raise warning(*kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "540ccf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', '5', '55')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.MinimumSamplesViolatedException('Number of samples contained in dataset 2 is below threshold (expected at least 5 samples, found 55 samples)')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreProcessingChecks.N_SAMPLES_BELOW_THRESHOLD('2', '5', '55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed8f5044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MissingDataException: djkfff'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(MissingDataException('djkfff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b94e8d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WarningType.REGULAR_WARNING: 1>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreProcessingChecks.MISSING_DATA_ALLOWED.warning_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6250fd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Missing data found in variable kdld'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raise_warning(PreProcessingChecks.MISSING_DATA_ALLOWED, 'kdld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4d29c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISSING_DATA_ALLOWED'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreProcessiINCORRECT_DATETIME_DATngChecks.MISSING_DATA_ALLOWED.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2827012b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(PreProcessingChecks.MISSING_DATA_ALLOWED.name, 'WarningType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1d90995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jfkfl djk\n"
     ]
    }
   ],
   "source": [
    "def func(*kwargs:str):\n",
    "    print(*kwargs)\n",
    "    \n",
    "func('jfkfl', 'djk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f775e",
   "metadata": {},
   "source": [
    "class PreProcessingChecker:\n",
    "    def check_...\n",
    "    def check_all(view=None, feature=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a5c2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NB_SAMPLES = 30  # TODO: specify it in data formt file (clinician should define it)\n",
    "\n",
    "class PreProcessingChecker:\n",
    "    def __init__(self, file_format_ref:dict,\n",
    "                data_frame: pd.DataFrame,\n",
    "                file_format_ref_name:str, \n",
    "                warning_logger: WarningReportLogger):\n",
    "        self._file_format_ref = file_format_ref\n",
    "        self._data_frame = data_frame\n",
    "        self._file_format_ref_name = file_format_ref_name\n",
    "        self._warning_logger = warning_logger\n",
    "        self._new_features_name = None\n",
    "        \n",
    "        self._view_feature_names = {v: [f for f in file_format_ref[v].keys()] for v in file_format_ref.keys()}\n",
    "        self._features = None\n",
    "        \n",
    "        self._warning_logger.clean_report()\n",
    "    \n",
    "    def _get_all_features(self, view:str) -> List[str]:\n",
    "        return self._view_feature_names.get(view)\n",
    "    \n",
    "    def _get_feature_defined_in_format_file(self, view:str, feature:str)-> str:\n",
    "        #feature_name = self._file_format_ref[view][feature]\n",
    "        print(view, feature)\n",
    "        if self._new_features_name is not None:\n",
    "            \n",
    "            _new_features_name = self._new_features_name.get(view)\n",
    "            \n",
    "            if _new_features_name is not None and feature in _new_features_name.keys():\n",
    "                feature = _new_features_name.get(feature)\n",
    "                \n",
    "        return feature\n",
    "    \n",
    "    def get_warning_logger(self):\n",
    "        return self._warning_logger.get_report()\n",
    "    \n",
    "    def update_views_features_name(self, new_features_name: Dict[str, Dict[str, str]]):\n",
    "        self._new_features_name = new_features_name\n",
    "        for view in self._view_feature_names:\n",
    "            if view in new_features_name:\n",
    "                for former_feature_name, new_feature_name in new_features_name[view].items():\n",
    "                    self._view_feature_names[view].append(new_feature_name)\n",
    "                    self._view_feature_names[view].remove(former_feature_name)\n",
    "        print('features names updated')\n",
    "    \n",
    "    def check_all(self, view:str=None, feature:str=None):\n",
    "        \n",
    "        if view is not None:\n",
    "            _views = [view]\n",
    "        \n",
    "        else:\n",
    "            _views = self._view_feature_names\n",
    "        \n",
    "        \n",
    "        for _view in _views:\n",
    "            # define here test that happens on whole dataset\n",
    "            \n",
    "            ###\n",
    "            self.check_number_of_samples(MIN_NB_SAMPLES, _view)\n",
    "            \n",
    "            \n",
    "            ####\n",
    "            _features = self._file_format_ref[_view].keys()\n",
    "            \n",
    "            for _feature in _features:\n",
    "                \n",
    "                # check fi feature does exist\n",
    "                _is_feature_exist = self.check_feature_exists_in_dataset(_view,\n",
    "                                                                         _feature)\n",
    "                if not _is_feature_exist:\n",
    "                    continue\n",
    "                \n",
    "                #\n",
    "                _is_format_file_correct = self.check_missing_entry_format_file_ref(_view,\n",
    "                                                                                  _feature)\n",
    "                if not _is_format_file_correct:\n",
    "                    continue\n",
    "                self.check_correct_variable_sub_type(_view, _feature)\n",
    "                self.check_missing_values(_view, _feature)\n",
    "                self.check_lower_bound(_view, _feature)\n",
    "                self.check_upper_bound(_view, _feature)\n",
    "                self.check_values_in_categorical_variable(_view, _feature)\n",
    "                \n",
    "                _feature_data_type = self._file_format_ref[_view][_feature]['data_type']\n",
    "                \n",
    "                if _feature_data_type == DataType.DATETIME.name:\n",
    "                    self.check_datetime_variable_compliance(_view,\n",
    "                                                           _feature)\n",
    "                    \n",
    "                if _feature_data_type == DataType.KEY.name:\n",
    "                    self.check_key_variable_compliance(_view,\n",
    "                                                      _feature)\n",
    "    \n",
    "    def check_number_of_samples(self, min_nb_samples: int, view_name: str='') -> bool:\n",
    "        #Checking samples limit\n",
    "\n",
    "        feature_name ='ALL' \n",
    "        sample_count = self._data_frame.shape[0]\n",
    "           \n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.N_SAMPLES_BELOW_THRESHOLD)\n",
    "        if sample_count < min_nb_samples:\n",
    "            success = False\n",
    "            try:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.N_SAMPLES_BELOW_THRESHOLD,\n",
    "                                            view_name, min_nb_samples, sample_count)\n",
    "            except MinimumSamplesViolatedException as err:\n",
    "                print(err)\n",
    "                self._warning_logger.add_exception(err)\n",
    "                warning_msg = str(err)\n",
    "            #message = critical_warning.display(f'Samples count exceeds the threshold limit {MIN_NB_SAMPLES}')\n",
    "        else:\n",
    "            success = True\n",
    "            warning_msg='Test passed'\n",
    "\n",
    "        self._warning_logger.write_checking_result(success,\n",
    "                                                   warning_msg,\n",
    "                                                   feature_name,\n",
    "                                                   view_name)\n",
    "\n",
    "        return success\n",
    "    \n",
    "    def check_feature_exists_in_dataset(self,\n",
    "                                        view:str,\n",
    "                                     feature_name: str) -> bool:\n",
    "        renamed_feature_name = self._get_feature_defined_in_format_file(view, feature_name)\n",
    "        if renamed_feature_name in self._data_frame.columns:\n",
    "            success = True\n",
    "        else:\n",
    "            success = False\n",
    "            self._warning_logger.write_new_entry(PreProcessingChecks.MISSING_FEATURE)\n",
    "            try:\n",
    "                raise_warning(PreProcessingChecks.MISSING_FEATURE,\n",
    "                                        feature_name)\n",
    "            except MissingFeatureException as exc:\n",
    "                warning_msg = str(exc)\n",
    "                self._warning_logger.add_exception(exc)\n",
    "            self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "            \n",
    "        return success\n",
    "    \n",
    "    def check_missing_entry_format_file_ref(self, view:str,\n",
    "                                          feature_name:str) -> bool:\n",
    "        \"\"\"Tests if format file ref is parsable\"\"\"\n",
    "        \n",
    "        success = True\n",
    "        warning_msg = 'Test passed'\n",
    "        \n",
    "        renamed_feature_name = self._get_feature_defined_in_format_file(view, feature_name)\n",
    "        \n",
    "        _view_format_file = self._file_format_ref[view]\n",
    "        _feature_format_file = _view_format_file.get(feature_name)\n",
    "        if _feature_format_file is not None:\n",
    "            _data_format_name = _feature_format_file.get('data_format')\n",
    "            _data_type_name = _feature_format_file.get('data_type')\n",
    "        else:\n",
    "            _data_format_name, _data_type_name = None, None\n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_FORMAT_FILE)\n",
    "        if _data_format_name is None or _data_type_name is None:\n",
    "            #success: bool, msg:str='', feature_name:str='\n",
    "            warning_msg = raise_warning(PreProcessingChecks.INCORRECT_FORMAT_FILE,\n",
    "                                    self._file_format_ref_name,\n",
    "                                    renamed_feature_name)\n",
    "\n",
    "            success = False\n",
    "        self._warning_logger.write_checking_result(success,\n",
    "                                             warning_msg,\n",
    "                                             feature_name)  \n",
    "        return success\n",
    "    \n",
    "    def check_correct_variable_sub_type(self, \n",
    "                                        view_name:str,\n",
    "                                    feature_name:str,\n",
    "                                    ) -> bool:\n",
    "        \"\"\"checks consistancy between general data type and subtype\"\"\"\n",
    "        \n",
    "        renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        \n",
    "        column = self._data_frame[renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        success = True\n",
    "        warning_msg = 'test passed'\n",
    "        data_format_name = _feature_format_ref.get('data_format')\n",
    "        data_type_name = _feature_format_ref.get('data_type')\n",
    "\n",
    "        #feature_name = column.name\n",
    "\n",
    "        # first test\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.DATA_TYPE_MISMATCH)\n",
    "        if data_format_name is None or data_type_name is None:\n",
    "\n",
    "            warning_msg = 'test skipped'\n",
    "        else:\n",
    "            try:\n",
    "                data_type = utils.find_data_type(data_format_name, data_type_name)\n",
    "                warning_msg = 'test passed'\n",
    "            except ValueError as err:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.DATA_TYPE_MISMATCH, \n",
    "                                           data_format_name, data_type_name)\n",
    "                success = False\n",
    "\n",
    "        self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "\n",
    "        # second test \n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_DATA_TYPE)\n",
    "        if data_format_name is None or data_type_name is None:\n",
    "            warning_msg = 'test skipped'\n",
    "        else:\n",
    "            actual_dtype = column.dtype\n",
    "\n",
    "\n",
    "            _does_column_have_correct_data_type = any(t == actual_dtype for t in data_type.value)\n",
    "            if not _does_column_have_correct_data_type:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.INCORRECT_DATA_TYPE, \n",
    "                                           feature_name, data_type_name, str(actual_dtype))\n",
    "                success = False\n",
    "            else:\n",
    "                warning_msg = 'test passed'\n",
    "            self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "\n",
    "        return success\n",
    "    \n",
    "    \n",
    "    def check_missing_values(self, \n",
    "                             view_name: str,\n",
    "                         feature_name: str) -> bool:\n",
    "        \"\"\"checks if missing data are present in column, and triggers error depending\n",
    "        of the fact that missing data are whether allowed or not in the format_ref_file\"\"\"\n",
    "        \n",
    "        renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        _is_missing_data = utils.check_missing_data(_column)\n",
    "        _is_missing_values_authorized = _feature_format_ref.get('is_missing_values', 'test_skipped')\n",
    "        success = True\n",
    "\n",
    "        \n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.MISSING_DATA_ALLOWED)\n",
    "\n",
    "        if _is_missing_values_authorized == 'test_skipped':\n",
    "            warning_msg = 'Test skipped'\n",
    "            success = None\n",
    "        elif _is_missing_data:\n",
    "            success = False\n",
    "            # test fails: \n",
    "            if _is_missing_values_authorized:\n",
    "                # case where missing values are present BUT allowed\n",
    "                warning_msg = raise_warning(PreProcessingChecks.MISSING_DATA_ALLOWED,\n",
    "                                           feature_name)\n",
    "            else:\n",
    "                # case where missing values are present AND NOT allowed\n",
    "                try:\n",
    "                    warning_msg = raise_warning(PreProcessingChecks.MISSING_DATA_NOT_ALLOWED,\n",
    "                                               feature_name)\n",
    "                except MissingDataException as err:\n",
    "                    print(err)\n",
    "                    self._warning_logger.add_exception(err)\n",
    "                    warning_msg = str(err)\n",
    "        else:\n",
    "            # test passed\n",
    "            warning_msg = 'Test passed'\n",
    "\n",
    "        self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "\n",
    "        return success\n",
    "    \n",
    "    \n",
    "    def check_lower_bound(self,\n",
    "                          view_name:str,\n",
    "                          feature_name:str) -> bool:\n",
    "    \n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        \n",
    "        # remove nan (missing values) from vriable \n",
    "        _column_without_nan = _column.dropna()\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        lower_bound = _feature_format_ref.get('lower_bound')\n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND)\n",
    "        if lower_bound is not None:\n",
    "\n",
    "            # should work for both numerical and datetime data types\n",
    "\n",
    "            is_lower_bound_correct = np.all(_column_without_nan >= lower_bound)\n",
    "\n",
    "\n",
    "            if not is_lower_bound_correct:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND,\n",
    "                                               feature_name, lower_bound)\n",
    "            else:\n",
    "                warning_msg = 'Test passed'\n",
    "        else:\n",
    "            warning_msg = 'Test skipped'\n",
    "            is_lower_bound_correct = None\n",
    "        self._warning_logger.write_checking_result(is_lower_bound_correct, warning_msg, feature_name)\n",
    "\n",
    "        return is_lower_bound_correct\n",
    "    \n",
    "    \n",
    "    def check_upper_bound(self, view_name: str, feature_name:str) -> bool:\n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        \n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        \n",
    "        # remove nan (missing values) from vriable \n",
    "        _column_without_nan = _column.dropna()\n",
    "        upper_bound = _feature_format_ref.get('upper_bound')\n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.OUTLIER_DETECTION_UPPER_BOUND)\n",
    "        if upper_bound is not None:\n",
    "             # should work for both numerical and datetime data sets\n",
    "            is_upper_bound_correct = np.all(_column_without_nan <= lower_bound)\n",
    "\n",
    "            if not is_upper_bound_correct:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND,\n",
    "                                            feature_name, upper_bound)\n",
    "            else:\n",
    "                warning_nsg = 'Test passed'\n",
    "\n",
    "        else:\n",
    "            warning_msg = 'Test skipped'\n",
    "            is_upper_bound_correct = None\n",
    "\n",
    "        self._warning_logger.write_checking_result(is_upper_bound_correct, warning_msg, feature_name)\n",
    "        return is_upper_bound_correct\n",
    "    \n",
    "    \n",
    "    def check_values_in_categorical_variable(self, \n",
    "                                             view_name:str,\n",
    "                                             feature_name:str)-> bool:\n",
    "        \"\"\"Checks if values are contained in categorical variables\"\"\"\n",
    "\n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        \n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        categorical_values = _feature_format_ref.get('categorical_values')\n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_VALUES_CATEGORICAL_DATA)\n",
    "        if categorical_values is None:\n",
    "            warning_msg = 'test skipped'\n",
    "            success = None\n",
    "        else:\n",
    "            unique_values = utils.unique(column)\n",
    "            success = True\n",
    "            for val in unique_values:\n",
    "                if val not in categorical_values and not np.isnan(val):\n",
    "                    warning_msg = raise_warning(PreProcessingChecks.INCORRECT_VALUES_CATEGORICAL_DATA,\n",
    "                                               feature_name, val, *categorical_values)\n",
    "                    success = False\n",
    "            if success:\n",
    "                warning_msg = 'test passed'\n",
    "        self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "        return success\n",
    "\n",
    "    def check_missing_values_threshold(self,\n",
    "                                       view_name: str,\n",
    "                                   feature_name: str,\n",
    "                                  threshold: int = 50) -> bool:\n",
    "        #Checking if missing values exceed threshold limit(50%)\n",
    "        \n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        \n",
    "        min_nb_missing_data = math.ceil((threshold/100)*_column.shape[0])\n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.N_MISSING_DATA_ABOVE_THRESHOLD)\n",
    "        n_missing_data = _column.isnull().sum()\n",
    "        if (n_missing_data>min_nb_missing_data):\n",
    "            success = False\n",
    "            #message = critical_warning.display(f'Missing value exceeds threshold limit {MIN_NB_MISSING_DATA}',col) \n",
    "            warning_msg = raise_warning(PreProcessingChecks.N_MISSING_DATA_ABOVE_THRESHOLD,\n",
    "                                        feature_name, n_missing_data,\n",
    "                                        min_nb_missing_data)\n",
    "        else:\n",
    "            success = True\n",
    "            warning_msg ='Test passed'\n",
    "\n",
    "        #report['check_missing_values_limit'] = report_details\n",
    "        self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "        return success\n",
    "    \n",
    "    \n",
    "    def check_key_variable_compliance(self, \n",
    "                                      view_name:str,\n",
    "                                      feature_name:str) -> bool:\n",
    "        \"\"\"Performs data sanity check over variable of type `KEY`\n",
    "        warning should be Critical warnings\n",
    "        \"\"\"\n",
    "        # variables initialisation\n",
    "\n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        \n",
    "        # 1. check unicity of values in column\n",
    "\n",
    "        n_unique_samples = utils.unique(_column, number=True)\n",
    "        n_samples = _column.shape[0]\n",
    "\n",
    "        \n",
    "\n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.KEY_UNICITY_VIOLATED)\n",
    "        if n_unique_samples != n_samples:\n",
    "            success = False\n",
    "            warning_msg = raise_warning(PreProcessingChecks.KEY_UNICITY_VIOLATED,\n",
    "                                       feature_name,)\n",
    "        else:\n",
    "            warning_msg = 'Test passed'\n",
    "            success = True\n",
    "        self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "\n",
    "        return success\n",
    "\n",
    "\n",
    "    def check_datetime_variable_compliance(self,\n",
    "                                           view_name:str,\n",
    "                                           feature_name:str) -> bool:\n",
    "        \"\"\"additional data sanity checks for datetime variable\"\"\"\n",
    "        # test 1. check if datetime is parsable\n",
    "        \n",
    "        _renamed_feature_name = self._get_feature_defined_in_format_file(view_name,\n",
    "                                                                        feature_name)\n",
    "        _column = self._data_frame[_renamed_feature_name]\n",
    "        _feature_format_ref = self._file_format_ref[view_name][feature_name]\n",
    "        \n",
    "        # remove missing values (nan) from column\n",
    "        _column_without_nan = _column.dropna()\n",
    "        are_datetime_parsables =  np.all(_column_without_nan.apply(utils.is_datetime))\n",
    "        \n",
    "        self._warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_DATETIME_DATA)\n",
    "\n",
    "        if not are_datetime_parsables:\n",
    "            \n",
    "            print('Warning: at least one variable is not a datetime')\n",
    "            warning_msg = raise_warning(PreProcessingChecks.INCORRECT_DATETIME_DATA,\n",
    "                                        feature_name)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            warning_msg = 'Test passed'\n",
    "        self._warning_logger.write_checking_result(are_datetime_parsables,\n",
    "                                                     warning_msg,\n",
    "                                                     feature_name) \n",
    "\n",
    "        return are_datetime_parsables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b31892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_data_sanity_checks(data_format_ref: Dict[str, Dict[str, Any]],\n",
    "                           data_frame: pd.DataFrame,\n",
    "                           new_features_names : Dict[str, str],\n",
    "                           view_name: str, \n",
    "                           data_format_ref_name: str,\n",
    "                            warning_logger: WarningReportLogger):\n",
    "    \"\"\"\n",
    "    Runs all data sanity checks\n",
    "    Args:\n",
    "    \n",
    "     - data_frame: \n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    # run tests related to the whole dataset\n",
    "    \n",
    "    # data anity check : check if number of samples is above threshold\n",
    "    check_number_of_samples(data_frame, MIN_NB_SAMPLES, view_name,print(sel)\n",
    "                            warning_logger)\n",
    "    # iterates over all features within a view\n",
    "    for feature in data_format_ref:\n",
    "        if feature in new_feature_name:\n",
    "            # rename feature ( it has been renamed after join operation,\n",
    "            # because 2 or more views have the same feature name)\n",
    "            feature = new_feature_name.get(feature)\n",
    "        # data sanity check : check if format file is parsable\n",
    "        _is_format_file_correct = check_missing_entry_format_file_ref(data_format_ref[feature],\n",
    "                                           feature,\n",
    "                                            data_format_ref_name,\n",
    "                                           warning_logger)\n",
    "        \n",
    "        if _is_format_file_correct:\n",
    "            # can not parse the format_file_ref, so skipping next data sanity check\n",
    "            continue\n",
    "        # data sanity check: check if feature defined in format_file_ref, \n",
    "        # is also present in data_frame\n",
    "        check_feature_exists_in_dataset(data_frame, \n",
    "                                       feature,\n",
    "                                       warning_logger)\n",
    "        \n",
    "        # data sanity check: check if type and sub type are consistants\n",
    "        check_correct_variable_sub_type(data_format_ref[feature],\n",
    "                                        data_frame[feature],\n",
    "                                    warning_logger)\n",
    "        #check_missing_entry_format_file_ref()\n",
    "    \n",
    "        # data sanity check: check if for the specified feature, missing data\n",
    "        # are allowed\n",
    "        check_missing_values(data_format_ref[feature],\n",
    "                             data_frame[feature],\n",
    "                             warning_logger)\n",
    "        \n",
    "        # data sanity check: check if number of missing value in\n",
    "        # feature is below threshold\n",
    "def check_feature_exists_in_dataset(data_frame: pd.DataFrame,\n",
    "                                     feature_name: str,\n",
    "                                    warning_logger: WarningReportLogger) -> bool:\n",
    "    \n",
    "    if feature_name in data_frame.column:\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "        warning_logger.write_new_entry(PreProcessingChecks.MISSING_FEATURE)\n",
    "        try:\n",
    "            raise_warning(PreProcessingChecks.MISSING_FEATURE,\n",
    "                                    feature_name)\n",
    "        except MissingDataException as exc:\n",
    "            warning_msg = str(exc)\n",
    "        warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "        warning_logger.add_exception(exc)\n",
    "        \n",
    "    return success\n",
    "\n",
    "def check_key_variable_compliance(column: pd.Series,\n",
    "                                  warning_logger: WarningReportLogger=None) -> bool:\n",
    "    \"\"\"performs data sanity check over variable of type `KEY`\n",
    "    warning should be Critical warnings\n",
    "    \"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    \n",
    "    # 1. check unicity of values in column\n",
    "    \n",
    "    n_unique_samples = utils.unique(column, number=True)\n",
    "    n_samples = column.shape[0]_new_features_name.keys()\n",
    "    \n",
    "    feature_name = column.name\n",
    "    \n",
    "    warning_logger.write_new_entry(PreProcessingChecks.KEY_UNICITY_VIOLATED)\n",
    "    if n_unique_samples != n_samples:\n",
    "        success = False\n",
    "        warning_msg = raise_warning(PreProcessingChecks.KEY_UNICITY_VIOLATED,\n",
    "                                   feature_name,)\n",
    "    else:\n",
    "        warning_msg = 'Test passed'\n",
    "        success = True\n",
    "    warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "                \n",
    "    return success\n",
    "\n",
    "\n",
    "def check_datetime_variable_compliance(column: pd.Series, \n",
    "                                      warning_logger: WarningReportLogger):\n",
    "    \"\"\"additional data sanity checks for datetime variable\"\"\"\n",
    "    # test 1. check if datetime is parsable\n",
    "    \n",
    "    # remove nan\n",
    "    column_without_nan = column.dropna()\n",
    "    are_datetime_parsables =  np.all(column.apply(utils.is_datetime))\n",
    "    feature_name = column.namenew_features_name\n",
    "    warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_DATETIME_DATA)\n",
    "    \n",
    "    if not are_datetime_parsables:\n",
    "        success = False\n",
    "        print('Warning: at least one variable is not a datetime')\n",
    "        warning_msg = raise_warning(PreProcessingChecks.INCORRECT_DATETIME_DATA,\n",
    "                                    feature_name)\n",
    "        \n",
    "    else:\n",
    "        success = True\n",
    "        warning_msg = 'Test passed'\n",
    "    warning_logger.write_checking_result(success,\n",
    "                                     warning_msg,\n",
    "                                     feature_name) \n",
    "    \n",
    "    return success\n",
    "\n",
    "def check_missing_entry_format_file_ref(format_file_ref: Dict[str, Any],\n",
    "                                          feature_name:str='',\n",
    "                                        format_file_ref_name:str='',\n",
    "                                       warning_logger: WarningReportLogger=None) -> bool:\n",
    "    \"\"\"Tests if format file ref is parsable\"\"\"\n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    success = True\n",
    "    warning_msg = 'Test passed'\n",
    "    \n",
    "    warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_FORMAT_FILE)\n",
    "    if data_format_name is None or data_type_name is None:\n",
    "        #success: bool, msg:str='', feature_name:str='\n",
    "        warning_msg = raise_warning(PreProcessingChecks.INCORRECT_FORMAT_FILE,\n",
    "                                format_file_ref_name,\n",
    "                                feature_name)\n",
    "        \n",
    "        success = False\n",
    "    warning_logger.write_checking_result(success,\n",
    "                                         warning_msg,\n",
    "                                         feature_name)  \n",
    "    return success\n",
    "    \n",
    "def check_correct_variable_sub_type(format_file_ref: Dict[str, Any],\n",
    "                                    column: pd.Series,\n",
    "                                    warning_logger: WarningReportLogger,\n",
    "                                    ) -> bool:\n",
    "    \"\"\"checks consistancy between general data type and subtype\"\"\"\n",
    "    success = True\n",
    "    warning_msg = 'test passed'\n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    \n",
    "    feature_name = column.name\n",
    "    \n",
    "    # first test\n",
    "    warning_logger.write_new_entry(PreProcessingChecks.DATA_TYPE_MISMATCH)\n",
    "    if data_format_name is None or data_type_name is None:\n",
    "        \n",
    "        warning_msg = 'test skipped'\n",
    "    else:\n",
    "        try:\n",
    "            data_type = utils.find_data_type(data_format_name, data_type_name)\n",
    "            warning_msg = 'test passed'\n",
    "        except ValueError as err:\n",
    "            warning_msg = raise_warning(PreProcessingChecks.DATA_TYPE_MISMATCH, \n",
    "                                       data_format_name, data_type_name)\n",
    "            success = False\n",
    "    \n",
    "    self._warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "    \n",
    "    # second test \n",
    "    warning_logger.write_new_entry(PreProcessingChecks.INCORRECT_DATA_TYPE)\n",
    "    if data_format_name is None or data_type_name is None:\n",
    "        warning_msg = 'test skipped'\n",
    "    else:\n",
    "        actual_dtype = column.dtype\n",
    "    \n",
    "    \n",
    "        _does_column_have_correct_data_type = any(t == actual_dtype for t in data_type.value)\n",
    "        if not _does_column_have_correct_data_type:\n",
    "            warning_msg = raise_warning(PreProcessingChecks.INCORRECT_DATA_TYPE, \n",
    "                                       feature_name, data_type_name, str(actual_dtype))\n",
    "            success = False\n",
    "        else:\n",
    "            warning_msg = 'test passed'\n",
    "        warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "            \n",
    "    return success\n",
    "\n",
    "\n",
    "def check_missing_values(format_file_ref: Dict[str, Any],\n",
    "                         column: pd.Series,\n",
    "                         warning_logger:WarningReportLogger) -> bool:\n",
    "    \"\"\"checks if missing data are present in column, and triggers error depending\n",
    "    of the fact that missing data are whether allowed or not in the format_ref_file\"\"\"\n",
    "    warning_logger: WarningReportLogger\n",
    "    _is_missing_data = utils.check_missing_data(column)\n",
    "    _is_missing_values_authorized = format_file_ref.get('is_missing_values', 'test_skipped')\n",
    "    success = True\n",
    "    \n",
    "    feature_name = column.name\n",
    "    warning_report.write_new_entry(PreProcessingChecks.MISSING_DATA_ALLOWED)\n",
    "    \n",
    "    if _is_missing_values_authorized == 'test_skipped':\n",
    "        warning_msg = 'Test skipped'\n",
    "        success = None\n",
    "    elif _is_missing_data:\n",
    "        success = False\n",
    "        # test fails: \n",
    "        if _is_missing_values_authorized:\n",
    "            # case where missing values are present BUT allowed\n",
    "            warning_msg = raise_warning(PreProcessingChecks.MISSING_DATA_ALLOWED,\n",
    "                                       feature_name)\n",
    "        else:\n",
    "            # case where missing values are present AND NOT allowed\n",
    "            try:\n",
    "                warning_msg = raise_warning(PreProcessingChecks.MISSING_DATA_NOT_ALLOWED,\n",
    "                                           feature_name)\n",
    "            except MissingDataException as err:\n",
    "                print(err)\n",
    "                warning_logger.add_exception(err)\n",
    "                warning_msg = str(err)\n",
    "    else:\n",
    "        # test passed\n",
    "        warning_msg = 'Test passed'\n",
    "    \n",
    "    warning_report.write_checking_result(success, warning_msg, feature_name)\n",
    "        \n",
    "    return success\n",
    "\n",
    "\n",
    "def check_lower_bound(format_file_ref: Dict[str, Any],\n",
    "                      column: pd.Series,\n",
    "                      warning_logger:WarningReportLogger) -> bool:\n",
    "    \n",
    "    feature_name = column.name\n",
    "    # remove nan (missing values) from vriable \n",
    "    column_without_nan = column.dropna()\n",
    "    \n",
    "    lower_bound = format_file_ref.get('lower_bound')\n",
    "    \n",
    "    warning_report.write_new_entry(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND)\n",
    "    if lower_bound is not None:\n",
    "        \n",
    "        # should work for both numerical and datetime data types\n",
    "        \n",
    "        is_lower_bound_correct = np.all(column_without_nan >= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_lower_bound_correct:\n",
    "            warning_msg = raise_warning(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND,\n",
    "                                           feature_name, lower_bound)\n",
    "        else:\n",
    "            warning_msg = 'Test passed'\n",
    "    else:\n",
    "        warning_msg = 'Test skipped'\n",
    "        is_lower_bound_correct = None\n",
    "    warning_logger.write_checking_result(is_lower_bound_correct, warning_msg, feature_name)\n",
    "    \n",
    "    return is_lower_bound_correct\n",
    "\n",
    "def check_upper_bound(format_file_ref: Dict[str, Any],\n",
    "                      column: pd.Series,\n",
    "                      warning_logger:WarningReportLogger) -> bool:\n",
    "    feature_name = column.name\n",
    "    # remove nan (missing values) from vriable \n",
    "    column_without_nan = column.dropna()\n",
    "    upper_bound = format_file_ref.get('upper_bound')\n",
    "    \n",
    "    warning_report.write_new_entry(PreProcessingChecks.OUTLIER_DETECTION_UPPER_BOUND)\n",
    "    if upper_bound is not None:\n",
    "         # should work for both numerical and datetime data sets\n",
    "        is_upper_bound_correct = np.all(column_without_nan <= lower_bound)\n",
    "        \n",
    "        if not is_upper_bound_correct:\n",
    "            warning_msg = raise_warning(PreProcessingChecks.OUTLIER_DETECTION_LOWER_BOUND,\n",
    "                                        feature_name, upper_bound)\n",
    "        else:\n",
    "            warning_nsg = 'Test passed'\n",
    "            \n",
    "    else:\n",
    "        warning_msg = 'Test skipped'\n",
    "        is_upper_bound_correct = None\n",
    "        \n",
    "    warning_logger.write_checking_result(is_upper_bound_correct, warning_msg, feature_name)\n",
    "    return is_upper_bound_correct\n",
    "\n",
    "\n",
    "def check_values_in_categorical_variable(format_file_ref: Dict[str, Any],\n",
    "                                        column: pd.Series,\n",
    "                                        warning_logger:WarningReportLogger)-> bool:\n",
    "    \"\"\"Checks if values are contained in categorical variables\"\"\"\n",
    "    \n",
    "    \n",
    "    feature_name = column.name\n",
    "    categorical_values = format_file_ref.get('categorical_values')\n",
    "    \n",
    "    warning_report.write_new_entry(PreProcessingChecks.INCORRECT_VALUES_CATEGORICAL_DATA)\n",
    "    if categorical_values is None:\n",
    "        warning_msg = 'test skipped'\n",
    "        success = None\n",
    "    else:\n",
    "        unique_values = utils.unique(column)\n",
    "        success = True\n",
    "        for val in unique_values:\n",
    "            if val not in categorical_values and not np.isnan(val):\n",
    "                warning_msg = raise_warning(PreProcessingChecks.INCORRECT_VALUES_CATEGORICAL_DATA,\n",
    "                                           feature_name, val, *categorical_values)\n",
    "                success = False\n",
    "        if success:\n",
    "            warning_msg = 'test passed'\n",
    "    warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "    return success\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_missing_values_threshold(column: pd.Series,\n",
    "                                   warning_logger :WarningReportLogger,\n",
    "                                  threshold: int = 50) -> bool:\n",
    "    #Checking if missing values exceed threshold limit(50%)\n",
    "    feature_name = column.name\n",
    "    min_nb_missing_data = math.ceil((threshold/100)*column.shape[0])\n",
    "\n",
    "    warning_logger.write_new_entry(PreProcessingChecks.N_MISSING_DATA_ABOVE_THRESHOLD)\n",
    "    n_missing_data = data[col].isnull().sum()\n",
    "    if (n_missing_data>min_nb_missing_data):\n",
    "        success = False\n",
    "        #message = critical_warning.display(f'Missing value exceeds threshold limit {MIN_NB_MISSING_DATA}',col) \n",
    "        warning_msg = raise_warning(PreProcessingChecks.N_MISSING_DATA_ABOVE_THRESHOLD,\n",
    "                                    feature_name, n_missing_data,\n",
    "                                    min_nb_missing_data)\n",
    "    else:\n",
    "        success = True\n",
    "        warning_msg ='Test passed'\n",
    "   \n",
    "    #report['check_missing_values_limit'] = report_details\n",
    "    warning_logger.write_checking_result(success, warning_msg, feature_name)\n",
    "    return success\n",
    "\n",
    "\n",
    "def check_number_of_samples(data: Union[pd.DataFrame, pd.Series],\n",
    "                            min_nb_samples: int,\n",
    "                            view_name: str='',\n",
    "                            warning_logger:CustomWarning=None) -> bool:\n",
    "    #Checking samples limit\n",
    "    \n",
    "    sample_count = data.shape[0]\n",
    "    feature_name ='ALL'    \n",
    "    \n",
    "    warning_logger.write_new_entry(PreProcessingChecks.N_SAMPLES_BELOW_THRESHOLD)\n",
    "    if sample_count> min_nb_samples:\n",
    "        success = False\n",
    "        try:\n",
    "            warning_msg = raise_warning(PreProcessingChecks.N_SAMPLES_BELOW_THRESHOLD,\n",
    "                                        view_name, min_nb_samples, sample_count)\n",
    "        except MinimumSamplesViolatedException as err:\n",
    "            print(err)\n",
    "            \n",
    "        #message = critical_warning.display(f'Samples count exceeds the threshold limit {MIN_NB_SAMPLES}')\n",
    "    else:\n",
    "        success = True\n",
    "        warning_msg='Test passed'\n",
    "        \n",
    "    warning_logger.write_checking_result(success, warning_msg, feature_name, view_name)\n",
    "\n",
    "    return success\n",
    "\n",
    "def check_variable_compliance(column: pd.Series,\n",
    "                               format_file_ref: Dict[str, Any],\n",
    "                               col_name:str=None,\n",
    "                               warning=None) -> Tuple[bool, bool]:\n",
    "    \"\"\"performs a data sanity check on variable `col_name` given instruction in \n",
    "    data_file_ref\n",
    "    \"\"\"\n",
    "    is_test_passed = True\n",
    "    \n",
    "    \n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    # remove nan (missing values) from \n",
    "    column_without_nan = column.dropna()\n",
    "    \n",
    "    \n",
    "    if data_format_name is None:\n",
    "        print(f'critical wraning: data fromat {data_format_name} not understood')\n",
    "    # 1. check data sub type\n",
    "    try:\n",
    "        data_type = utils.find_data_type(data_format_name, data_type_name)\n",
    "    except ValueError as err:\n",
    "        data_type = None\n",
    "        print('Critical warning: data format and data type mismatch')\n",
    "    does_column_have_correct_data_type = any(t for t in data_type.value)\n",
    "    if not does_column_have_correct_data_type:\n",
    "        print(f'error: data type {column.dtype} doesnot have the data type specified in format reference file')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "\n",
    "    # 2. check if missing values are allowed\n",
    "    is_missing_data = utils.check_missing_data(column)\n",
    "    is_missing_values_authorized = format_file_ref.get('is_missing_values', 'test_skipped')\n",
    "    print('is_missing_values', is_missing_values_authorized, is_missing_data)\n",
    "    if is_missing_values_authorized == 'test_skipped':\n",
    "        print('missing_value test skipped')\n",
    "    elif not is_missing_values_authorized and is_missing_data:\n",
    "\n",
    "        print('Error found missing data but missing data are not authorized')\n",
    "    else:\n",
    "        print('test 2 passed')\n",
    "    \n",
    "    \n",
    "    # 3. check lower bound\n",
    "    print(format_file_ref)\n",
    "    lower_bound = format_file_ref.get('lower_bound')\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        \n",
    "        # should work for both numerical and datetime data sets\n",
    "        \n",
    "        is_lower_bound_correct = np.all(column_without_nan >= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_lower_bound_correct:\n",
    "            print('Warning: found some data below lower bound')\n",
    "        else:\n",
    "            print('test 3 passed')\n",
    "    else:\n",
    "        print('test 3 skipped ')\n",
    "    # 4. check upper bound\n",
    "    upper_bound = format_file_ref.get('upper_bound')\n",
    "    if upper_bound is not None:\n",
    "         # should work for both numerical and datetime data sets\n",
    "        is_upper_bound_correct = np.all(column_without_nan <= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_upper_bound_correct:\n",
    "            print('Warning: found some data  above upper bound')\n",
    "        else:\n",
    "            print('test 4 passed')\n",
    "            \n",
    "    else:\n",
    "        print('test 4 skipped')\n",
    "    # 5. check if possible_values are contained in variable\n",
    "    categorical_values = format_file_ref.get('categorical_values')    \n",
    "    if categorical_values is None:\n",
    "        print('categorical value check test skipped')\n",
    "    else:\n",
    "        unique_values = utils.unique(column)\n",
    "        _is_error_found = False\n",
    "        for val in unique_values:\n",
    "            if val not in categorical_values and not np.isnan(val):\n",
    "                print(f'critical warning: {val} not in possible values')\n",
    "                _is_error_found = True\n",
    "        if not _is_error_found:\n",
    "            print('test 5: passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb649b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad597a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a5627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eab20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "\n",
    "check_missing_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
