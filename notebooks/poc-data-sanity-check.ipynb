{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc33777f",
   "metadata": {},
   "source": [
    "# data sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1667e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional, Callable\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "import utils\n",
    "from data_type import DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645ef4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Tuple[Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    # check for each variable name existing in one view, that it doesnot exist in another\n",
    "    # view. if it is, rename both variables\n",
    "    # for this purpose, parse every combination once\n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "\n",
    "def join_muti_view_dataset(multi_view_dataset: pd.DataFrame,\n",
    "                           primary_key: str=None,\n",
    "                          as_multi_index: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    \n",
    "    _views_names = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "    \n",
    "    joined_dataframe = multi_view_dataset[_views_names[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_names)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_names[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_names[x]))\n",
    "    \n",
    "    if as_multi_index:\n",
    "        # convert plain dataframe into multi index dataframe\n",
    "        # primary key will have its own view\n",
    "        _header_labels = ['views', 'feature_name']\n",
    "        _primary_key_label = 'primary_key'\n",
    "        \n",
    "        _multi_index = multi_view_dataset.columns\n",
    "        _key_values = joined_dataframe[primary_key].values  # storing primary key\n",
    "\n",
    "        _all_features_names = []\n",
    "        _new_views_names = []\n",
    "        for view_name in _views_names:\n",
    "            # get all columns name for each view, and remove primary key\n",
    "            _features_names = list(multi_view_dataset[view_name].columns)\n",
    "            _features_names.remove(primary_key)\n",
    "            _all_features_names.extend(_features_names)\n",
    "\n",
    "            for feature_name in _features_names:\n",
    "                _new_views_names.append(view_name)\n",
    "                # appending as much as there are feature within each view\n",
    "            #features_name[name].remove(primary_key)\n",
    "\n",
    "        _header = pd.MultiIndex.from_arrays([ _new_views_names, _all_features_names],\n",
    "                                            names=_header_labels)\n",
    "        joined_dataframe  = pd.DataFrame(joined_dataframe[_all_features_names].values, columns=_header)\n",
    "        joined_dataframe[_primary_key_label, primary_key] = _key_values\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"\"\"\"\n",
    "    _views_names = list(format_file_ref.keys())\n",
    "    primary_key = None\n",
    "    _c_view = None\n",
    "    for view_name in _views_names:\n",
    "        file_content = format_file_ref[view_name]\n",
    "        _features_names = list(file_content.keys())\n",
    "        for feature_name in _features_names:\n",
    "            feature_content  = file_content[feature_name]\n",
    "            _d_format = feature_content.get('data_format')\n",
    "            \n",
    "            if _d_format == DataType.KEY.name:\n",
    "                if _c_view is None:\n",
    "                    primary_key = feature_name\n",
    "                    _c_view = view_name\n",
    "                    print(f'found primary key {primary_key}')\n",
    "                else:\n",
    "                    print(f'error: found 2 primary keys is same view {view_name}')\n",
    "        _c_view = None\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "\n",
    "def select_data_from_format_file_ref(datasets: Dict[str, Dict[str, Any]],\n",
    "                                     format_file: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"returns an updated dataset containing only the features detailed in format_file\"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    updated_dataset = {}\n",
    "    _views_format_file = list(format_file.keys())\n",
    "    \n",
    "    for view in _views_format_file:\n",
    "        if view in datasets.keys():\n",
    "            # only extract features from format_file\n",
    "            _format_file_features = list(format_file[view].keys())\n",
    "            _current_dataset_feature = datasets[view].columns.tolist()\n",
    "            try:\n",
    "                updated_dataset[view] = datasets[view][_format_file_features]\n",
    "            except KeyError as ke:\n",
    "                # catch error if a column is specified in data format file\n",
    "                # but not found in dataset\n",
    "                _missing_feature = []\n",
    "                for feature in _format_file_features:\n",
    "                    if feature not in _current_dataset_feature:\n",
    "                        _missing_feature.append(feature)\n",
    "                print('Error: th following features', *_missing_feature, f'are not found in view: {view}')\n",
    "        else:\n",
    "            # trigger error\n",
    "            print(f'error!: missing view {view} in dataset')\n",
    "            \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fb620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    }
   ],
   "source": [
    "multi_format_file_ref = utils.load_format_file_ref('multi_format_file')\n",
    "multi_dataset_to_check = utils.load_tabular_datasets(r'test7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CustomWarning():\n",
    "    \n",
    "        def __init__(self, disclosure: int,level: str):\n",
    "            #super().__init__('l')\n",
    "            self.disclosure = disclosure\n",
    "            self.level = level\n",
    "        \n",
    "        def display(self,message, columns = ' ' ):  \n",
    "            \n",
    "            \n",
    "            logger = logging.getLogger('mylogger')\n",
    "            #logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            #handler = logging.FileHandler('mylog.log')\n",
    "            #formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            #handler.setFormatter(formatter)\n",
    "            #logger.addHandler(handler)\n",
    "            \n",
    "            if self.disclosure == 1:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    logger.warning(message)\n",
    "            elif self.disclosure == 2:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    message = 'Critical Warning.' + message\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    message = 'Regular Warning.' + message\n",
    "                    logger.warning(message)\n",
    "            elif self.disclosure == 3:\n",
    "                if self.level == 'CRITICAL':\n",
    "                    message = 'Critical Warning. ' + message + 'Columns affected :' + columns\n",
    "                    logger.critical(message)\n",
    "                elif self.level == 'WARNING':\n",
    "                    message = 'Regular Warning. ' + message  + 'Columns affected :' + columns\n",
    "                    logger.warning(message) \n",
    "            print (message) \n",
    "            return message\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207638d",
   "metadata": {},
   "source": [
    "existing tests\n",
    "\n",
    "- test keys (should be done before joining) \n",
    " |-> unicity of value\n",
    "- test datetime\n",
    " |-> are they datetime parsable (for that i am using `dateutil` python package)\n",
    " \n",
    "- test variables including\n",
    " |-> test if data have missing values and missing values are not allowed\n",
    " | -> test correct categories / sub categories\n",
    " | -> test lower bound\n",
    " | -> test upper bound\n",
    " | -> check if defined values are contained in categorical variables\n",
    " \n",
    "- data transformation\n",
    "\n",
    "|-> interpolate missing values (if allowed) using specific method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d485c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regular Warning.hsskks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Warning.hsskks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regular Warning.hsskks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = CustomWarning(2, 'WARNING')\n",
    "inst.display('hsskks', 'kl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef4959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "343be64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found primary key pkey\n",
      "found primary key pkey\n",
      "found primary key pkey\n",
      "primary key pkey\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.98667</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.78856</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name discrete       city   e file1.1      2           file1.time  \\\n",
       "0                64.0      Lille  16   False  False  2018-01-03 04:00:00   \n",
       "1                26.0      Lille  96    True   True  2018-01-02 04:00:00   \n",
       "2                61.0      Paris   8    True   True  2018-01-01 09:00:00   \n",
       "3                29.0      Paris   6    True  False  2018-01-04 20:00:00   \n",
       "4                99.0      Lille  79    True   True  2018-01-04 09:00:00   \n",
       "..                ...        ...  ..     ...    ...                  ...   \n",
       "95                9.0      Paris  62    True   True  2018-01-02 13:00:00   \n",
       "96               98.0  Marseille  49   False   True  2018-01-02 21:00:00   \n",
       "97               21.0      Lille  14   False  False  2018-01-02 06:00:00   \n",
       "98               42.0  Marseille  10    True  False  2018-01-02 01:00:00   \n",
       "99                3.0      Paris  89    True   True  2018-01-03 22:00:00   \n",
       "\n",
       "feature_name  pressure e.1 gender blood type file2.1           file2.time  \\\n",
       "0              0.98667  98  WOMAN          A   False  2018-01-02 06:00:00   \n",
       "1             0.996889  35    MAN         AB    True  2018-01-01 00:00:00   \n",
       "2             0.777026  65    MAN          A   False  2018-01-02 10:00:00   \n",
       "3             0.877527  81    MAN         AB    True  2018-01-03 12:00:00   \n",
       "4             0.447389  88  WOMAN          O    True  2018-01-01 10:00:00   \n",
       "..                 ...  ..    ...        ...     ...                  ...   \n",
       "95            0.953184  53    MAN         AB   False  2018-01-02 05:00:00   \n",
       "96            0.442283  35    MAN        NaN    True  2018-01-05 02:00:00   \n",
       "97            0.988543  67    MAN          B   False  2018-01-01 12:00:00   \n",
       "98            0.059791  48    MAN          B    True  2018-01-02 09:00:00   \n",
       "99            0.939352  13    MAN          B   False  2018-01-03 02:00:00   \n",
       "\n",
       "feature_name        pH                  pkey  \n",
       "0                  NaN  qpqorfhylu gmfjy bdj  \n",
       "1             0.023107  kkmjozalfyirgsire ui  \n",
       "2             0.587685  ezfasuuycdda foisjte  \n",
       "3             0.894073  faxiqkt xggzmwzoidbg  \n",
       "4             0.026831  znwhlj rwzdutnagwasy  \n",
       "..                 ...                   ...  \n",
       "95             0.78856  zeqhcikzdodus jn qjf  \n",
       "96            0.402979  iicthcvfmkajbvr gzir  \n",
       "97                 NaN  ztjakcsk bhjoksdz lm  \n",
       "98            0.651801   sabunaa opt vpulnxj  \n",
       "99            0.751969  qmbexyexvgromrm admu  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(multi_format_file_ref.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(multi_format_file_ref)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(multi_dataset_to_check, multi_format_file_ref)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names,\n",
    "                                                             primary_key)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "multi_df_joined = join_muti_view_dataset(multi_df_to_check, primary_key)\n",
    "    \n",
    "df_to_check = multi_df_joined.droplevel(0, axis=1)  # remove views from dataset\n",
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3201f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>views</th>\n",
       "      <th colspan=\"2\" halign=\"left\">contatct</th>\n",
       "      <th colspan=\"8\" halign=\"left\">file1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">file2</th>\n",
       "      <th>primary_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 04:00:00</td>\n",
       "      <td>0.98667</td>\n",
       "      <td>98</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 04:00:00</td>\n",
       "      <td>0.996889</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0.777026</td>\n",
       "      <td>65</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>0.587685</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-04 20:00:00</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>81</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 12:00:00</td>\n",
       "      <td>0.894073</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 09:00:00</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>88</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>53</td>\n",
       "      <td>MAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 05:00:00</td>\n",
       "      <td>0.78856</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 21:00:00</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>35</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 06:00:00</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>67</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-02 01:00:00</td>\n",
       "      <td>0.059791</td>\n",
       "      <td>48</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02 09:00:00</td>\n",
       "      <td>0.651801</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-03 22:00:00</td>\n",
       "      <td>0.939352</td>\n",
       "      <td>13</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-03 02:00:00</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "views        contatct            file1                                      \\\n",
       "feature_name discrete       city     e file1.1      2           file1.time   \n",
       "0                64.0      Lille    16   False  False  2018-01-03 04:00:00   \n",
       "1                26.0      Lille    96    True   True  2018-01-02 04:00:00   \n",
       "2                61.0      Paris     8    True   True  2018-01-01 09:00:00   \n",
       "3                29.0      Paris     6    True  False  2018-01-04 20:00:00   \n",
       "4                99.0      Lille    79    True   True  2018-01-04 09:00:00   \n",
       "..                ...        ...   ...     ...    ...                  ...   \n",
       "95                9.0      Paris    62    True   True  2018-01-02 13:00:00   \n",
       "96               98.0  Marseille    49   False   True  2018-01-02 21:00:00   \n",
       "97               21.0      Lille    14   False  False  2018-01-02 06:00:00   \n",
       "98               42.0  Marseille    10    True  False  2018-01-02 01:00:00   \n",
       "99                3.0      Paris    89    True   True  2018-01-03 22:00:00   \n",
       "\n",
       "views                                          file2                       \\\n",
       "feature_name  pressure e.1 gender blood type file2.1           file2.time   \n",
       "0              0.98667  98  WOMAN          A   False  2018-01-02 06:00:00   \n",
       "1             0.996889  35    MAN         AB    True  2018-01-01 00:00:00   \n",
       "2             0.777026  65    MAN          A   False  2018-01-02 10:00:00   \n",
       "3             0.877527  81    MAN         AB    True  2018-01-03 12:00:00   \n",
       "4             0.447389  88  WOMAN          O    True  2018-01-01 10:00:00   \n",
       "..                 ...  ..    ...        ...     ...                  ...   \n",
       "95            0.953184  53    MAN         AB   False  2018-01-02 05:00:00   \n",
       "96            0.442283  35    MAN        NaN    True  2018-01-05 02:00:00   \n",
       "97            0.988543  67    MAN          B   False  2018-01-01 12:00:00   \n",
       "98            0.059791  48    MAN          B    True  2018-01-02 09:00:00   \n",
       "99            0.939352  13    MAN          B   False  2018-01-03 02:00:00   \n",
       "\n",
       "views                            primary_key  \n",
       "feature_name        pH                  pkey  \n",
       "0                  NaN  qpqorfhylu gmfjy bdj  \n",
       "1             0.023107  kkmjozalfyirgsire ui  \n",
       "2             0.587685  ezfasuuycdda foisjte  \n",
       "3             0.894073  faxiqkt xggzmwzoidbg  \n",
       "4             0.026831  znwhlj rwzdutnagwasy  \n",
       "..                 ...                   ...  \n",
       "95             0.78856  zeqhcikzdodus jn qjf  \n",
       "96            0.402979  iicthcvfmkajbvr gzir  \n",
       "97                 NaN  ztjakcsk bhjoksdz lm  \n",
       "98            0.651801   sabunaa opt vpulnxj  \n",
       "99            0.751969  qmbexyexvgromrm admu  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4447dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1\n",
      "test 1 passed\n",
      "is_missing_values True False\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values True False\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values True True\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': True}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "contatct\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n",
      "file2\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': 'bool', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'DATETIME', 'data_type': 'DATETIME', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "datetime parsed\n",
      "test 1 passed\n",
      "is_missing_values False True\n",
      "Error found missing data but missing data are not authorized\n",
      "{'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "is_missing_values False False\n",
      "test 2 passed\n",
      "{'data_format': 'KEY', 'data_type': 'CHARACTER', 'values': 'object', 'is_missing_values': False}\n",
      "test 3 skipped \n",
      "test 4 skipped\n",
      "categorical value check test skipped\n",
      "test 1 passed\n",
      "test 2 passed\n"
     ]
    }
   ],
   "source": [
    "# Data sanity check\n",
    "\n",
    "new_feature_name = { v: list(pre_parsed_dataset_to_check[v].columns) for v in views_names}\n",
    "new_feature_name\n",
    "\n",
    "for view in views_names:\n",
    "    print(view)\n",
    "    \n",
    "    feature_names = list(multi_format_file_ref[view].keys())\n",
    "    for n_feature_name, feature_name in zip(new_feature_name[view], feature_names):\n",
    "        check_variable_compliance(df_to_check[n_feature_name], multi_format_file_ref[view][feature_name])\n",
    "        data_format = multi_format_file_ref[view][feature_name].get('data_format')\n",
    "        if data_format == DataType.DATETIME.name:\n",
    "            # addtional check for DATETIME data format\n",
    "            check_datetime_variable_compliance(df_to_check[n_feature_name])\n",
    "            \n",
    "        if data_format == DataType.KEY.name:\n",
    "            check_key_variable_compliance(df_to_check[n_feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b26e186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>e</th>\n",
       "      <th>file1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>file1.time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>e.1</th>\n",
       "      <th>gender</th>\n",
       "      <th>blood type</th>\n",
       "      <th>pkey</th>\n",
       "      <th>discrete</th>\n",
       "      <th>city</th>\n",
       "      <th>pkey</th>\n",
       "      <th>file2.1</th>\n",
       "      <th>file2.time</th>\n",
       "      <th>pH</th>\n",
       "      <th>pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.088082</td>\n",
       "      <td>63</td>\n",
       "      <td>MAN</td>\n",
       "      <td>A</td>\n",
       "      <td>zmixzrgvxrjqxoe sluk</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>qpqorfhylu gmfjy bdj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>0.774788</td>\n",
       "      <td>20</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>vrzahnpfluspdcbfnaqt</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>kkmjozalfyirgsire ui</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xkdawggpnuulcewuoyzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.514092</td>\n",
       "      <td>2</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>ezfasuuycdda foisjte</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.407279</td>\n",
       "      <td>khuulhwgwnjggrfoefce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>70</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>gwj luzejwdxzsiljxzd</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>faxiqkt xggzmwzoidbg</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.536301</td>\n",
       "      <td>xxysdmwwmjsmyhaswfdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>90</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>jjdvcnofivbqhirxzdyo</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>znwhlj rwzdutnagwasy</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.749443</td>\n",
       "      <td>ldejfuij mnbnf wwmms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>0.295578</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>A</td>\n",
       "      <td>hrvepmqjn llgbzplshv</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>zeqhcikzdodus jn qjf</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrmdecb s pohtmrcdj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>81</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.474322</td>\n",
       "      <td>41</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>B</td>\n",
       "      <td>wroevwyuamxibzshlxxh</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.388389</td>\n",
       "      <td>whmwrpvqmerdpwwzxasf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.927511</td>\n",
       "      <td>7</td>\n",
       "      <td>MAN</td>\n",
       "      <td>B</td>\n",
       "      <td>ywadcykylymkdtzfctpg</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Lille</td>\n",
       "      <td>ztjakcsk bhjoksdz lm</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.889067</td>\n",
       "      <td>pnrepvmrxqabdlvisclv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.494798</td>\n",
       "      <td>11</td>\n",
       "      <td>MAN</td>\n",
       "      <td>O</td>\n",
       "      <td>ruchbfa zwgenxslegrl</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>sabunaa opt vpulnxj</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.402979</td>\n",
       "      <td>iicthcvfmkajbvr gzir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>74</td>\n",
       "      <td>WOMAN</td>\n",
       "      <td>AB</td>\n",
       "      <td>zeapltpxuvuibfybxcll</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Paris</td>\n",
       "      <td>qmbexyexvgromrm admu</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.667349</td>\n",
       "      <td>kiejdmbuih awhuifwwd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name   e file1.1      2           file1.time  pressure e.1 gender  \\\n",
       "0             98    True  False  2018-01-01 00:00:00  0.088082  63    MAN   \n",
       "1             83   False   True  2018-01-01 01:00:00  0.774788  20    MAN   \n",
       "2             73   False  False  2018-01-01 02:00:00  0.514092   2  WOMAN   \n",
       "3             45    True   True  2018-01-01 03:00:00  0.832881  70  WOMAN   \n",
       "4             84    True  False  2018-01-01 04:00:00  0.696152  90    MAN   \n",
       "..            ..     ...    ...                  ...       ...  ..    ...   \n",
       "95            66    True   True  2018-01-04 23:00:00  0.295578  41  WOMAN   \n",
       "96            81   False   True  2018-01-05 00:00:00  0.474322  41  WOMAN   \n",
       "97            82    True   True  2018-01-05 01:00:00  0.927511   7    MAN   \n",
       "98            18   False   True  2018-01-05 02:00:00  0.494798  11    MAN   \n",
       "99            70   False   True  2018-01-05 03:00:00  0.316395  74  WOMAN   \n",
       "\n",
       "feature_name blood type                  pkey discrete       city  \\\n",
       "0                     A  zmixzrgvxrjqxoe sluk     64.0      Lille   \n",
       "1                     O  vrzahnpfluspdcbfnaqt     26.0      Lille   \n",
       "2                     A  pnrepvmrxqabdlvisclv     61.0      Paris   \n",
       "3                    AB  gwj luzejwdxzsiljxzd     29.0      Paris   \n",
       "4                     B  jjdvcnofivbqhirxzdyo     99.0      Lille   \n",
       "..                  ...                   ...      ...        ...   \n",
       "95                    A  hrvepmqjn llgbzplshv      9.0      Paris   \n",
       "96                    B  wroevwyuamxibzshlxxh     98.0  Marseille   \n",
       "97                    B  ywadcykylymkdtzfctpg     21.0      Lille   \n",
       "98                    O  ruchbfa zwgenxslegrl     42.0  Marseille   \n",
       "99                   AB  zeapltpxuvuibfybxcll      3.0      Paris   \n",
       "\n",
       "feature_name                  pkey file2.1           file2.time        pH  \\\n",
       "0             qpqorfhylu gmfjy bdj    True  2018-01-01 00:00:00  0.023107   \n",
       "1             kkmjozalfyirgsire ui   False  2018-01-01 01:00:00       NaN   \n",
       "2             ezfasuuycdda foisjte    True  2018-01-01 02:00:00  0.407279   \n",
       "3             faxiqkt xggzmwzoidbg    True  2018-01-01 03:00:00  0.536301   \n",
       "4             znwhlj rwzdutnagwasy    True  2018-01-01 04:00:00  0.749443   \n",
       "..                             ...     ...                  ...       ...   \n",
       "95            zeqhcikzdodus jn qjf    True  2018-01-04 23:00:00       NaN   \n",
       "96            iicthcvfmkajbvr gzir   False  2018-01-05 00:00:00  0.388389   \n",
       "97            ztjakcsk bhjoksdz lm    True  2018-01-05 01:00:00  0.889067   \n",
       "98             sabunaa opt vpulnxj    True  2018-01-05 02:00:00  0.402979   \n",
       "99            qmbexyexvgromrm admu   False  2018-01-05 03:00:00  0.667349   \n",
       "\n",
       "feature_name                  pkey  \n",
       "0             kkmjozalfyirgsire ui  \n",
       "1             xkdawggpnuulcewuoyzz  \n",
       "2             khuulhwgwnjggrfoefce  \n",
       "3             xxysdmwwmjsmyhaswfdb  \n",
       "4             ldejfuij mnbnf wwmms  \n",
       "..                             ...  \n",
       "95            wrmdecb s pohtmrcdj   \n",
       "96            whmwrpvqmerdpwwzxasf  \n",
       "97            pnrepvmrxqabdlvisclv  \n",
       "98            iicthcvfmkajbvr gzir  \n",
       "99            kiejdmbuih awhuifwwd  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b31892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_data_sanity_check(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "     - data_frame: \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def check_key_variable_compliance(column: pd.Series,\n",
    "                                  \n",
    "                                  col_name:str=None,\n",
    "                                  warning=None) -> bool:\n",
    "    \"\"\"performs data sanity check over variable of type `KEY`\n",
    "    warning should be Critical warnings\n",
    "    \"\"\"\n",
    "    # variables initialisation\n",
    "    is_test_passed = True \n",
    "    \n",
    "    # 1. check unicity of values in column\n",
    "    \n",
    "    n_unique_samples = utils.unique(column, number=True)\n",
    "    n_samples = column.shape[0]\n",
    "    \n",
    "    if n_unique_samples != n_samples:\n",
    "        is_test_passed = False\n",
    "        print(f'error: keys not unique ! b of samples= {n_samples} and unique values {n_unique_samples}')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "    # 2. check if missing database contained in key (key should not contain any missing data)\n",
    "    if utils.check_missing_data(column):\n",
    "        is_test_passed = False\n",
    "        print('error: missing data found in key')\n",
    "    else:\n",
    "        \n",
    "        print('test 2 passed')\n",
    "\n",
    "                \n",
    "    return is_test_passed\n",
    "\n",
    "\n",
    "def check_datetime_variable_compliance(column: pd.Series):\n",
    "    \"\"\"additional data sanity checks for datetime variable\"\"\"\n",
    "    # test 1. check if datetime is parsable\n",
    "    \n",
    "    # remove nan\n",
    "    column_without_nan = column.dropna()\n",
    "    are_datetime_parsables =  np.all(column.apply(utils.is_datetime))\n",
    "    if not are_datetime_parsables:\n",
    "        print('Warning: at least one variable is not a datetime')\n",
    "        \n",
    "    else:\n",
    "        print('datetime parsed')\n",
    "        \n",
    "def check_variable_compliance(column: pd.Series,\n",
    "                               format_file_ref: Dict[str, Any],\n",
    "                               col_name:str=None,\n",
    "                               warning=None) -> Tuple[bool, bool]:\n",
    "    \"\"\"performs a data sanity check on variable `col_name` given instruction in \n",
    "    data_file_ref\n",
    "    \"\"\"\n",
    "    is_test_passed = True\n",
    "    \n",
    "    \n",
    "    data_format_name = format_file_ref.get('data_format')\n",
    "    data_type_name = format_file_ref.get('data_type')\n",
    "    # remove nan (missing values) from \n",
    "    column_without_nan = column.dropna()\n",
    "    \n",
    "    \n",
    "    if data_format_name is None:\n",
    "        print(f'critical wraning: data fromat {data_format_name} not understood')\n",
    "    # 1. check data sub type\n",
    "    try:\n",
    "        data_type = utils.find_data_type(data_format_name, data_type_name)\n",
    "    except ValueError as err:\n",
    "        data_type = None\n",
    "        print('Critical warning: data format and data type mismatch')\n",
    "    does_column_have_correct_data_type = any(t for t in data_type.value)\n",
    "    if not does_column_have_correct_data_type:\n",
    "        print(f'error: data type {column.dtype} doesnot have the data type specified in format reference file')\n",
    "    else:\n",
    "        print('test 1 passed')\n",
    "\n",
    "    # 2. check if missing values are allowed\n",
    "    is_missing_data = utils.check_missing_data(column)\n",
    "    is_missing_values_authorized = format_file_ref.get('is_missing_values', 'test_skipped')\n",
    "    print('is_missing_values', is_missing_values_authorized, is_missing_data)\n",
    "    if is_missing_values_authorized == 'test_skipped':\n",
    "        print('missing_value test skipped')\n",
    "    elif not is_missing_values_authorized and is_missing_data:\n",
    "\n",
    "        print('Error found missing data but missing data are not authorized')\n",
    "    else:\n",
    "        print('test 2 passed')\n",
    "    \n",
    "    \n",
    "    # 3. check lower bound\n",
    "    print(format_file_ref)\n",
    "    lower_bound = format_file_ref.get('lower_bound')\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        \n",
    "        # should work for both numerical and datetime data sets\n",
    "        \n",
    "        is_lower_bound_correct = np.all(column_without_nan >= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_lower_bound_correct:\n",
    "            print('Warning: found some data below lower bound')\n",
    "        else:\n",
    "            print('test 3 passed')\n",
    "    else:\n",
    "        print('test 3 skipped ')\n",
    "    # 4. check upper bound\n",
    "    upper_bound = format_file_ref.get('upper_bound')\n",
    "    if upper_bound is not None:\n",
    "         # should work for both numerical and datetime data sets\n",
    "        is_upper_bound_correct = np.all(column_without_nan <= lower_bound)\n",
    "        \n",
    "            \n",
    "        if not is_upper_bound_correct:\n",
    "            print('Warning: found some data  above upper bound')\n",
    "        else:\n",
    "            print('test 4 passed')\n",
    "            \n",
    "    else:\n",
    "        print('test 4 skipped')\n",
    "    # 5. check if possible_values are contained in variable\n",
    "    categorical_values = format_file_ref.get('categorical_values')    \n",
    "    if categorical_values is None:\n",
    "        print('categorical value check test skipped')\n",
    "    else:\n",
    "        unique_values = utils.unique(column)\n",
    "        _is_error_found = False\n",
    "        for val in unique_values:\n",
    "            if val not in categorical_values and not np.isnan(val):\n",
    "                print(f'critical warning: {val} not in possible values')\n",
    "                _is_error_found = True\n",
    "        if not _is_error_found:\n",
    "            print('test 5: passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eab20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "\n",
    "check_missing_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
